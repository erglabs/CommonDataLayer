<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Common Data Layer</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Common Data Layer</a></li><li class="chapter-item expanded "><a href="how_it_works.html"><strong aria-hidden="true">2.</strong> How does it work</a></li><li class="chapter-item expanded "><a href="getting_started.html"><strong aria-hidden="true">3.</strong> Getting Started</a></li><li class="chapter-item expanded "><a href="deployment/index.html"><strong aria-hidden="true">4.</strong> Deployment</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deployment/local/index.html"><strong aria-hidden="true">4.1.</strong> Local deployment</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deployment/local/docker-compose.html"><strong aria-hidden="true">4.1.1.</strong> docker-compose</a></li><li class="chapter-item expanded "><a href="deployment/local/helm.html"><strong aria-hidden="true">4.1.2.</strong> Helm</a></li></ol></li><li class="chapter-item expanded "><a href="deployment/production/index.html"><strong aria-hidden="true">4.2.</strong> Production-grade deployment</a></li></ol></li><li class="chapter-item expanded "><a href="schemas_and_views.html"><strong aria-hidden="true">5.</strong> Schemas and Views</a></li><li class="chapter-item expanded "><a href="examples/index.html"><strong aria-hidden="true">6.</strong> Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">6.1.</strong> TODO</div></li></ol></li><li class="chapter-item expanded "><a href="features/index.html"><strong aria-hidden="true">7.</strong> Features</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="features/ordering.html"><strong aria-hidden="true">7.1.</strong> Message ordering</a></li><li class="chapter-item expanded "><a href="features/materialization.html"><strong aria-hidden="true">7.2.</strong> Materialization</a></li><li class="chapter-item expanded "><a href="features/client_routing.html"><strong aria-hidden="true">7.3.</strong> Client routing</a></li></ol></li><li class="chapter-item expanded "><a href="versioning.html"><strong aria-hidden="true">8.</strong> Versioning</a></li><li class="chapter-item expanded "><a href="protocol.html"><strong aria-hidden="true">9.</strong> Protocol schema</a></li><li class="chapter-item expanded "><a href="benchmarks.html"><strong aria-hidden="true">10.</strong> Benchmarks</a></li><li class="chapter-item expanded "><a href="architecture/index.html"><strong aria-hidden="true">11.</strong> Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/management.html"><strong aria-hidden="true">11.1.</strong> Management Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/cli.html"><strong aria-hidden="true">11.1.1.</strong> CLI</a></li><li class="chapter-item expanded "><a href="architecture/web_admin.html"><strong aria-hidden="true">11.1.2.</strong> Admin Web Panel</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/api.html"><strong aria-hidden="true">11.2.</strong> GraphQL API</a></li><li class="chapter-item expanded "><a href="architecture/configuration.html"><strong aria-hidden="true">11.3.</strong> Configuration Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/schema_registry.html"><strong aria-hidden="true">11.3.1.</strong> Schema Registry</a></li><li class="chapter-item expanded "><a href="architecture/leader_elector.html"><strong aria-hidden="true">11.3.2.</strong> Leader Elector</a></li><li class="chapter-item expanded "><a href="architecture/edge_registry.html"><strong aria-hidden="true">11.3.3.</strong> Edge Registry</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/ingestion.html"><strong aria-hidden="true">11.4.</strong> Ingestion Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/data_router.html"><strong aria-hidden="true">11.4.1.</strong> Data Router</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/storage.html"><strong aria-hidden="true">11.5.</strong> Storage Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/command_service.html"><strong aria-hidden="true">11.5.1.</strong> Command Service</a></li><li class="chapter-item expanded "><a href="architecture/db_shrinker_storage.html"><strong aria-hidden="true">11.5.2.</strong> Db Shrinker Storage</a></li><li class="chapter-item expanded "><a href="architecture/query_service.html"><strong aria-hidden="true">11.5.3.</strong> Query Service</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">11.6.</strong> Materialization Layer</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/object_builder.html"><strong aria-hidden="true">11.6.1.</strong> Object Builder</a></li><li class="chapter-item expanded "><a href="architecture/partial_update_engine.html"><strong aria-hidden="true">11.6.2.</strong> Partial Update Engine</a></li><li class="chapter-item expanded "><a href="architecture/materializer_general.html"><strong aria-hidden="true">11.6.3.</strong> General Materializer</a></li><li class="chapter-item expanded "><a href="architecture/materializer_ondemand.html"><strong aria-hidden="true">11.6.4.</strong> On Demand Materializer</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/retrieval.html"><strong aria-hidden="true">11.7.</strong> Retrieval Layer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/query_router.html"><strong aria-hidden="true">11.7.1.</strong> Query Router</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/utils.html"><strong aria-hidden="true">11.8.</strong> Utils</a></li></ol></li><li class="chapter-item expanded "><a href="configuration/index.html"><strong aria-hidden="true">12.</strong> Configuration</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="configuration/api.html"><strong aria-hidden="true">12.1.</strong> api</a></li><li class="chapter-item expanded "><a href="configuration/command-service.html"><strong aria-hidden="true">12.2.</strong> command-service</a></li><li class="chapter-item expanded "><a href="configuration/data-router.html"><strong aria-hidden="true">12.3.</strong> data-router</a></li><li class="chapter-item expanded "><a href="configuration/edge-registry.html"><strong aria-hidden="true">12.4.</strong> edge-registry</a></li><li class="chapter-item expanded "><a href="configuration/materializer-general.html"><strong aria-hidden="true">12.5.</strong> materializer-general</a></li><li class="chapter-item expanded "><a href="configuration/materializer-ondemand.html"><strong aria-hidden="true">12.6.</strong> materializer-ondemand</a></li><li class="chapter-item expanded "><a href="configuration/object-builder.html"><strong aria-hidden="true">12.7.</strong> object-builder</a></li><li class="chapter-item expanded "><a href="configuration/partial-update-engine.html"><strong aria-hidden="true">12.8.</strong> partial-update-engine</a></li><li class="chapter-item expanded "><a href="configuration/query-router.html"><strong aria-hidden="true">12.9.</strong> query-router</a></li><li class="chapter-item expanded "><a href="configuration/query-service.html"><strong aria-hidden="true">12.10.</strong> query-service</a></li><li class="chapter-item expanded "><a href="configuration/query-service-ts.html"><strong aria-hidden="true">12.11.</strong> query-service-ts</a></li><li class="chapter-item expanded "><a href="configuration/schema-registry.html"><strong aria-hidden="true">12.12.</strong> schema-registry</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">13.</strong> RFCs</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="rfc/NAF-Commit_message_formalization_and_enforcement.html"><strong aria-hidden="true">13.1.</strong> Commit message formalization</a></li><li class="chapter-item expanded "><a href="rfc/NAF-CDL_documentation_generation.html"><strong aria-hidden="true">13.2.</strong> CDL Documentation Generation</a></li><li class="chapter-item expanded "><a href="rfc/NAF-Branching_strategy.html"><strong aria-hidden="true">13.3.</strong> Branching strategy</a></li><li class="chapter-item expanded "><a href="rfc/NAF-Documentation_and_RFC_process.html"><strong aria-hidden="true">13.4.</strong> Documentation restructure and RFC process</a></li><li class="chapter-item expanded "><a href="rfc/CDLF-0000C-00-rfc-01.html"><strong aria-hidden="true">13.5.</strong> 0000C - Alternative communication method</a></li><li class="chapter-item expanded "><a href="rfc/CDLF-0000E-00-rfc-01.html"><strong aria-hidden="true">13.6.</strong> 0000E - MessagePack support</a></li><li class="chapter-item expanded "><a href="rfc/CDLF-00010-00-rfc-01.html"><strong aria-hidden="true">13.7.</strong> 00010 - CdlIM Versioning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="rfc/CDLF-00010-00-rfc-02.html"><strong aria-hidden="true">13.7.1.</strong> 00010 - CdlIM Versioning 2.0</a></li></ol></li><li class="chapter-item expanded "><a href="rfc/CDLF-00011-00-rfc-01.html"><strong aria-hidden="true">13.8.</strong> 00011 - Materialization - Overview</a></li><li class="chapter-item expanded "><a href="rfc/CDLF-00012-00-rfc-01.html"><strong aria-hidden="true">13.9.</strong> 00012 - Edge Registry</a></li><li class="chapter-item expanded "><a href="rfc/CDLF-00013-00-rfc-01.html"><strong aria-hidden="true">13.10.</strong> 00013 - Materialized Views</a></li><li class="chapter-item expanded "><a href="rfc/CDLF-00015-00-rfc-01.html"><strong aria-hidden="true">13.11.</strong> 00015 - CDL publishing deployment configurations</a></li><li class="chapter-item expanded "><a href="rfc/CDLF-00016-00-rfc-01.html"><strong aria-hidden="true">13.12.</strong> 00016 - Schema-Registry-less CDL deployment</a></li><li class="chapter-item expanded "><a href="rfc/CDLF-00017-00-rfc-01.html"><strong aria-hidden="true">13.13.</strong> 00017 - Query raw routes</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">The Common Data Layer</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://github.com/epiphany-platform/CommonDataLayer" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#common-data-layer" id="common-data-layer">Common Data Layer</a></h1>
<p>The Common Data Layer (CDL) is a data storage service. Its primary goals are performance, versatility, scalability, and ease-of-modification.</p>
<h1><a class="header" href="#how-does-it-work" id="how-does-it-work">How does it work</a></h1>
<p>Data intake is all performed over Message Queue and via the Data Router. Message Queue (MQ) is an abstract entity and the CDL currently supports <a href="https://kafka.apache.org/">kafka</a> and <a href="https://www.rabbitmq.com/">RabbitMQ</a>. CDL listens over a single topic queue for messages keyed on strings, each providing a schema ID. The schema ID is used to load the appropriate topic (stored per-schema in the schema registry), which is used to route the message along to the correct repository.</p>
<p>For each repository, a command service is listening to its specific MQ topic for incoming messages. Each message is stored according to the repository's format. Though most of our command service implementations use append-only storage with each value under a key being assigned a version, it is not required by user-implemented command services.</p>
<p>The query router is used to direct requests for data to the appropriate repository. Each repository also has a query service listening for gRPC requests for data. These query services are used for direct queries of data from the repositories. As repositories are meant to be easily introduced to an already running CDL, but the topic per repository can't be used to make a gRPC request, each schema also stores the dynamic address of the query service it belongs to.</p>
<h1><a class="header" href="#getting-started" id="getting-started">Getting Started</a></h1>
<p>For infomration on specific services and their responsibilities:</p>
<ul>
<li><a href="./architecture/command_service.html">Command Service</a></li>
<li><a href="./architecture/data_router.html">Data Router</a></li>
<li><a href="./architecture/schema_registry.html">Schema Registry</a></li>
<li><a href="./architecture/query_service.html">Query Service</a></li>
</ul>
<h1><a class="header" href="#installation" id="installation">Installation</a></h1>
<p>CDL is a written in Rust. See Rust's <a href="https://www.rust-lang.org/tools/install">installation</a> guide to install. Below are the pre-requesites needed to get started: </p>
<ul>
<li>Rust</li>
<li>Docker</li>
<li>Docker Compose</li>
</ul>
<p>You can download <a href="https://docs.docker.com/desktop/">docker desktop</a> for both Windows and MacOS to intall docker and docker compose on your local machine.</p>
<h2><a class="header" href="#working-with-cdl-locally" id="working-with-cdl-locally">Working with CDL Locally</a></h2>
<p>Below is a following simple amount of steps to getting started working with the services in the CDL locally on your machine. To build and install container images of services within the CDL, run <code>build.sh</code> in root directory of this project.</p>
<p>Please review how to set up CDL locally on your machine but viewing <a href="deployment/local/index.html">local setup</a> documentations for a sample deployment. </p>
<p>Below we will walk through a simple use case of the CDL:</p>
<p><strong>Use Case</strong></p>
<ul>
<li>Create Schema</li>
<li>Insert Data</li>
<li>Query Data</li>
</ul>
<h2><a class="header" href="#add-schema-via-cli" id="add-schema-via-cli">Add Schema via CLI</a></h2>
<p>A schema can be added through the CLI tool localed in the <code>cdl-cli</code> directory. To be able to run the cli you must have a rust compiler. The following command below creates the schema with a name according a json schema in a file as well as sets the topic for routing data through kafka. </p>
<pre><code>cargo run --bin cdl -- --registry-addr &lt;registry_address&gt; schema add --name &lt;schema_name&gt; --topic &quot;cdl.document.input&quot; --file &lt;file_path_to_json&gt;
</code></pre>
<p>Here is the sample JSON schema format that the CDL anticipates and ultimatley will validate data by. Please review <a href="./architecture/schema_registry.html">README</a> in <code>schema-registry</code> directory for more information.</p>
<pre><code>{
	&quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;,
    &quot;$id&quot;: &quot;http://example.com/product.schema.json&quot;,
	&quot;definitions&quot;: {
		&quot;1.0.0&quot;: {
            &quot;description&quot;: &quot;A work order&quot;,
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;property1&quot;: {
                    &quot;description&quot;: &quot;&quot;,
                    &quot;type&quot;: &quot;integer&quot;
                },
                &quot;property2&quot;: {
                    &quot;description&quot;:&quot;&quot;,
                    &quot;type&quot;: &quot;string&quot; 
                },
            },
            &quot;required&quot;: [&quot;property1&quot;]
        }
    }
}
</code></pre>
<p><strong>NOTE</strong>: Schema's can be added via <a href="https://grpc.io/docs/what-is-grpc/introduction/">gRPC</a> to the schema registry. Ensure that you have <code>protoc</code> installed on your machine you machine generate <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/crates/rpc/proto">proto</a> files in a supported language and make requests via a client.</p>
<h2><a class="header" href="#insert-data" id="insert-data">Insert Data</a></h2>
<p>Data can be inserted into the system by data being written to Kafka or ingested through RabbitMQ. Data must be in JSON format with the following fields: <code>schemaId</code>, <code>objectId</code> and <code>data</code> to be routed through the CDL. 
It's worth noting that CDL doesn't rely on message key unless <a href="./features/ordering.html">message ordering</a> feature is enabled. However in order to keep system more performant it's advised to pass NULL as message key or evenly distributed strings.</p>
<p>Below is an example of the what input data would look like. Both ID fields are UUIDs.</p>
<pre><code>{
    &quot;schemaId&quot;: &lt;UUID&gt;,
    &quot;objectId&quot;: &lt;UUID&gt;,
    &quot;data&quot;: &quot;{ \&quot;some_propery&quot;: \&quot;object\&quot;}&quot;

}
</code></pre>
<h3><a class="header" href="#publish-messae-via-rabbitmq" id="publish-messae-via-rabbitmq">Publish messae via RabbitMQ</a></h3>
<p>Below is a sample <code>curl</code> command you can also publish a message through RabbitMQ web admin tool through a exchange or directly to a queue. Review <a href="deployment/local/index.html">local setup</a> for configuration details on Kafka, RabbitMQ.</p>
<p>The command below example takes input data and publishes to the default exchange in RabbitMQ. The message gets consumed and is sent to kafka and published to topic which is determined by <code>schemaId</code>.  The message is then routed to command service which handles routing and storage of data by type. </p>
<pre><code>curl -i -u ${user}:${pass} -H &quot;Accept: application/json&quot; \
-H &quot;Content-Type:application/json&quot; \
-XPOST -d'{&quot;properties&quot;:{},&quot;routing_key&quot;:&quot;my_key&quot;,&quot;payload&quot;:&quot;my body&quot;,&quot;payload_encoding&quot;:&quot;string&quot;}'\
http://${ampq_url}/api/exchanges/%2F/${exchange}/publish
</code></pre>
<h2><a class="header" href="#query-data" id="query-data">Query Data</a></h2>
<h3><a class="header" href="#query-via-query-service" id="query-via-query-service">Query via Query Service</a></h3>
<p>Following this example local deployment, you can query for data saved. Here data is saved within the PR.
Ensure that environment variables are set for <code>POSTGRES_USERNAME</code>, <code>POSTGRES_PASSWORD</code>, <code>POSTGRES_HOST</code>, <code>POSTGRES_PORT</code>, <code>POSTGRES_DBNAME</code>, <code>POSTGRES_SCHEMA</code>, <code>INPUT_PORT</code> and <code>DS_QUERY_URL</code> or run query service directly on machine.</p>
<pre><code>cargo run --bin query_service -- \
--schema-registry-addr &lt;registry_addr&gt; \
--ds-query-url &lt;ds-url&gt;  \
--input-port &lt;input_port&gt;
</code></pre>
<h3><a class="header" href="#query-data-via-query-router" id="query-data-via-query-router">Query Data via Query Router</a></h3>
<p>The Query Router works with the query services to route requests to the correct repository, determined per schema (based on its query address).</p>
<pre><code>cargo run --bin query_router -- \
--schema-registry-addr &lt;schema_registry_addr&gt; \
--cache-capacity &lt;cache_capacity&gt; \
--input-port &lt;input_port&gt;
</code></pre>
<h2><a class="header" href="#deployment" id="deployment">Deployment</a></h2>
<p>See <a href="./deployment/index.html">chapter in the book</a></p>
<h1><a class="header" href="#deployment-1" id="deployment-1">Deployment</a></h1>
<ul>
<li><a href="deployment/local/index.html">Local deployment</a></li>
<li><a href="deployment/production/index.html">Production-grade deployment</a></li>
</ul>
<h1><a class="header" href="#local-deployment" id="local-deployment">Local deployment</a></h1>
<p>Currently CDL supports two ways of local deployment:
via <a href="deployment/local/helm.html">HELM chart</a> and via <a href="deployment/local/docker-compose.html">docker-compose</a></p>
<h1><a class="header" href="#docker-compose" id="docker-compose">docker-compose</a></h1>
<h2><a class="header" href="#preamble" id="preamble">Preamble</a></h2>
<blockquote>
<p>Intended way of deploying CDL is through helm files.</p>
</blockquote>
<p>Contents of this folder aren't meant for use on production and they may be lagging behind our k8s deployment. 
Sole purpose of this directory is to prepare exemplary development environment, from which anyone can startup their development on 
<code>common data layer</code> without Kubernetes knowledge. Contents of docker-compose may not contain all applications, so be aware of that. You may alter it
on your local machine to your needs.</p>
<p>For k8s deployment, please refer to our <a href="deployment/local/helm.html">documentation</a>. </p>
<h2><a class="header" href="#requirements" id="requirements">Requirements</a></h2>
<ul>
<li>docker</li>
<li>docker-compose</li>
<li>rust (optionally)</li>
</ul>
<h2><a class="header" href="#volume" id="volume">Volume</a></h2>
<p>The directory <code>./docker-volume</code> is used as a volume. Please note it is not fully <code>.gitignore</code>d because we rely on some setup scripts attached via volumes.</p>
<h2><a class="header" href="#deployment-2" id="deployment-2">Deployment</a></h2>
<p>You must first add environment variables:</p>
<p><code>DOCKER_BUILDKIT=1</code><br />
<code>COMPOSE_DOCKER_CLI_BUILD=1</code></p>
<p>Environment with infrastructure alone is started via:</p>
<p><code>docker-compose up -d</code></p>
<p>If you want to add cdl components to it, you must specify <code>-f</code> options:</p>
<p><code>docker-compose -f docker-compose.cdl-kafka.yml -f docker-compose.yml up -d</code>
or
<code>docker-compose -f docker-compose.cdl-rabbit.yml -f docker-compose.yml up -d</code></p>
<p>Sometimes it's useful to store data on disk (eg. for debugging), we can achieve this by adding <code>-f docker-compose.host-storage.yml</code> to combination:</p>
<p><code>docker-compose -f docker-compose.host-storage.yml -f docker-compose.yml up -d</code></p>
<h2><a class="header" href="#entry-points-in-system" id="entry-points-in-system">Entry points in system</a></h2>
<h3><a class="header" href="#kafka" id="kafka">Kafka</a></h3>
<p>You can write to kafka on <code>localhost:9092</code>.
Default <em>data-router</em> topic is <code>cdl.data.input</code>.
By default there is no replication on <em>schema_registry</em>. Postgres <em>command_service</em> input channel is <code>cdl.document.data</code>.</p>
<p>Errors are written to <code>cdl.reports</code>.</p>
<h3><a class="header" href="#rabbitmq" id="rabbitmq">Rabbitmq</a></h3>
<p>You can write to rabbit on <code>localhost:5672</code>.
Default <em>data-router</em> fanout exchange is <code>cdl.data.input</code>.
There is also managament panel available at <code>localhost:15672</code>. The credentials are <code>user</code>/<code>CHANGEME</code>.
By default there is no replication on <em>schema_registry</em>. Postgres <em>command_service</em> input channel is <code>cdl.document.data</code>.</p>
<p>Errors are written to <code>cdl.reports</code> fanout exchange and can be read via <code>cdl.reports</code> queue.</p>
<h3><a class="header" href="#postgresql" id="postgresql">PostgreSQL</a></h3>
<p>To access postgres you must have some postgresql client installed.</p>
<p>For command line it's best to refer to your OS package manager (<code>homebrew</code> on OSX, <code>apt</code> on Ubuntu, <code>choco</code> on Windows).</p>
<p><code>psql -U postgres --password -h localhost</code>
the password is <code>1234</code></p>
<h3><a class="header" href="#schema-registry" id="schema-registry">Schema registry</a></h3>
<p>Schema registry can be either accessed via <a href="https://github.com/epiphany-platform/CommonDataLayer/blob/develop/crates/rpc/proto/schema_registry.proto">gRPC</a>, or via <code>cdl-cli</code>. Using <code>cdl-cli</code> will require presence of rust compiler on your local machine.
Tips on how to install rust are available on <a href="https://rustup.rs/">rustup website</a>.</p>
<p>From main directory of this project you can run <code>cdl-cli</code> via:</p>
<p><code>cargo run -p cdl-cli -- --help</code></p>
<p>Registry address is <code>http://localhost:50101</code>.</p>
<p>eg.</p>
<ul>
<li>Adding new schema:</li>
</ul>
<blockquote>
<p><code>cargo run -p cdl-cli -- --registry-addr &quot;http://localhost:50101&quot; schema add --name default-document</code></p>
</blockquote>
<ul>
<li>Setting schema topic (in order for this schema to be routed to <code>command-service</code> topic must be <code>cdl.document.input</code>)</li>
</ul>
<blockquote>
<p><code>cargo run -p cdl-cli -- --registry-addr &quot;http://localhost:50101&quot; schema set-topic --id 0a626bba-15ff-11eb-8004-000000000000 --topic &quot;cdl.document.input&quot;</code></p>
</blockquote>
<ul>
<li>Getting all schemas</li>
</ul>
<blockquote>
<p><code>cargo run -p cdl-cli -- --registry-addr &quot;http://localhost:50101&quot; schema names</code></p>
</blockquote>
<h1><a class="header" href="#recipes" id="recipes">Recipes</a></h1>
<h2><a class="header" href="#druid-timeseries-env" id="druid-timeseries-env">Druid timeseries env</a></h2>
<pre><code>docker-compose -f docker-compose.cdl-kafka.yml -f docker-compose.yml -f docker-compose.druid.yml up -d \
    postgres \
    zoo_kafka \
    kafka \
    zoo_druid \
    coordinator \
    broker \
    historical \
    router \
    middlemanager \
    schema_registry \
    data_router \
    druid_command \
    druid_query \
    query_router
</code></pre>
<h1><a class="header" href="#helm" id="helm">Helm</a></h1>
<p>For most use cases using docker-compose is a good way for local development. However some features can be developed/tested only inside Kubernetes cluster environments.</p>
<h2><a class="header" href="#requirements-1" id="requirements-1">Requirements</a></h2>
<ul>
<li><a href="https://docs.docker.com/engine/install/">docker</a>, <a href="https://docs.docker.com/compose/install/">docker-compose</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">minikube</a> (other type of local Kubernetes cluster may be used, but some commands may differ)</li>
<li><a href="https://helm.sh/docs/intro/install/">helm</a></li>
</ul>
<h2><a class="header" href="#setting-up-local-cluster" id="setting-up-local-cluster">Setting up local cluster</a></h2>
<h3><a class="header" href="#start-the-cluster" id="start-the-cluster">Start the cluster</a></h3>
<p>Decide how much resources can be used by the local k8s cluster and start it. </p>
<p>Note: Resources aren't blocked if k8s cluster is idle, parent system can still use them. It's recommended to use all/almost all of available cpus because rust compilation takes place in this environment(and it can take a while especially on a release build).</p>
<p><code>minikube start --cpus 8 --memory 8192 --driver=docker</code></p>
<h2><a class="header" href="#building-docker-image" id="building-docker-image">Building docker image</a></h2>
<p>Next step is to build docker images which will be used by k8s pods. </p>
<pre><code class="language-shell">eval $(minikube docker-env)
ENV=PROD DOCKER_BUILDKIT=1 ./build.sh
</code></pre>
<p>First command will change docker daemon we're communicating with to docker inside minikube. From now on any docker command run from current shell will be executed inside minikube docker daemon. To connect to your standard docker daemon just start a new shell.
Second command builds docker image, it may take some time first time you build the image. You can change <code>ENV=PROD</code> to <code>ENV=DEV</code> if you want shorter build time(in cost of less performant output).</p>
<h2><a class="header" href="#spin-up-infrastructure-services" id="spin-up-infrastructure-services">Spin up infrastructure services</a></h2>
<p>To start necessary infrastructure(not necessary if you've deployed infrastructure yourself/you want to connect to services on our azure cluster):</p>
<p><code>helm install --values ./deployment/helm/infrastructure/values.yaml infrastructure ./deployment/helm/infrastructure</code></p>
<p>If you want to use druid repository you also need to start druid.
<code>helm install --values ./deployment/helm/infrastructure-druid/values.yaml infrastructure-druid ./deployment/helm/infrastructure-druid</code></p>
<h2><a class="header" href="#installing-cdl" id="installing-cdl">Installing CDL</a></h2>
<p>To install the solution you need to execute:</p>
<p><code>helm install --values ./helm/cdl/values-local.yaml cdl ./deployment/helm/cdl</code></p>
<p>After a moment Kubernetes pods should get started. You can check their status by <code>kubectl get pods</code></p>
<p>Note: Using default <code>values.yaml</code> file(or skipping this parameter) will install configuration for our cloud cluster.</p>
<h2><a class="header" href="#removing-cdl" id="removing-cdl">Removing CDL</a></h2>
<p>To remove current installation of CDL you can use:</p>
<p><code>helm uninstall cdl</code></p>
<p>This operation will take ~30 seconds(default Kubernetes timeout).</p>
<h2><a class="header" href="#upgrading-cdl" id="upgrading-cdl">Upgrading CDL</a></h2>
<p>The easiest way to update whole deployment is to uninstall it, rebuild docker image and reinstall the helm chart.</p>
<h2><a class="header" href="#useful-commands" id="useful-commands">Useful commands</a></h2>
<ul>
<li><code>minikube list services</code> - list services running on minikube(port numbers for input, output etc.) </li>
<li><code>minikube dashboard</code> - runs web dashboard of the Kubernetes cluster</li>
<li><code>kubectl get pods</code> - get list of pods</li>
<li><code>kubectl exec cdl-rust-storage-0 -it -- /bin/ash</code> - run commands directly on single k8s pod</li>
<li><code>kubectl logs cdl-rust-storage-0</code> - get logs generated by pod</li>
<li><code>kubectl describe pod cdl-rust-storage-0</code> - gets more information about a pod, may contain info why pod is not starting etc.</li>
</ul>
<h2><a class="header" href="#troubleshooting" id="troubleshooting">Troubleshooting</a></h2>
<p>Few problems you might encounter during development on local k8s cluster:</p>
<h3><a class="header" href="#tls-errors-while-running-docker-commands" id="tls-errors-while-running-docker-commands">TLS errors while running docker commands</a></h3>
<p>Sometimes(if minikube isn't properly stopped) it changes cluster address. This results with errors while trying to connect to minikube docker daemon. In order to fix it we have to restart the daemon.
<em>This issue should be fixed on current minikube version.</em></p>
<h1><a class="header" href="#production-grade-deployment" id="production-grade-deployment">Production-grade deployment</a></h1>
<p>TODO</p>
<h1><a class="header" href="#schemas-and-views" id="schemas-and-views">Schemas and Views</a></h1>
<h2><a class="header" href="#schemas" id="schemas">Schemas</a></h2>
<p>Schemas are the format in which data is to be sent to the Common Data Layer. Each schema is assigned
a random UUID on creation and initially is created with a name and an initial definition. The name is
not required to be unique among all schemas (as the UUID is the unique identifier of schemas), but is
simply for identifying the schema when searching for schemas. It can be updated at any time.</p>
<p>The definition is a <a href="https://json-schema.org/">JSON Schema</a> document that describes the expected format of data stored
under the given schema. It is assigned the semantic version 1.0.0, and cannot be updated after creation.
Rather, updates can be made to the definition by inserting another definition with a new semantic version
strictly larger than any other existing version assigned to that schema.</p>
<p>When validating data against a schema, either the latest version of the definition is used, or optionally 
a semantic version range can be provided, and the latest version meeting the range is used.</p>
<p>Schemas can also have multiple views, described below.</p>
<p>An example of a CDL schema might be: </p>
<pre><code class="language-json">{
    &quot;id&quot;: &quot;&lt;schema UUID&gt;&quot;,
    &quot;name&quot;: &quot;Vector&quot;,
    &quot;definitions&quot;: {
        &quot;1.0.0&quot;: {
            &quot;x&quot;: &quot;number&quot;,
            &quot;y&quot;: &quot;number&quot;,
            &quot;z&quot;: &quot;number&quot;
        },
        &quot;1.0.0&quot;: {
            &quot;w&quot;: &quot;number&quot;,
            &quot;x&quot;: &quot;number&quot;,
            &quot;y&quot;: &quot;number&quot;,
            &quot;z&quot;: &quot;number&quot;
        }
    },
    &quot;views&quot;: [
        &quot;&lt;view 1 UUID&gt;&quot;,
        &quot;&lt;view 2 UUID&gt;&quot;,
        &quot;&lt;view 3 UUID&gt;&quot;
    ]
}
</code></pre>
<h2><a class="header" href="#views" id="views">Views</a></h2>
<p>Views describe projections of data defined by a specific schema. As with schemas, each view is assigned
a UUID on creation and initially is created with a name and an initial definition. Like schemas, the
name is a vanity name for searching purposes only and can be updated at any time, as the UUID is the unique
identifier used. On creation, a view is assigned to a schema and cannot be assigned to a different one.</p>
<p>The definition is a <a href="https://jmespath.org/">JMESPath</a> expression which describes how to project data defined under the
parent schema into the desired output format. Unlike with schemas, view definitions are editable at any time
and are not versioned, though this feature may be added in the future.</p>
<p>An example of a CDL view might be:</p>
<pre><code class="language-json">{
    &quot;id&quot;: &quot;&lt;view UUID&gt;&quot;,
    &quot;name&quot;: &quot;two dimensions&quot;,
    &quot;definition&quot;: &quot;{ x: x, y: y }&quot;
}
</code></pre>
<h1><a class="header" href="#examples" id="examples">Examples</a></h1>
<p>TODO</p>
<h1><a class="header" href="#features" id="features">Features</a></h1>
<h2><a class="header" href="#guidelines" id="guidelines">Guidelines</a></h2>
<ul>
<li>
<p><code>name or description</code> field demonstrates only brief info about the feature itself, to learn more, please follow to the RFC in question.</p>
</li>
<li>
<p>There may be multiple RFCs/documents for each feature, but usually only one will be linked here.</p>
</li>
<li>
<p>Generally, the state of the feature looks should follow these guidelines:</p>
<ul>
<li><code>[discussion/idea/request]-&gt;[TechSpec/RFC] and optionally [PoC] -&gt; [RC] -&gt; [Ready]</code></li>
<li><code>RC</code> means that the feature is waiting for the release</li>
<li><code>Ready</code> means that the feature is tested, merged and released</li>
<li><code>Retired</code> or <code>Abandoned</code> means that the feature was dropped due to various factors, including but not limited to lack of support, legacy code or lack of usage.</li>
<li><code>Suggestion</code> means the feature was requested or suggested, and is in preplanning or plannig stage. This shoud result in a Technical Specification or in a doc.</li>
<li>Features in <code>TechSpec</code> phase can have missing or broken document links, since documentation is usually waiting for pull request or have additional comments pending.</li>
</ul>
</li>
<li>
<p>in case there is a developmnent release we will mark it as XXX-rc, those are not fully fledged releases, a lot of the things can be broken there or undergoing testing
this is basically a beta branch that may or may not result in stabilization and/or release. As such State for those features will remain <code>RC</code> until released to master</p>
</li>
</ul>
<h2><a class="header" href="#feature-list" id="feature-list">Feature List</a></h2>
<table><thead><tr><th>Feature ID</th><th>name or description</th><th>State</th><th>CDL version</th><th>latestRFC</th></tr></thead><tbody>
<tr><td>CDLF-00001-00</td><td>Basic Document Repository</td><td>Retired</td><td>0.0.1</td><td>N/A</td></tr>
<tr><td>CDLF-00002-00</td><td>Basic Binary Repository</td><td>Retired</td><td>0.0.1</td><td>N/A</td></tr>
<tr><td>CDLF-00003-00</td><td>Basic Timeseries Repository</td><td>Retired</td><td>0.0.1</td><td>N/A</td></tr>
<tr><td>CDLF-00004-00</td><td>Query Service and Query Routing</td><td>Ready</td><td>0.0.1</td><td>N/A</td></tr>
<tr><td>CDLF-00005-00</td><td>Query Service: Postgres</td><td>Ready</td><td>0.0.1</td><td>N/A</td></tr>
<tr><td>CDLF-00006-00</td><td>Automatic Query Destination</td><td>Ready</td><td>0.0.1</td><td>N/A</td></tr>
<tr><td>CDLF-00007-00</td><td>System Metrics Support</td><td>Ready</td><td>0.0.1</td><td>N/A</td></tr>
<tr><td>CDLF-00008-00</td><td>Victoria Metrics Support</td><td>Ready</td><td>0.0.9</td><td>N/A</td></tr>
<tr><td>CDLF-00009-00</td><td>CDL Input Message Batching</td><td>Ready</td><td>1.0.0</td><td>N/A</td></tr>
<tr><td>CDLF-0000A-00</td><td>Message Ordering</td><td>Ready</td><td>1.0.0</td><td><a href="features/./ordering.html">DOC</a></td></tr>
<tr><td>CDLF-0000B-00</td><td>Access Groups</td><td>Suggestion</td><td>-----</td><td>N/A</td></tr>
<tr><td>CDLF-0000C-00</td><td>Full GRPC communication support</td><td>Ready</td><td>1.0.0</td><td><a href="features/../rfc/CDLF-0000C-00-rfc-01.html">RFC</a></td></tr>
<tr><td>CDLF-0000D-00</td><td>Service Mesh (istio)</td><td>Abandoned</td><td>-----</td><td>N/A</td></tr>
<tr><td>CDLF-0000E-00</td><td>CDL Input Message Format - MessagePack</td><td>TechSpec</td><td>-----</td><td><a href="features/../rfc/CDLF-0000E-00-rfc-01.html">RFC</a></td></tr>
<tr><td>CDLF-0000F-00</td><td>Configuration Service</td><td>Suggestion</td><td>-----</td><td></td></tr>
<tr><td>CDLF-00010-00</td><td>Protocol Versioning</td><td>TechSpec</td><td>-----</td><td><a href="features/../rfc/CDLF-00010-00-rfc-01.html">RFC</a></td></tr>
<tr><td>CDLF-00011-00</td><td>Basic Materialization</td><td>Abandoned</td><td>-----</td><td><a href="features/../rfc/CDLF-00011-00-rfc-01.html">RFC</a></td></tr>
<tr><td>CDLF-00012-00</td><td>Edge registry</td><td>Ready</td><td>1.0.0</td><td><a href="features/../rfc/CDLF-00012-00-rfc-01.html">RFC</a></td></tr>
<tr><td>CDLF-00013-00</td><td>Materialized views</td><td>Ready</td><td>1.0.0</td><td><a href="features/../rfc/CDLF-00013-00-rfc-01.html">RFC</a></td></tr>
<tr><td>CDLF-00014-00</td><td>Materialization - Filters</td><td>Ready</td><td>1.0.0</td><td>N/A</td></tr>
<tr><td>CDLF-00016-00</td><td>Schema-Registry-less CDL deployment</td><td>Ready</td><td>1.0.0</td><td><a href="features/../rfc/CDLF-00016-00-rfc-01.html">RFC</a></td></tr>
<tr><td>CDLF-00017-00</td><td>Materialization - Computation</td><td>TechSpec</td><td>-----</td><td></td></tr>
<tr><td>CDLF-00018-00</td><td>Materialization - Materialized Types</td><td>TechSpec</td><td>-----</td><td></td></tr>
<tr><td>CDLF-00019-00</td><td>Materializer - OnDemand</td><td>Ready</td><td>1.0.0</td><td>N/A</td></tr>
<tr><td>CDLF-0001A-00</td><td>Materializer - General</td><td>Ready</td><td>1.0.0</td><td>N/A</td></tr>
<tr><td>CDLF-0001B-00</td><td>Materialization - Notifications</td><td>Ready</td><td>1.0.0</td><td>N/A</td></tr>
<tr><td>CDLF-0001C-00</td><td>Object-side configuration</td><td>Suggestion</td><td>-----</td><td></td></tr>
<tr><td>CDLF-0001D-00</td><td>CIM Object Valdiation</td><td>Suggestion</td><td>-----</td><td></td></tr>
<tr><td>CDLF-0001E-00</td><td>Materialization - Relationships</td><td>Idea</td><td>-----</td><td></td></tr>
</tbody></table>
<h2><a class="header" href="#references" id="references">References:</a></h2>
<p><a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/docs/rfc">CDL RFC directory</a></p>
<h1><a class="header" href="#message-ordering" id="message-ordering">Message ordering</a></h1>
<h2><a class="header" href="#index" id="index">Index</a></h2>
<ul>
<li><a href="features/ordering.html#what_is">What is message ordering</a></li>
<li><a href="features/ordering.html#why">Why does it matter</a>
<ul>
<li><a href="features/ordering.html#soccer">Football/soccer game</a></li>
<li><a href="features/ordering.html#trafic_monitor">Network traffic monitor</a></li>
<li><a href="features/ordering.html#pros">Pros and cons</a></li>
</ul>
</li>
<li><a href="features/ordering.html#message_ordering_in_cdl">Message ordering in CDL</a></li>
<li><a href="features/ordering.html#how_to_use">How to use it</a>
<ul>
<li><a href="features/ordering.html#overall">Overall</a></li>
<li><a href="features/ordering.html#kafka">Communication through Apache Kafka</a></li>
<li><a href="features/ordering.html#rabbit">Communication through RabbitMQ</a></li>
<li><a href="features/ordering.html#rpc">Communication through RPC</a></li>
</ul>
</li>
</ul>
<h2><a class="header" href="#a-namewhat_isawhat-is-message-ordering" id="a-namewhat_isawhat-is-message-ordering"><a name="what_is"></a>What is message ordering</a></h2>
<p>Message ordering is a guarantee that certain messages will be processed by CDL in the same order there were sent to the system. Proper ordering matter mostly to user applications which base their business logic on real-time data where no approximation is allowed. </p>
<h2><a class="header" href="#a-namewhyawhy-does-it-matter" id="a-namewhyawhy-does-it-matter"><a name="why"></a>Why does it matter</a></h2>
<p>Let's consider two use cases which will show us why order of received messages might/might not matter: </p>
<h3><a class="header" href="#a-namesoccerafootballsoccer-game" id="a-namesoccerafootballsoccer-game"><a name="soccer"></a>Football/soccer game</a></h3>
<p>We develop an application which decides strategy for playing football matches. Based on actual score we decide if our team should play more offensively, defensively or utilize balanced play style. </p>
<p>Incoming events: </p>
<ul>
<li>(1) Match start </li>
<li>(2) We scored a goal </li>
<li>(3) Enemy scored a goal </li>
<li>(4) Match end </li>
</ul>
<p>Linearizable system (system which can establish exact order for each message), will see events in the order they were produced. Our application will start game with balanced play style, go defense after 2nd event is received (we scored a goal) and change it back to balanced once enemy hits us back. </p>
<p>Without proper message ordering same situation can be processed (seen by clients) differently. If second message got delayed for some reason and came out of order, we could play with completely different strategy. We would start game in balanced formation, then the 3rd message will show up (enemy scored a goal), so we'll think we're losing the game and start playing offensively. After some time, 2nd message will finally show up and we'll end game in balanced formation. </p>
<p>From user perspective it would seem that our program is broken because we played more offensively once we were one point ahead of the enemy, which could cause us to lose our advantage. </p>
<p>It's worth noting that if we play other sport discipline e.g., a basketball or volleyball making decisions with few delayed messages can be considered valid behavior - there are more data points, so it would be fine to use approximated score (difference of one or two points doesn't change the general strategy because there are much more points in general). In such case we don't make our decision based on loosing single point but make it if we're losing by a few points. Loosing single point (single message) has a small impact on our decision-making process. </p>
<h3><a class="header" href="#a-nametrafic_monitoranetwork-traffic-monitor" id="a-nametrafic_monitoranetwork-traffic-monitor"><a name="trafic_monitor"></a>Network traffic monitor</a></h3>
<p>We develop an application which measure network traffic. It can show different statistics per chosen period. </p>
<p>We're looking for statistics, so it's often fine to approximate the data. We mostly do our job on many data points at once (time period) so even skipping some of them would mostly be fine. What is also important is the fact that we're not processing a real-time data - we're showing data from some time ago (a month, an hour etc.), so even if messages come in the wrong order, they will be available in the system once we query them. </p>
<p>In this case message ordering is not that important. We're using historical data which will be correct regardless of message ordering.</p>
<h3><a class="header" href="#a-nameprosapros" id="a-nameprosapros"><a name="pros"></a>Pros</a></h3>
<ul>
<li>without message ordering system might see a state of things that have never happened (not just delayed state) </li>
</ul>
<h3><a class="header" href="#a-nameconsacons" id="a-nameconsacons"><a name="cons"></a>Cons</a></h3>
<ul>
<li>fully linearizable systems can be really slow - inerrability drastically limits system ability to process data in parallel (and scale horizontally) </li>
</ul>
<h2><a class="header" href="#a-namemessage_ordering_in_cdlamessage-ordering-in-cdl" id="a-namemessage_ordering_in_cdlamessage-ordering-in-cdl"><a name="message_ordering_in_cdl"></a>Message ordering in CDL</a></h2>
<p>CDL supports three message ordering strategies: </p>
<ul>
<li>Fully ordered messages(linearizable) </li>
<li>Message ordering defined by causality </li>
<li>Unordered messages (no message ordering guarantees) </li>
</ul>
<p>Message ordering defined by causality is a middle ground between two opposite strategies. It allows you to keep message ordering for some of the messages without performance costs of full linearization. E.g., in our first example we could say that order of messages regarding same game is important, but we don't care about order of two messages related to different sport events. </p>
<h2><a class="header" href="#a-namehow_to_useahow-to-use-it" id="a-namehow_to_useahow-to-use-it"><a name="how_to_use"></a>How to use it</a></h2>
<h3><a class="header" href="#a-nameoverallaoverall" id="a-nameoverallaoverall"><a name="overall"></a>Overall</a></h3>
<p>In CDL message ordering guarantees are defined on per message level. In CDL data ingestion message format there is an optional field called <code>order_group_id</code>. This field should contain user generated UUID with ordering info. If you set this value CDL guarantees that messages with the same <code>order_group_id</code> will be processed in the order they were send to CDL. Otherwise, if field is left empty, no ordering guarantees are met for this message. This behavior allows us to support causality ordering (multiple <code>order_group_id</code> values), linearizability (same <code>order_group_id</code> for each message) or to skip ordering guarantees at all(<code>ordering_group_id</code> not provided). </p>
<h3><a class="header" href="#a-namekafkaacommunication-through-apache-kafka" id="a-namekafkaacommunication-through-apache-kafka"><a name="kafka"></a>Communication through Apache Kafka</a></h3>
<p>If you’re using Kafka as a message bus following requirements needs to be met for message ordering to work correctly: </p>
<ul>
<li>Kafka partitioning should be based on message key </li>
<li>Message keys of data coming to CDL should be set to <code>order_group_id</code> or left empty if message order is not important</li>
<li>Scaling data router and command service is possible up to number of Kafka partitions. If you need more service instances you must have enough Kafka partitions to feed them messages. </li>
</ul>
<h3><a class="header" href="#a-namerabbitacommunication-through-rabbitmq" id="a-namerabbitacommunication-through-rabbitmq"><a name="rabbit"></a>Communication through RabbitMQ</a></h3>
<p>If you’re using RabbitMQ as a message bus following requirements needs to be met for message ordering to work correctly: </p>
<ul>
<li>You must create proper exchange-queue bindings in RabbitMQ 
<ul>
<li>create 2+ queues - one for unordered messages, one or more for ordered ones; number of queues is the limit of horizontal instance scaling (exception - unordered messages) </li>
<li>create one exchange which name will be saved in schema registry</li>
<li>create binding between exchange and queues in a way that messages with <code>unordered</code> message key will go to queues with unordered data, other keys will be split between other queues </li>
</ul>
</li>
<li>Configure command service instances: 
<ul>
<li>Unordered message queue can be passed to each command service instance </li>
<li>Ordered message queue can be passed to single command service (exclusive consumer) </li>
</ul>
</li>
<li>Message keys of data coming to CDL should be set to <code>order_group_id</code> or left empty if message order is not important</li>
</ul>
<p>Unfortunately, that means that scaling command services can be done only manually (automatic scaling may be implemented by <a href="https://github.com/epiphany-platform/CommonDataLayer/issues/185">#185</a>), instances which process only unordered messages can be scaled automatically. </p>
<h3><a class="header" href="#a-namerpcacommunication-through-rpcwip" id="a-namerpcacommunication-through-rpcwip"><a name="rpc"></a>Communication through RPC(WIP)</a></h3>
<p>In case of communication through RPC message ordering is guaranteed by request/response pattern and its client responsibility to decide if messages can be sent (and processed) in parallel. <code>order_group_id</code> field is ignored. </p>
<h1><a class="header" href="#materialization" id="materialization">Materialization</a></h1>
<h2><a class="header" href="#tutorial-how-to-materialize-data-in-cdl" id="tutorial-how-to-materialize-data-in-cdl">Tutorial: How to materialize data in CDL</a></h2>
<p>Tools used:</p>
<ul>
<li>GNU/Linux system</li>
<li><code>docker</code> and <code>docker-compose</code> - for running dependencies</li>
<li><a href="https://github.com/FedericoPonzi/Horust"><code>horust</code></a> for running CDL locally</li>
<li>any gRPC client (either custom-made or generic like <a href="https://github.com/uw-labs/bloomrpc">BloomRPC</a>) - for communication with schema registry.</li>
<li><code>python3</code> - for pushing data to kafka topic </li>
<li><code>psql</code> - for checking materialized data in Postgres</li>
<li>web browser - for checking Jaeger traces &amp; graphQL interactive API.</li>
</ul>
<h3><a class="header" href="#preparing-environment" id="preparing-environment">Preparing environment</a></h3>
<p>The easiest way to setup an environment is to use <a href="https://github.com/epiphany-platform/CommonDataLayer-deployment">Common Data Layer deployment repository</a>.</p>
<p>You can use one of the examples provided there to run everything with one command.</p>
<h3><a class="header" href="#setup" id="setup">Setup</a></h3>
<p>To materialize view one needs proper setup.</p>
<p>Firstly, system needs schema which informs where store data (for example in document storage like Postgres).
Secondly, it needs view definition which informs what fields are necessary and from which schema it should take it. It also defines where to put materialized data.</p>
<p>During this tutorial we are going to introduce several ways of inserting and mutating state of CDL. One of them (graphQL) is designed only for managament and test purposes.
However, it is the easiest way to quickly manually test the materialization pipeline.</p>
<h4><a class="header" href="#a-manual-setup" id="a-manual-setup">A) Manual setup</a></h4>
<p>Most common and production-like way to setup is by sending requests to schema registry.</p>
<h5><a class="header" href="#adding-new-schema" id="adding-new-schema">Adding new schema</a></h5>
<h6><a class="header" href="#a-grpc-api" id="a-grpc-api">A) gRPC API</a></h6>
<p>To add new schema we need to load schema_registry.proto to our client. In this tutorial we are going to use BloomRPC.</p>
<p>All proto files are stored in <code>crates/rpc/proto</code> directory.</p>
<p>In proto file we can see that message <code>NewSchema</code> uses <code>bytes</code> as a definition.
In bloomRPC it means we need to encode our json definition in base 64:</p>
<p>Schema definition:</p>
<pre><code class="language-json">{
    &quot;name&quot;: &quot;string&quot;
} 
</code></pre>
<p>Encoded:</p>
<pre><code class="language-base64">ewogICAgIm5hbWUiOiAic3RyaW5nIgp9
</code></pre>
<p>Usually schema registry API is available at <a href="http://localhost:50101">http://localhost:50101</a>.</p>
<p>RPC request (<code>schema_registry.SchemaRegistry.AddSchema</code>):</p>
<pre><code class="language-json">{
    &quot;metadata&quot;: {
        &quot;name&quot;: &quot;tutorial-schema&quot;,
        &quot;insert_destination&quot;: &quot;cdl.document.1.data&quot;,
        &quot;query_address&quot;: &quot;http://localhost:50201&quot;,
        &quot;schema_type&quot;: 2
    },
    &quot;definition&quot;: &quot;ewogICAgIm5hbWUiOiAic3RyaW5nIgp9&quot;
} 
</code></pre>
<p>RPC response:</p>
<pre><code class="language-json">{
    &quot;id&quot;: &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;
} 
</code></pre>
<p>Lets save this UUID for later. It is <code>schema_id</code>.</p>
<h6><a class="header" href="#b-graphql-api-management-and-test-purposes-only" id="b-graphql-api-management-and-test-purposes-only">B) graphQL API (management and test purposes only)</a></h6>
<p>Instead of using gRPC we can also leverage graphQL Gateway API to manage all schemas:
An easy to use interface is available at <a href="http://localhost:50106/graphiql">http://localhost:50106/graphiql</a></p>
<p>Mutation request:</p>
<pre><code class="language-graphql">mutation addSchema {
    addSchema(new: {
        insertDestination: &quot;cdl.document.1.data&quot;,
        name: &quot;tutorial-schema&quot;,
        queryAddress: &quot;http://localhost:50201&quot;,
        type: DOCUMENT_STORAGE,
        definition: {
            name: &quot;string&quot;
        }
    }) { id }
}
</code></pre>
<p>As you can see it does not require encoding definition, and it can return all schema metadata, therefore we filter it to retrieve only <code>schema_id</code>.</p>
<h5><a class="header" href="#adding-new-view" id="adding-new-view">Adding new view</a></h5>
<p>Next we need to add new view definition.</p>
<h6><a class="header" href="#a-grpc-api-1" id="a-grpc-api-1">A) gRPC API</a></h6>
<p>gRPC request (<code>schema_registry.SchemaRegistry.AddViewToSchema</code>):</p>
<pre><code class="language-json">{
  &quot;schema_id&quot;: &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;,
  &quot;name&quot;: &quot;tutorial-view&quot;,
  &quot;materializer_address&quot;: &quot;http://localhost:50203&quot;,
  &quot;materializer_options&quot;: &quot;{\&quot;table\&quot;: \&quot;MATERIALIZED_VIEW\&quot;}&quot;,
  &quot;fields&quot;: {
    &quot;worker_name&quot;: &quot;{ \&quot;field_name\&quot;: \&quot;name\&quot; }&quot;
  }
}
</code></pre>
<p>As you can see this time <code>materializer_options</code> are not using <code>bytes</code> format but are encoded in the string.</p>
<p>See <a href="https://github.com/epiphany-platform/CommonDataLayer/issues/442">Issue #442</a>.</p>
<p>gRPC response:</p>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;ddfc1f23-7b13-4d8e-b8ab-1def8eb30a4e&quot;
}
</code></pre>
<p>This is the <code>view_id</code>.</p>
<h6><a class="header" href="#b-graphql-api-management-and-test-purposes-only-1" id="b-graphql-api-management-and-test-purposes-only-1">B) graphQL API (management and test purposes only)</a></h6>
<p>Mutation request:</p>
<pre><code class="language-graphql">mutation addView{
  addView(schemaId: &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;, newView: {
    name: &quot;tutorial-view&quot;,
    materializerAddress: &quot;http://localhost:50203&quot;,
    materializerOptions: {
      table: &quot;MATERIALIZED_VIEW&quot;
    },
    fields: {
      worker_name: {
        field_name: &quot;name&quot;
      }
    }
  }) {
    id
  }
}
</code></pre>
<h4><a class="header" href="#b-loading-initial-schema" id="b-loading-initial-schema">B) Loading initial schema</a></h4>
<p>While manual setup is fine for one-time test, it quickly becomes mundane work.
To mitigate this problem, we created a solution to pre-populate schema registry.</p>
<p>In fact, <a href="https://github.com/epiphany-platform/CommonDataLayer-deployment">Common Data Layer deployment repository</a> already contains <a href="https://github.com/epiphany-platform/CommonDataLayer-deployment/blob/develop/bare/setup/schema-registry/initial-schema.kafka.json"><code>bare/setup/schema-registry/initial-schema.kafka.json</code></a> which describes what views and schemas should be inserted on startup.</p>
<h3><a class="header" href="#inserting-data" id="inserting-data">Inserting data</a></h3>
<p>To materialize data first we need to insert it to the CDL.</p>
<h4><a class="header" href="#a-python-script" id="a-python-script">A) Python script</a></h4>
<p>For that purpose we can write very simple Python script:</p>
<pre><code class="language-python">from kafka import KafkaProducer
from kafka.errors import KafkaError

producer = KafkaProducer(bootstrap_servers=['localhost:9092'])

with open('data.json', rb) as binary_file:
    data = binary_file.read()
    future = producer.send('cdl.data.input', data)
    
    try:
        record_metadata = future.get(timeout=10)
    except KafkaError:
        log.exception()
        pass
        
    print (record_metadata.topic)
    print (record_metadata.partition)
    print (record_metadata.offset)
</code></pre>
<p><code>data.json</code> should look like:</p>
<pre><code class="language-json">[
    {
        &quot;objectId&quot;: &quot;dc8cc976-412b-11eb-8000-100000000000&quot;,
        &quot;schemaId&quot;: &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;,
        &quot;data&quot;: { &quot;name&quot;: &quot;John&quot; }
    },
    {
        &quot;objectId&quot;: &quot;dc8cc976-412b-11eb-8000-000000000000&quot;,
        &quot;schemaId&quot;: &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;,
        &quot;payload&quot;: { &quot;name&quot;: &quot;Alice&quot; }
    }
]
</code></pre>
<h4><a class="header" href="#b-graphql-api-management-and-test-purposes-only-2" id="b-graphql-api-management-and-test-purposes-only-2">B) graphQL API (management and test purposes only)</a></h4>
<p>graphQL request:</p>
<pre><code class="language-graphql">mutation insertBatch {
  insertBatch(
    messages: [
      {
        objectId: &quot;dc8cc976-412b-11eb-8000-100000000000&quot;,
        schemaId: &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;,
        payload: { name: &quot;John&quot; }
      }
      {
        objectId: &quot;dc8cc976-412b-11eb-8000-000000000000&quot;,
        schemaId: &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;,
        payload: { name: &quot;Alice&quot; }
      }
    ]
  )
}
</code></pre>
<h3><a class="header" href="#querying-materialized-data" id="querying-materialized-data">Querying materialized data</a></h3>
<p>After a second we should be able to see our materialized view in Postgres.</p>
<pre><code class="language-sh">psql -U postgres --password -h localhost
</code></pre>
<p>The default password for local dev Postgres is <code>1234</code>, but shhh, dont tell anyone ;-)</p>
<pre><code>postgres=# select * from cdl.materialized_view; 
              object_id               | worker_name
--------------------------------------+-------------
 dc8cc976-412b-11eb-8000-100000000000 | &quot;John&quot;
 dc8cc976-412b-11eb-8000-000000000000 | &quot;Alice&quot;
(2 rows)
</code></pre>
<h3><a class="header" href="#on-demand-materialization" id="on-demand-materialization">On-Demand materialization</a></h3>
<p>Instead of waiting seconds for on-the-fly materialization, we can also demand materialized view via gRPC call.</p>
<h4><a class="header" href="#a-grpc-api-2" id="a-grpc-api-2">A) gRPC API</a></h4>
<p>gRPC request to <a href="http://localhost:50108/">http://localhost:50108/</a> (<code>materializer_ondemand.OnDemandMaterializer.Materialize</code>):</p>
<pre><code class="language-json">{
  &quot;view_id&quot;: &quot;ddfc1f23-7b13-4d8e-b8ab-1def8eb30a4e&quot;,
  &quot;schemas&quot;: {
    &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;: {
      &quot;object_ids&quot;: [
         &quot;dc8cc976-412b-11eb-8000-100000000000&quot;,
         &quot;dc8cc976-412b-11eb-8000-000000000000&quot;
      ]
    }
  }
}
</code></pre>
<p>For now user needs to use filter and enlist in the request all object ids. There is, however, <a href="https://github.com/epiphany-platform/CommonDataLayer/issues/429">an issue which should mitigate this problem</a> very soon.</p>
<p>This call returns the stream of rows, instead of collection. Thanks to that, both object builder, on demand materializer and client code don't have to allocate enormous amount of memory when handling bigger tables.</p>
<p>It also means client code can start processing data faster.</p>
<p>BloomRPC returns messages in seperate tabs:</p>
<p>Stream 1:</p>
<pre><code class="language-json">{
  &quot;fields&quot;: {
    &quot;worker_name&quot;: &quot;\&quot;John\&quot;&quot;
  },
  &quot;object_id&quot;: &quot;dc8cc976-412b-11eb-8000-100000000000&quot;
}
</code></pre>
<p>Stream 2:</p>
<pre><code class="language-json">{
  &quot;fields&quot;: {
    &quot;worker_name&quot;: &quot;\&quot;Alice\&quot;&quot;
  },
  &quot;object_id&quot;: &quot;dc8cc976-412b-11eb-8000-000000000000&quot;
}
</code></pre>
<h4><a class="header" href="#b-graphql-api-management-and-test-purposes-only-3" id="b-graphql-api-management-and-test-purposes-only-3">B) graphQL API (management and test purposes only)</a></h4>
<p>graphQL request:</p>
<pre><code class="language-graphql">query onDemandView {
  onDemandView(
    request: {
      viewId: &quot;ddfc1f23-7b13-4d8e-b8ab-1def8eb30a4e&quot;
      schemas: [
        {
          id: &quot;22c8ac58-155e-4643-ab44-42e96dbb88c7&quot;
          objectIds: [
            &quot;dc8cc976-412b-11eb-8000-100000000000&quot;,
            &quot;dc8cc976-412b-11eb-8000-000000000000&quot;
          ]
        }
      ]
    }
  ) {
    id
    rows {
      fields
      objectId
    }
  }
}
</code></pre>
<p>Unfortunately, graphQL does not support streaming, which means all rows are collected to the array before sending it to the client.
Please use it wisely and carefuly (on smaller sets of data).</p>
<h3><a class="header" href="#appendix-checking-traces-in-jaeger" id="appendix-checking-traces-in-jaeger">Appendix: Checking traces in Jaeger</a></h3>
<p>For troubleshooting or when you are curious how CDL works, we recommend using Jaeger telemetry sink, which by default is available at <a href="http://localhost:16686/search">http://localhost:16686/search</a>.</p>
<h1><a class="header" href="#client-routing" id="client-routing">Client routing</a></h1>
<p>CDL allows users to bypass routing present in SR via custom routing table defined in <code>configuration.toml</code> files (more info at <a href="features/../configuration/index.html">configuration documentation</a>).
This should be used at user's discretion - as routing will not be managed by CDL stack there's possibility of missing messages, or other issues that may arise from using this.
Nevertheless, if there's requirement of customized, static routing, this can be arranged via following steps:</p>
<h2><a class="header" href="#routing-data-for-qr-and-dr" id="routing-data-for-qr-and-dr">Routing data for QR and DR</a></h2>
<p>Both QR and DR accept configuration section:</p>
<pre><code class="language-toml">[repositories]
backup_data = { insert_destination = &quot;cdl.document.none.data&quot;, query_address = &quot;http://localhost:50202&quot;, repository_type = &quot;DocumentStorage&quot; }
</code></pre>
<p>It's a dictionary, where each entry is an object consisting of 3 fields:</p>
<ul>
<li>insert_destination - DR will route messages based on this field; it must use main <code>communication_method</code></li>
<li>query_address - QR will request data from QS located at given address</li>
<li>repository_type - DocumentStorage or Timeseries; used by QR for querying</li>
</ul>
<h2><a class="header" href="#sending-statically-routed-messages-to-cdl" id="sending-statically-routed-messages-to-cdl">Sending statically routed messages to cdl</a></h2>
<p>For purpose of static routing, CDL accepts <code>options</code> object within CDL Input Message:</p>
<pre><code class="language-json">{
    &quot;objectId&quot;: &quot;09a1048e-81dc-4286-821c-91d48086ce05&quot;,
    &quot;schemaId&quot;: &quot;9d111ba6-b855-41dc-9f91-227c2fdb4c18&quot;,
    &quot;data&quot;: { &quot;field&quot;: false, &quot;id&quot;: 5 },
    &quot;options&quot;: { &quot;repositoryId&quot;: &quot;backup_data&quot; }
}
</code></pre>
<p>Within that object, there's optional field <code>repositoryId</code> that will tell DR to use it's value to lookup predefined <code>repositories</code>.
In above case, this will cause DR to route message to <code>cdl.document.none.data</code> topic (assuming <code>communication_method</code> is kafka).</p>
<h2><a class="header" href="#querying-statically-routed-messages" id="querying-statically-routed-messages">Querying statically routed messages</a></h2>
<p>Some for QR, there's additional header on <code>single</code> and <code>multiple</code> routes: <code>REPOSITORY_ID</code>. You can use it to point QR to specific entry in routing table.</p>
<h1><a class="header" href="#cdl-versioning" id="cdl-versioning">CDL VERSIONING</a></h1>
<h2><a class="header" href="#versioning-scheme" id="versioning-scheme">Versioning scheme</a></h2>
<ul>
<li>CDL is versioned with <a href="https://semver.org/">semver</a> (MAJOR.MINOR.PATCH)</li>
<li>Major releases may break your product. Look for changelog/release notes to see if you need to update your code in order for it to work properly with cdl. </li>
<li>Minor releases will add new features without breaking any functionality.</li>
<li>Patch releases should be treated as hofixes. They will not require any changes to your deployment configuration other then version number change. They may introduce new config settings which will be optional.</li>
<li>Major and minor releases might require changes to deployment configuration. </li>
</ul>
<h2><a class="header" href="#release" id="release">Release</a></h2>
<ul>
<li>On each release repository state will be marked with a tag and docker images will be build.</li>
<li>Standard(non-tagged) releases will be made only from main branch.</li>
<li>Tagged releases may be created from different branch, unless stated otherwise we do not guarantee theirs stability.</li>
</ul>
<h2><a class="header" href="#breaking-changes" id="breaking-changes">Breaking changes</a></h2>
<p>By breaking change we mean a change in public api, that is not backward-compatible. Public api defines how clients can query CDL to do some tasks on it. Internal service communication protocol changes are not considered a public api.</p>
<h2><a class="header" href="#internal-components-changes" id="internal-components-changes">Internal components changes</a></h2>
<p>Once CDL hits 1.0.0 we will support clients with custom versions of CDL components. Clients will be able to write their own repositories(e.g in case no repository for their storage technology exists).</p>
<p>However we can't guarantee that each CDL release will work correctly with custom components. Adding new features to document repository would require custom document repositories to also include those new features. This means that for deployments with custom CDL components each version change can potentially break deployed system. Because of that each component type will have their own version number(different from CDL version). You'll have to look for version changes for each custom component type to determine if you can update CDL safely.</p>
<h2><a class="header" href="#release-schedule" id="release-schedule">Release schedule</a></h2>
<p>There is no strict release schedule at this point. New versions will be released when there is a need for them(e.g. new feature implemented).</p>
<h1><a class="header" href="#protocol-schema" id="protocol-schema">Protocol schema</a></h1>
<p>TODO</p>
<h1><a class="header" href="#benchmarks" id="benchmarks">Benchmarks</a></h1>
<p>TODO</p>
<h1><a class="header" href="#architecture" id="architecture">Architecture</a></h1>
<p>The CDL consists of six layers, each horizontally scalable and replaceable.</p>
<p><img src="architecture/../mdbook-plantuml-img/87efb07624224cd0aa4762b6316a85b372ef0656.svg" alt="" /></p>
<h2><a class="header" href="#management-layer" id="management-layer">Management Layer</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/cli.html">cdl-cli</a></td><td>Provides a command-line interface for managing schemas in the schema registry and storing and retrieving data</td></tr>
<tr><td><a href="architecture/web_admin.html">web-admin</a></td><td>Admin Web Panel - provides GUI interface for managing schemas and storing and retrieving data</td></tr>
</tbody></table>
<h2><a class="header" href="#graphql-api" id="graphql-api">GraphQL API</a></h2>
<p><a href="architecture/api.html">API</a> - used as a backend service for web-admin, provides unified interface to manage CDL.</p>
<h2><a class="header" href="#configuration-layer" id="configuration-layer">Configuration Layer</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/./edge_registry.html">edge-registry</a></td><td>Store and manage schema and object relations (for materialization purposes)</td></tr>
<tr><td><a href="architecture/schema_registry.html">schema-registry</a></td><td>Manage user-defined schemas that define the format of incoming values and their respective topics</td></tr>
<tr><td><a href="architecture/leader_elector.html">leader-elector</a></td><td>Elect master nodes in replicated services (<em>only for the Schema Repository, currently</em>)</td></tr>
</tbody></table>
<h2><a class="header" href="#ingestion-layer" id="ingestion-layer">Ingestion Layer</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/data_router.html">data-router</a></td><td>Route incoming data from and through MQ for consumption by the specific Command Service</td></tr>
</tbody></table>
<h2><a class="header" href="#storage-layer" id="storage-layer">Storage Layer</a></h2>
<p>Storage layer, which is sometimes called &quot;repository&quot;.</p>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/query_service.html">query-service</a></td><td>Wrap each individual database for retrieval of data</td></tr>
<tr><td><a href="architecture/command_service.html">command-service</a></td><td>Intake data from a MQ and storage, in specific database</td></tr>
<tr><td><a href="architecture/db_shrinker_storage.html">db-shrinker-storage</a></td><td>A service to remove older data from storage</td></tr>
</tbody></table>
<h2><a class="header" href="#materialization-layer" id="materialization-layer">Materialization Layer</a></h2>
<p>Internal layer which materializes views</p>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/object_builder.html">object-builder</a></td><td>Responsible for fetching data from various repositories and joining it together</td></tr>
<tr><td><a href="architecture/partial_update_engine.html">partial-update-engine</a></td><td>Responsible for reading command-service notifications and sending materialization requests to object builder</td></tr>
<tr><td><a href="architecture/materializer_general.html">materializer-general</a></td><td>Responsible for materializing data on the fly into database</td></tr>
<tr><td><a href="architecture/materializer_ondemand.html">materializer-ondemand</a></td><td>Responsible for materializing data on demand without saving results in any database</td></tr>
</tbody></table>
<h2><a class="header" href="#retrieval-layer" id="retrieval-layer">Retrieval Layer</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td><a href="architecture/query_router.html">query-router</a></td><td>Route incoming requests to query service based on schema id</td></tr>
</tbody></table>
<h2><a class="header" href="#additional-crates" id="additional-crates">Additional crates</a></h2>
<table><thead><tr><th>Crate Name</th><th>Purpose</th></tr></thead><tbody>
<tr><td>rpc</td><td>A collection of GRPC proto files and automatically generated client/server code.</td></tr>
<tr><td>utils</td><td>A collection of utilities used throughout the Common Data Layer</td></tr>
</tbody></table>
<h2><a class="header" href="#useful-directories" id="useful-directories">Useful directories</a></h2>
<table><thead><tr><th>Directory</th><th>Purpose</th></tr></thead><tbody>
<tr><td>deployment/helm</td><td>helm charts for kubernetes deployment</td></tr>
<tr><td>deployment/compose</td><td>sample deployment guide for docker (development-only)</td></tr>
<tr><td>xtask</td><td>utility tool to generate code from rpc proto schemas</td></tr>
<tr><td>benchmarking</td><td>scripts and scaffolding data for benchmarking</td></tr>
<tr><td>tests</td><td>component tests</td></tr>
<tr><td>examples</td><td>exemplary client of cdl</td></tr>
<tr><td>docs</td><td>cdl documentation</td></tr>
</tbody></table>
<h1><a class="header" href="#management-layer-1" id="management-layer-1">Management Layer</a></h1>
<p>Consists of services and tools responsible for manipulating and managing CDL.</p>
<p>CDL provides two different User Interfaces:</p>
<ul>
<li><a href="architecture/./cli.html">CLI</a></li>
<li><a href="architecture/./web_admin.html">Admin Web Panel</a></li>
</ul>
<h1><a class="header" href="#cli" id="cli">CLI</a></h1>
<h2><a class="header" href="#technical-description" id="technical-description">Technical Description</a></h2>
<p>The CDL-CLI is the official tool for interacting with the CDL's Schema Registry, used both for viewing and manipulating schemas and their respective data.</p>
<p>For this tool to work, please make sure that the Schema Registry's gRPC server is listening on a public port. Currently, the Schema Registry only exposes a gRPC API, which is faster than a JSON API but less convenient to use. There is some progress with a JSON API for convenience, as well as a TUI (terminal user interface) and a website.</p>
<p>Communication Methods:</p>
<ul>
<li>GRPC</li>
</ul>
<h2><a class="header" href="#how-to-guide" id="how-to-guide">How to guide</a></h2>
<p>For the sake of concision, though you will probably be running <code>cargo run --bin cdl -- &lt;options&gt;</code>,
this README will simply describe commands with the shorthand <code>cdl &lt;options&gt;</code>.</p>
<p><em>Note: This assumes you are running the common data layer locally for now. The ports for</em>
<em>schema registry and the storage service are copied from the <code>docker-compose.yml</code> file, but</em>
<em>if you are using different ports, you should provide those with the <code>--port</code> option.</em></p>
<h4><a class="header" href="#manipulate-views" id="manipulate-views">Manipulate Views</a></h4>
<p>To add a view under a schema already defined in the registry, run
<code>cdl schema views -s &lt;schema_name&gt; add -n &lt;view_name&gt; -v &lt;JMESPath_view&gt;</code>. Views can only be added
to schemas if they do not already exist on the schema; the <code>update</code> command can be used the same way
as <code>add</code> to update existing schema views.</p>
<p><em>Make sure that views are valid <a href="https://jmespath.org/">JMESPath</a> expressions.</em></p>
<p>To list all views of a schema alphabetically, run <code>cdl schema views -s &lt;schema_name&gt; names</code>.</p>
<p>To get a specific view on a schema, run <code>cdl schema views -s &lt;schema_name&gt; get -n &lt;view_name&gt;</code>.</p>
<h4><a class="header" href="#manipulate-schemas" id="manipulate-schemas">Manipulate Schemas</a></h4>
<h6><a class="header" href="#add-schema" id="add-schema">Add Schema</a></h6>
<p><code>cdl --registry-address &quot;http://localhost:6400 schema &lt;add|get|names|update&gt; --name &lt;schemaname&gt; \ --query-address &lt;query-service-uri&gt;&quot; \ --topic &lt;ingest-topic&gt; \ --file &lt;optional:schema-path&gt; </code></p>
<ul>
<li>If <code>--file</code> is provided, the specified file must have valid JSON inside.</li>
<li>If <code>--file</code> is missing, the CLI will expect JSON to be piped in over <code>stdin</code>.</li>
<li>A schema containing <code>true</code> will accept any valid JSON data.</li>
<li>New schemas are assigned a random UUID on creation, which will be printed after a successful insert.</li>
</ul>
<h6><a class="header" href="#list-schemas" id="list-schemas">List Schemas</a></h6>
<p>To print all existing schema names and their respective ID's:
<code>cdl --registry-address &quot;http://localhost:6400 schema names</code></p>
<h1><a class="header" href="#admin-web-panel" id="admin-web-panel">Admin Web Panel</a></h1>
<p>This is the management portal for the CDL, useful for updating
configuration, manipulating data.</p>
<h2><a class="header" href="#setup-1" id="setup-1">Setup</a></h2>
<p>This site is written with <a href="https://svelte.dev/">Svelte.JS</a> and <a href="https://www.typescriptlang.org/">TypeScript</a>.
To run or develop this site, you'll need to install <a href="https://npmjs.com/">NPM</a>:
I recommend using a version manager to install the latest version
like <a href="https://github.com/Schniz/fnm">fnm</a>, a Rust-based version manager for NPM.</p>
<p>Once you have NPM in your path, run <code>npm i</code> in this directory to
install all package dependencies.</p>
<h2><a class="header" href="#running" id="running">Running</a></h2>
<p>For development, the command <code>npm run dev</code> will run a dev server
on localhost:5000 (or a random port if 5000 is taken) which you
can access from your local browser.</p>
<h2><a class="header" href="#deployment-3" id="deployment-3">Deployment</a></h2>
<p>This site is deployed as a plain folder, specifically the <code>public</code>
folder in the root of this repo. Before deploying that folder, make
sure to run <code>npm run build</code> to build an optimized version of this
site and save it to the <code>public</code> directory.</p>
<h1><a class="header" href="#graphql-api-1" id="graphql-api-1">GraphQL API</a></h1>
<p>Server which provides <code>/graphql</code> and <code>/graphiql</code> routes for CDL management.
It is self-describing, interactive and easy to use way to manage your instance.</p>
<h1><a class="header" href="#getting-started-on-local-machine-via-docker-compose" id="getting-started-on-local-machine-via-docker-compose">Getting started on local machine (via docker-compose)</a></h1>
<p>Check our <a href="architecture/../deployment/local/docker-compose.html">guide</a> to see how to deploy API locally.</p>
<p>You can access interactive graphQL editor at http://localhost:50106/graphiql. It supports auto-completion, has built-in documentation explorer and history. </p>
<p>Because our schema-registry in docker-compose is automatically initialized with some schemas, you can start making queries right away, like:</p>
<pre><code class="language-graphql">{
    schemas {
      id,
      definitions {
        version,
        definition
      },
      views {
        expression
      }
    }
}
</code></pre>
<h3><a class="header" href="#configuration-environment-variables" id="configuration-environment-variables">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>SCHEMA_REGISTRY_ADDR</td><td>Address of schema registry gRPC API</td><td>http://schema_registry:50101</td><td>yes</td><td></td></tr>
<tr><td>EDGE_REGISTRY_ADDR</td><td>Address of edge registry gRPC API</td><td>http://edge_registry:50110</td><td>yes</td><td></td></tr>
<tr><td>ON_DEMAND_MATERIALIZER_ADDR</td><td>Address of on demand materializer gRPC API</td><td>http://materializer_ondemand:50108</td><td>yes</td><td></td></tr>
<tr><td>QUERY_ROUTER_ADDR</td><td>Address of query router gRPC API</td><td>http://query_router:50103</td><td>yes</td><td></td></tr>
<tr><td>COMMUNICATION_METHOD</td><td>The method of communication with external services</td><td><code>kafka</code> / <code>amqp</code> / <code>grpc</code></td><td>yes</td><td></td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration" id="kafka-configuration">Kafka Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address to Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>postgres_command</code></td><td>yes</td><td></td></tr>
<tr><td>REPORT_SOURCE</td><td>Kafka topic on which API listens for notifications</td><td><code>cdl.notifications</code></td><td>yes</td><td></td></tr>
<tr><td>INSERT_DESTINATION</td><td>Kafka topic to which API inserts new objects</td><td><code>cdl.data.input</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration" id="amqp-configuration">AMQP Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>postgres_command</code></td><td>yes</td><td></td></tr>
<tr><td>REPORT_SOURCE</td><td>AMQP queue on which API listens for notifications</td><td><code>cdl.notifications</code></td><td>yes</td><td></td></tr>
<tr><td>INSERT_DESTINATION</td><td>AMQP exchange to which API inserts new objects</td><td><code>cdl.data.input</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#grpc-configuration" id="grpc-configuration">gRPC Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>grpc</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INSERT_DESTINATION</td><td>gRPC service address on which API inserts new objects</td><td><code>http://data_router:50101</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h1><a class="header" href="#configuration-layer-1" id="configuration-layer-1">Configuration Layer</a></h1>
<p>Consists of services responsible for holding state and configuration of CDL.<br />
Currently only the Schema Registry resides here, which keeps information about schemas and views.</p>
<h1><a class="header" href="#schema-registry-1" id="schema-registry-1">Schema Registry</a></h1>
<h3><a class="header" href="#technical-description-1" id="technical-description-1">Technical Description</a></h3>
<p>The Schema Registry (<code>SR</code> for short) is responsible for storing configuration about the data types handled by CDL. It is a persistent graph database, that can be queried via gRPC (other means of interaction are in progress). Currently there is no GUI nor TUI; user interaction is currently performed with the <a href="architecture/cli.html">CDL-CLI</a>. Replication across multiple instances of the Schema Registry is supported.</p>
<p>Interacts with:</p>
<ul>
<li>nothing on its own</li>
</ul>
<p>Is used by:</p>
<ul>
<li>Data Router</li>
<li>Query Router</li>
<li>cdl-cli</li>
</ul>
<p>Query methods:</p>
<ul>
<li>gRPC (clients may use cdl-cli CLI application)</li>
</ul>
<p>Communication methods (supported repositores):</p>
<ul>
<li>Kafka (with other schema-registry instances)</li>
</ul>
<h3><a class="header" href="#configuration-environment-variables-1" id="configuration-environment-variables-1">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>COMMUNICATION_METHOD</td><td>The method of communication with external services</td><td><code>kafka</code> / <code>amqp</code> / <code>grpc</code></td><td>yes</td><td></td></tr>
<tr><td>REPLICATION_ROLE</td><td>(deprecated)</td><td><code>master</code> / <code>slave</code> / <code>none</code></td><td>yes</td><td></td></tr>
<tr><td>DB_NAME</td><td>Database name</td><td><code>schema-registry</code></td><td>yes</td><td></td></tr>
<tr><td>POD_NAME</td><td>(deprecated) used to promote to <code>master</code> role</td><td><code>schema1</code></td><td>no</td><td></td></tr>
<tr><td>EXPORT_DIR</td><td>Directory to save state of the database. The state is saved in newly created folder with timestamp</td><td><code>/var/db</code></td><td>no</td><td></td></tr>
<tr><td>IMPORT_FILE</td><td>JSON file from which SR should load initial state. If the state already exists this env variable will be ignored</td><td><code>/var/db/initial-schema.json</code></td><td>no</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>58105</td><td>no</td><td>58105</td></tr>
<tr><td>STATUS_PORT</td><td>Port exposing status of the application</td><td>3000</td><td>no</td><td>3000</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration-1" id="kafka-configuration-1">Kafka Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address of Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>schema_registry</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration-1" id="amqp-configuration-1">AMQP Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>schema_registry</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#replication-configuration" id="replication-configuration">Replication Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> does NOT equal <code>grpc</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>REPLICATION_SOURCE</td><td>Kafka topic/AMQP queue</td><td><code>cdl.schema_registry.internal</code></td><td>yes</td><td></td></tr>
<tr><td>REPLICATION_DESTINATION</td><td>Kafka topic/AMQP exchange</td><td><code>cdl.schema_registry.internal</code></td><td>yes</td><td></td></tr>
</tbody></table>
<p>Mind that GRPC uses HTTP2 as its transport protocol (L4), so SCHEMA_REGISTRY_ADDR must be provided as <code>http://ip_or_name:port</code></p>
<h1><a class="header" href="#leader-elector" id="leader-elector">Leader Elector</a></h1>
<p>TODO</p>
<h1><a class="header" href="#edge-registry" id="edge-registry">Edge registry</a></h1>
<h3><a class="header" href="#technical-description-2" id="technical-description-2">Technical Description</a></h3>
<p>Registry is responsible for storage of relations between schemas and objects.</p>
<h3><a class="header" href="#communication" id="communication">Communication</a></h3>
<p>There are two methods of communicating with <code>ER</code> - gRPC and MessageQueue (RabbitMQ and Kafka are supported in this place).</p>
<h4><a class="header" href="#grpc-communication" id="grpc-communication">gRPC communication</a></h4>
<p>GRPC communication allows to access whole feature set of <code>ER</code> and is required for querying.
List of available commands can be found in registry's <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/crates/rpc/proto">proto file</a>.</p>
<h4><a class="header" href="#message-queue-communication" id="message-queue-communication">Message queue communication</a></h4>
<p>MQ currently serves as  an alternative means of ingestion for object relation data (called <code>edge</code> within registry).
Messages must follow JSON Schema:</p>
<pre><code class="language-json">{
  &quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;,
  &quot;type&quot;: &quot;array&quot;,
  &quot;items&quot;: [
    {
      &quot;type&quot;: &quot;object&quot;,
      &quot;properties&quot;: {
        &quot;relation_id&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;pattern&quot;: &quot;[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}&quot;
        },
        &quot;parent_object_id&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;pattern&quot;: &quot;[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}&quot;
        },
        &quot;child_object_ids&quot;: {
          &quot;type&quot;: &quot;array&quot;,
          &quot;items&quot;: [
            {
              &quot;type&quot;: &quot;string&quot;,
              &quot;pattern&quot;: &quot;[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}&quot;
            }
          ]
        }
      },
      &quot;required&quot;: [
        &quot;relation_id&quot;,
        &quot;parent_object_id&quot;,
        &quot;child_object_ids&quot;
      ]
    }
  ]
}
</code></pre>
<p>eg.:</p>
<pre><code class="language-json">[
    {
      &quot;relation_id&quot;: &quot;4d987502-8800-11eb-b5cb-0242ac130003&quot;,
      &quot;parent_object_id&quot;: &quot;79bbc2d5-92a6-43ad-b182-d6b9dd49184c&quot;,
      &quot;child_object_ids&quot;: [
        &quot;627f84c7-d9f0-4665-b54d-2fcb5422ce02&quot;, 
        &quot;627f84c7-d9f0-4665-b54d-2fcb5422ce03&quot;
      ]
    }
]
</code></pre>
<p>Each entry in top level array represents one-to-many relation within <code>relation_id</code>. 
Such <code>relation_id</code> should be added beforehand, via gRPC api, between objects schemas.
<code>ER</code> at this time does not validate correctness of inserted data, so it's up to user to ensure that <code>edges</code> and <code>relations</code> are configured properly.</p>
<h3><a class="header" href="#configuration-environment-variables-2" id="configuration-environment-variables-2">Configuration (Environment variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>POSTGRES_USERNAME</td><td></td><td>postgres</td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PASSWORD</td><td></td><td>1234qwer</td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_HOST</td><td></td><td>192.168.0.42</td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PORT</td><td></td><td>5432</td><td>no</td><td>5432</td></tr>
<tr><td>POSTGRES_DBNAME</td><td></td><td>postgres</td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_SCHEMA</td><td></td><td>cdl</td><td>no</td><td>postgres</td></tr>
<tr><td>RPC_PORT</td><td>gRPC server port</td><td>50110</td><td>no</td><td>50110</td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus metrics</td><td>58105</td><td>no</td><td>58105</td></tr>
<tr><td>STATUS_PORT</td><td>Port exposing status of the application</td><td>3000</td><td>no</td><td>3000</td></tr>
<tr><td>CONSUMER_METHOD</td><td>MQ ingestion method, can be <code>kafka</code> or <code>rabbitmq</code></td><td>kafka</td><td>yes</td><td></td></tr>
<tr><td>CONSUMER_HOST</td><td>Kafka broker or RabbitMQ host</td><td>192.168.0.51:9092</td><td>yes</td><td></td></tr>
<tr><td>CONSUMER_TAG</td><td>Kafka group_id or RabbitMQ tag</td><td>cdl_edge_registry</td><td>yes</td><td></td></tr>
<tr><td>CONSUMER_SOURCE</td><td>Kafka topic or RabbitMQ queue</td><td>cdl.egde.input</td><td>yes</td><td></td></tr>
</tbody></table>
<h1><a class="header" href="#ingestion-layer-1" id="ingestion-layer-1">Ingestion Layer</a></h1>
<p>Services in this layer are responsible for accepting generic messages from external systems via a message queue, validating them and and forwarding the message to correct repository.<br />
Currently consists only of the <a href="architecture/data_router.html">Data Router</a>. The <a href="architecture/data_router.html">Data Router</a> accepts messages in the following format:</p>
<pre><code class="language-json">{
  &quot;schemaId&quot;: &quot;ca435cee-2944-41f7-94ff-d1b26e99ba48&quot;,
  &quot;objectId&quot;: &quot;fc0b95e1-07eb-4bf8-b691-1a85a49ef8f0&quot;,
  &quot;data&quot;: { ...valid json object }
}
</code></pre>
<p>For more details, see the Data Router's <a href="architecture/data_router.html">readme</a>.</p>
<h1><a class="header" href="#data-router" id="data-router">Data Router</a></h1>
<h3><a class="header" href="#technical-description-3" id="technical-description-3">Technical Description</a></h3>
<p>The data router (internally <code>DR</code> is also used) is responsible for taking in input data and routing it to the correct storage based on 
the data's schema and its associated topic. </p>
<h3><a class="header" href="#communication-1" id="communication-1">Communication</a></h3>
<p>The data router routes requests from RabbitMQ and Kafka to the correct storage solution based on the schema and data type.
Topic and some of the basic configuration is obtained from Schema Registry. Data are routed and deposited onto configured queues.</p>
<p>Interacts with:</p>
<ul>
<li>Command Service (optional, either)</li>
<li>Message Queue (optional, either)</li>
<li>Schema Registry</li>
</ul>
<p>Ingest methods:</p>
<ul>
<li>Kafka</li>
</ul>
<p>Internal communication methods:</p>
<ul>
<li>Kafka (command-service)</li>
<li>gRPC (schema-registry)</li>
</ul>
<p>Below are the example data required by data router:</p>
<pre><code># high level description
{
    &quot;schemaId&quot;: &lt;UUID&gt;,
    &quot;objectId&quot;: &lt;UUID&gt;,
    &quot;data&quot;: { &quot;some_property&quot;: &quot;object&quot;}
}

# type description
{
    &quot;objectId&quot;(string) : (128bit valid uuid),
    &quot;schemaID&quot;(string) : (128bit valid uuid),
    &quot;data&quot;(string) : (array,dict,object,string, literally anything),
}

# example, minimalistic one liner
{ &quot;objectId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;schemaId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;data&quot;: {} }
</code></pre>
<p>Messages can be batched together, however please mind, that batched messages works best when used with the same schemaId.
Otherwise, messages will be split into sub-batches containing messages with the same schemaId</p>
<pre><code>[
  { &quot;objectId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;schemaId&quot;: f79d7ebd-4260-4919-9ba3-45ea6701f065, &quot;data&quot;: {} }
  { &quot;objectId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;schemaId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;data&quot;: {} }
  { &quot;objectId&quot;: 0369de4f-8025-4cf8-b6df-9446b51e4fd0, &quot;schemaId&quot;: 9056c0b3-2ceb-42a6-a6b6-9718c3e273bc, &quot;data&quot;: {} }
  { &quot;objectId&quot;: 0369de4f-8025-4cf8-b6df-9446b51e4fd0, &quot;schemaId&quot;: 07087162-e499-48f1-ad4a-cee7e77f1965, &quot;data&quot;: {} }
]
</code></pre>
<p>Please mind that internally, each message will get its own timestamp, with which data started being processed by CDL. This information is invisible for user.</p>
<h3><a class="header" href="#configuration-environment-variables-3" id="configuration-environment-variables-3">Configuration (Environment Variables)</a></h3>
<p>To configure the Data Router, set the following environment variables:</p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>COMMUNICATION_METHOD</td><td>The method of communication with external services</td><td><code>kafka</code> / <code>amqp</code> / <code>grpc</code></td><td>yes</td><td></td></tr>
<tr><td>INPUT_SOURCE</td><td>Kafka topic or AMQP queue</td><td><code>cdl.data.input</code></td><td>no, when <code>grpc</code> has been chosen</td><td></td></tr>
<tr><td>SCHEMA_REGISTRY_ADDR</td><td>Address of schema registry gRPC API</td><td>http://schema_registry:50101</td><td>yes</td><td></td></tr>
<tr><td>CACHE_CAPACITY</td><td>How many entries the cache can hold</td><td>1024</td><td>yes</td><td></td></tr>
<tr><td>TASK_LIMIT</td><td>Max requests handled in parallel</td><td>128</td><td>yes</td><td>128</td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>51805</td><td>no</td><td>51805</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration-2" id="kafka-configuration-2">Kafka Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address of Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>data_router</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration-2" id="amqp-configuration-2">AMQP Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>data_router</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#grpc-configuration-1" id="grpc-configuration-1">gRPC Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>grpc</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>GRPC_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
</tbody></table>
<p>Mind that GRPC uses HTTP2 as its transport protocol (L4), so SCHEMA_REGISTRY_ADDR must be provided as <code>http://ip_or_name:port</code></p>
<p>See an example <a href="architecture/../deployment/index.html">configuration</a> of deployment of data router and other services.</p>
<h1><a class="header" href="#storage-layer-1" id="storage-layer-1">Storage Layer</a></h1>
<p>Consists of repositories for storing data.</p>
<p>Currently we support 2 types of repositories:</p>
<ul>
<li>Document
<ul>
<li>PostgreSQL</li>
</ul>
</li>
<li>Timeseries
<ul>
<li>Druid</li>
<li>Victoria Metrics</li>
</ul>
</li>
</ul>
<h1><a class="header" href="#command-services" id="command-services">Command Services</a></h1>
<p>Services that translate messages received from the <a href="architecture/data_router.html">Data Router</a> into their respective database's format. Currently only one Command Service implementation exists
and is built in such way that it can support multiple databases (one at a time).</p>
<h3><a class="header" href="#technical-description-4" id="technical-description-4">Technical Description</a></h3>
<p>The Command-Service (commonly refered also as <code>CS</code>, or <code>CSPG</code> - indicating posgres instance), interfaces storage repositories with the CDL ecosystem.</p>
<p>Interacts with:</p>
<ul>
<li>Data Router (optional, either)</li>
<li>Message Queue (optional, either)</li>
<li>Supported Repository (one of)</li>
</ul>
<p>Ingest methods:</p>
<ul>
<li>Kafka</li>
<li>RabbitMq</li>
<li>GRPC (currently either only one instance without kubernetes)</li>
</ul>
<p>Egest methods (supported repositories):</p>
<ul>
<li>Postgresql (tested on 12, should support anything &gt;=9, advised 13)</li>
<li>VictoriaMetrics</li>
<li>Druid</li>
<li>Sleight (CDL's document storage)</li>
<li>Troika (CDL's binary data repo)</li>
<li>.. or anything with matching GRPC :)</li>
</ul>
<h3><a class="header" href="#configuration-environment-variables-4" id="configuration-environment-variables-4">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>COMMUNICATION_METHOD</td><td>The method of communication with external services</td><td><code>kafka</code> / <code>amqp</code> / <code>grpc</code></td><td>yes</td><td></td></tr>
<tr><td>REPORT_DESTINATION</td><td>Kafka topic/AMQP exchange/callback URL to send notifications to (reporting disabled when empty)</td><td><code>cdl.notifications</code></td><td>no</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>51805</td><td>no</td><td>51805</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#postgres-configuration" id="postgres-configuration">Postgres Configuration</a></h4>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>POSTGRES_USERNAME</td><td>Username</td><td><code>cdl</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PASSWORD</td><td>Password</td><td><code>cdl1234</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_HOST</td><td>Host of the server</td><td><code>127.0.0.1</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PORT</td><td>Port on which the server listens</td><td>5432</td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_DBNAME</td><td>Database name</td><td><code>cdl</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_SCHEMA</td><td>SQL Schema available for service</td><td><code>cdl</code></td><td>no</td><td><code>public</code></td></tr>
</tbody></table>
<h4><a class="header" href="#druid-configuration" id="druid-configuration">Druid Configuration</a></h4>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>DRUID_OUTPUT_BROKERS</td><td>Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>DRUID_OUTPUT_TOPIC</td><td>Kafka topic</td><td><code>cdl.timeseries.internal.druid</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#victoria-metrics-configuration" id="victoria-metrics-configuration">Victoria Metrics Configuration</a></h4>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>VICTORIA_METRICS_OUTPUT_URL</td><td>Address of Victoria Metrics</td><td><code>http://victoria_metrics:8428</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration-3" id="kafka-configuration-3">Kafka Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address of Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>postgres_command</code></td><td>yes</td><td></td></tr>
<tr><td>ORDERED_SOURCES</td><td>Topics with ordered messages</td><td><code>cdl.timeseries.vm.1.data</code></td><td>no, but one of <code>ORDERED_SOURCES</code> and <code>UNORDERED_SOURCES</code> has to be present</td><td></td></tr>
<tr><td>UNORDERED_SOURCES</td><td>Topics with unordered messages</td><td><code>cdl.timeseries.vm.2.data</code></td><td>no, but one of <code>ORDERED_SOURCES</code> and <code>UNORDERED_SOURCES</code> has to be present</td><td></td></tr>
<tr><td>TASK_LIMIT</td><td>Max requests handled in parallel</td><td>32</td><td>yes</td><td>32</td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration-3" id="amqp-configuration-3">AMQP Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>postgres_command</code></td><td>yes</td><td></td></tr>
<tr><td>ORDERED_SOURCES</td><td>Queues with ordered messages</td><td><code>cdl.timeseries.vm.1.data</code></td><td>no, but one of <code>ORDERED_SOURCES</code> and <code>UNORDERED_SOURCES</code> has to be present</td><td></td></tr>
<tr><td>UNORDERED_SOURCES</td><td>Queues with unordered messages</td><td><code>cdl.timeseries.vm.2.data</code></td><td>no, but one of <code>ORDERED_SOURCES</code> and <code>UNORDERED_SOURCES</code> has to be present</td><td></td></tr>
<tr><td>TASK_LIMIT</td><td>Max requests handled in parallel</td><td>32</td><td>yes</td><td>32</td></tr>
</tbody></table>
<h4><a class="header" href="#grpc-configuration-2" id="grpc-configuration-2">gRPC Configuration</a></h4>
<p><em>(if <code>COMMUNICATION_METHOD</code> equals <code>grpc</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>GRPC_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>REPORT_ENDPOINT_URL</td><td>URL to send notifications to</td><td><code>notifications:50102</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h1><a class="header" href="#db-shrinker-storage" id="db-shrinker-storage">Db Shrinker Storage</a></h1>
<h2><a class="header" href="#usage" id="usage">Usage</a></h2>
<p><code>db-shrinker-postgres &lt;connection-string&gt;</code></p>
<p>eg.</p>
<p><code>db-shrinker-postgres 'postgresql://postgres:1234@localhost:5432/postgres'</code></p>
<h2><a class="header" href="#description" id="description">Description</a></h2>
<p>This binary merges all versions of documents stored in PostgreSQL into one, 'most recent' version.
It handles whole and partial updates to documents in mention.</p>
<h2><a class="header" href="#testing" id="testing">Testing</a></h2>
<p>Currently only manual testing is supported.
You must have local postgres database provisioned for CDL document repository.</p>
<p>Setting up python env is done via <code>pip install -r tests/requirements.txt</code>.</p>
<p>Running <code>python data_loader.py</code> from tests directory should load sample data to your db. Just make sure that PSQL
connection string located in that file refers to your database instance.</p>
<p>After that you can run <code>db-shirnker-postgres</code> with same postgres connection string and compare data changed in your
database with <code>expected</code> entry in each test case.</p>
<h1><a class="header" href="#query-service" id="query-service">Query Service</a></h1>
<p>Each Query Service serves a common set of queries, and translates those into their respective database's query language.
Two query-services are present: one for timeseries databases, and one for documents.</p>
<h3><a class="header" href="#technical-description-5" id="technical-description-5">Technical Description</a></h3>
<p>The query service (<code>QS</code> or for example for postgresql its <code>QSPG</code>), is responsible for querying data from specific repository. It offers two paths that can be accessed:
First path depends on type of repo</p>
<h3><a class="header" href="#communication-2" id="communication-2">Communication</a></h3>
<p>Communication to query service is done through <a href="https://grpc.io/docs/what-is-grpc/introduction/">gRPC</a> based on two <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/crates/rpc/proto">endpoints</a> of querying for data by <code>SCHEMA_ID</code> or multiple <code>OBJECT_ID</code>s. Query service communicates with multiple databases such as postgresql, druid, victoria metrics. Query service also communicates with <a href="architecture/schema_registry.html">schema registry</a>. </p>
<p>Interacts with:</p>
<ul>
<li>Druid</li>
<li>Postgresql</li>
<li>VictoriaMetrics (accidentally also Prometheus)</li>
<li>Sled</li>
<li>Troika</li>
<li>.. any similar grpc-able repo</li>
</ul>
<p>Query methods:</p>
<ul>
<li>GRPC (req-response)</li>
</ul>
<p>Communication protocols:</p>
<ul>
<li>database specific</li>
</ul>
<h3><a class="header" href="#configuration-environment-variables-5" id="configuration-environment-variables-5">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>51805</td><td>no</td><td>51805</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#postgres-configuration-1" id="postgres-configuration-1">Postgres Configuration</a></h4>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>POSTGRES_USERNAME</td><td>Username</td><td><code>cdl</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PASSWORD</td><td>Password</td><td><code>cdl1234</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_HOST</td><td>Host of the server</td><td><code>127.0.0.1</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_PORT</td><td>Port on which the server listens</td><td>5432</td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_DBNAME</td><td>Database name</td><td><code>cdl</code></td><td>yes</td><td></td></tr>
<tr><td>POSTGRES_SCHEMA</td><td>SQL Schema available for service</td><td><code>cdl</code></td><td>no</td><td><code>public</code></td></tr>
</tbody></table>
<p>See an example <a href="architecture/../deployment/index.html">configuration</a> of deployment of data router and other services. </p>
<h1><a class="header" href="#object-builder" id="object-builder">Object builder</a></h1>
<h3><a class="header" href="#technical-description-6" id="technical-description-6">Technical Description</a></h3>
<p>Object builder responsibility is creating complex objects according to recipes - view definitions. </p>
<p>Object Builder Loop:</p>
<ul>
<li>Wait for request to build a view </li>
<li>Fetch view definition from schema registry</li>
<li>Fetch objects from repositories</li>
<li>Perform filtering by custom fields, join operations </li>
<li>Send data to materializers component or to the requesting party</li>
</ul>
<p>It is important to note that object builder output contains view id, change list received from partial update engine, and requested objects with information how they were created (each returned object contains ids of every object which was used for its creation). </p>
<h3><a class="header" href="#communication-3" id="communication-3">Communication</a></h3>
<p>There are two methods of communicating with <code>OB</code> - gRPC and MessageQueue (RabbitMQ and Kafka are supported in this place).</p>
<h4><a class="header" href="#grpc-communication-1" id="grpc-communication-1">gRPC communication</a></h4>
<p>gRPC communication allows to materialize view on demand. Materialized view is not saved in any database, but sent as a response via gRPC.</p>
<h4><a class="header" href="#message-queue-communication-1" id="message-queue-communication-1">Message queue communication</a></h4>
<p>MQ currently serves as a main method of ingestion for view that needs to be materialized in database.
Messages payload are just UUIDs of the view that needs to be updated/created. There is no JSON encoding.</p>
<p>eg.:</p>
<pre><code>627f84c7-d9f0-4665-b54d-2fcb5422ce02
</code></pre>
<h3><a class="header" href="#configuration-environment-variables-6" id="configuration-environment-variables-6">Configuration (Environment variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>gRPC server port</td><td>50110</td><td>yes</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus metrics</td><td>58105</td><td>no</td><td>58105</td></tr>
<tr><td>STATUS_PORT</td><td>Port exposing status of the application</td><td>3000</td><td>no</td><td>3000</td></tr>
<tr><td>MQ_METHOD</td><td>MQ ingestion method, can be <code>kafka</code> or <code>rabbitmq</code></td><td>kafka</td><td>no</td><td></td></tr>
<tr><td>SCHEMA_REGISTRY_ADDR</td><td>Address of schema registry gRPC API</td><td>http://schema_registry:50101</td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#kafka-configuration-4" id="kafka-configuration-4">Kafka Configuration</a></h4>
<p><em>(if <code>MQ_METHOD</code> equals <code>kafka</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>KAFKA_BROKERS</td><td>Address of Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td></td></tr>
<tr><td>KAFKA_GROUP_ID</td><td>Group ID of the consumer</td><td><code>schema_registry</code></td><td>yes</td><td></td></tr>
<tr><td>MQ_SOURCE</td><td>Topic</td><td><code>cdl.materialization</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h4><a class="header" href="#amqp-configuration-4" id="amqp-configuration-4">AMQP Configuration</a></h4>
<p><em>(if <code>MQ_METHOD</code> equals <code>amqp</code>)</em></p>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>AMQP_CONNECTION_STRING</td><td>Connection URL to AMQP Server</td><td><code>amqp://user:CHANGEME@rabbitmq:5672/%2f</code></td><td>yes</td><td></td></tr>
<tr><td>AMQP_CONSUMER_TAG</td><td>Consumer tag</td><td><code>schema_registry</code></td><td>yes</td><td></td></tr>
<tr><td>MQ_SOURCE</td><td>Queue name</td><td><code>cdl.materialization</code></td><td>yes</td><td></td></tr>
</tbody></table>
<h1><a class="header" href="#partial-update-engine" id="partial-update-engine">Partial Update Engine</a></h1>
<h2><a class="header" href="#configuration-environment-variables-7" id="configuration-environment-variables-7">Configuration (Environment variables)</a></h2>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>kafka_brokers</td><td>Address of Kafka brokers</td><td><code>kafka:9093</code></td><td>yes</td><td>no</td></tr>
<tr><td>kafka_group_id</td><td>Group ID of the consumer</td><td><code>pue</code></td><td>yes</td><td>no</td></tr>
<tr><td>notification_topic</td><td>Kafka topic for notifications</td><td><code>3000</code></td><td>yes</td><td>no</td></tr>
<tr><td>schema_registry_addr</td><td>Address of schema registry gRPC API</td><td><code>http://schema_registry:50101</code></td><td>yes</td><td>no</td></tr>
<tr><td>metrics_port</td><td>Port to listen on for Prometheus requests</td><td><code>13456</code></td><td>no(default)</td><td><code>58105</code></td></tr>
<tr><td>sleep_phase_length</td><td>Duration of sleep phase in seconds</td><td><code>666</code></td><td>yes</td><td>no</td></tr>
</tbody></table>
<h1><a class="header" href="#materializer---general" id="materializer---general">Materializer - General</a></h1>
<h2><a class="header" href="#configuration-environment-variables-8" id="configuration-environment-variables-8">Configuration (Environment variables)</a></h2>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>gRPC server port</td><td><code>50110</code></td><td>yes</td><td>no</td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus metrics</td><td><code>58105</code></td><td>no(default)</td><td>58105</td></tr>
<tr><td>STATUS_PORT</td><td>Port exposing status of the application</td><td><code>3000</code></td><td>no(default)</td><td>3000</td></tr>
<tr><td>OBJECT_BUILDER_ADDR</td><td>Address of object builder (grpc)</td><td><code>http://objectbuilder:50101</code></td><td>yes</td><td>no</td></tr>
<tr><td>MATERIALIZER</td><td>Type of materializer being used</td><td><code>postgres</code></td><td>yes</td><td>no</td></tr>
</tbody></table>
<h3><a class="header" href="#configuration-for-postgres-materializer" id="configuration-for-postgres-materializer">Configuration for Postgres Materializer</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>POSTGRES_USERNAME</td><td>Postgres Username</td><td><code>postgres</code></td><td>yes</td><td>no</td></tr>
<tr><td>POSTGRES_PASSWORD</td><td>Postgres Password</td><td><code>P422w0rd</code></td><td>yes</td><td>no</td></tr>
<tr><td>POSTGRES_PORT</td><td>Postgres Port</td><td><code>5432</code></td><td>no(default)</td><td>5432</td></tr>
<tr><td>POSTGRES_DBNAME</td><td>Postgres Database Name</td><td><code>cdl</code></td><td>yes</td><td>no</td></tr>
<tr><td>POSTGRES_SCHEMA</td><td>Postgres Schema Name</td><td><code>public</code></td><td>yes</td><td>public</td></tr>
</tbody></table>
<h1><a class="header" href="#materializer---on-demand" id="materializer---on-demand">Materializer - On Demand</a></h1>
<h2><a class="header" href="#configuration-environment-variables-9" id="configuration-environment-variables-9">Configuration (Environment variables)</a></h2>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>gRPC server port</td><td>50110</td><td>yes</td><td>no</td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus metrics</td><td>58105</td><td>no(default)</td><td>58105</td></tr>
<tr><td>STATUS_PORT</td><td>Port exposing status of the application</td><td>3000</td><td>no(default)</td><td>3000</td></tr>
<tr><td>OBJECT_BUILDER_ADDR</td><td>Address of object builder (grpc)</td><td>http://objectbuilder:50101</td><td>yes</td><td>no</td></tr>
</tbody></table>
<h1><a class="header" href="#retrieval-layer-1" id="retrieval-layer-1">Retrieval Layer</a></h1>
<p>Services in this layer are responsible for responding on queries from external systems via REST.
Currently consists only of the <a href="architecture/query_router.html">Query Router</a>. </p>
<p>TODO: Query router JSON format.</p>
<h1><a class="header" href="#query-router" id="query-router">Query Router</a></h1>
<h3><a class="header" href="#technical-description-7" id="technical-description-7">Technical Description</a></h3>
<p>The Query Router (<code>QR</code>), is responsible for forwarding requests to specific query services. In CDL messages can be stored in any available repository, data router acts as a single entry point to multi-repo system and query router allows that data to be fetched easily.
Query Router first queries SR, then basing on received config, finds out specific QS that, hopefully, should be able to respond to specific query. Logic of that process is based on repo_type and query-service address stored with schema itself.</p>
<h3><a class="header" href="#communication-4" id="communication-4">Communication</a></h3>
<p>Interacts with:</p>
<ul>
<li>Query Service</li>
<li>Schema Registry</li>
</ul>
<p>Query methods:</p>
<ul>
<li>REST (request-response)</li>
</ul>
<p>Communication protocols:</p>
<ul>
<li>gRPC with query-services (request-response)</li>
<li>gRPC with schema-registry (request-response)</li>
</ul>
<h3><a class="header" href="#configuration-environment-variables-10" id="configuration-environment-variables-10">Configuration (Environment Variables)</a></h3>
<table><thead><tr><th>Name</th><th>Short Description</th><th>Example</th><th>Mandatory</th><th>Default</th></tr></thead><tbody>
<tr><td>INPUT_PORT</td><td>Port to listen on</td><td>50103</td><td>yes</td><td></td></tr>
<tr><td>SCHEMA_REGISTRY_ADDR</td><td>Address of schema registry gRPC API</td><td>http://schema_registry:50101</td><td>yes</td><td></td></tr>
<tr><td>CACHE_CAPACITY</td><td>How many entries the cache can hold</td><td>1024</td><td>yes</td><td></td></tr>
<tr><td>METRICS_PORT</td><td>Port to listen on for Prometheus requests</td><td>51805</td><td>no</td><td>51805</td></tr>
<tr><td>RUST_LOG</td><td>Log level</td><td><code>trace</code></td><td>no</td><td></td></tr>
</tbody></table>
<h2><a class="header" href="#running-1" id="running-1">Running</a></h2>
<p>To run the <strong>query-router</strong> requires the <a href="architecture/schema_registry.html">Schema Registry</a> to be running and the <a href="architecture/query_service.html">Query Services</a> or the <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/crates/query-service-ts">Timeseries Query Services</a> connected to their respective repositories.</p>
<p><em>Note: Currently, the cache is valid forever: changing a schema's <strong>query-service</strong> address will not update in the <strong>query-router</strong>.</em></p>
<h2><a class="header" href="#functionality" id="functionality">Functionality</a></h2>
<p>REST API specification is available in <a href="https://github.com/epiphany-platform/CommonDataLayer/blob/develop/crates/query-router/api.yml">OpenAPI 3.0 spec</a>.</p>
<p>Currently, the <strong>query-router</strong> can:</p>
<ul>
<li>handle querying data by ID from document repositories,</li>
<li>query range of data by ID from time series repositories,</li>
<li>query data from repositories by SCHEMA_ID.</li>
</ul>
<p>Rough sketch of working process:
<img src="architecture/../mdbook-plantuml-img/9208ca32e9197d5d57535abc78a61451de003302.svg" alt="" /></p>
<h1><a class="header" href="#utils" id="utils">Utils</a></h1>
<h1><a class="header" href="#cdl-configuration-wip" id="cdl-configuration-wip">CDL configuration (WIP)</a></h1>
<p>Each application accepts a configuration file in <code>.toml</code> format.
Example configuration is present in this directory:</p>
<ul>
<li><a href="configuration/./api.html">api</a></li>
<li><a href="configuration/./command-service.html">command-service</a></li>
<li><a href="configuration/./data-router.html">data-router</a></li>
<li><a href="configuration/./edge-registry.html">edge-registry</a></li>
<li><a href="configuration/./materializer-general.html">materializer-general</a></li>
<li><a href="configuration/./materializer-ondemand.html">materializer-ondemand</a></li>
<li><a href="configuration/./object-builder.html">object-builder</a></li>
<li><a href="configuration/./partial-update-engine.html">partial-update-engine</a></li>
<li><a href="configuration/./query-router.html">query-router</a></li>
<li><a href="configuration/./query-service.html">query-service</a></li>
<li><a href="configuration/./query-service-ts.html">query-service-ts</a></li>
<li><a href="configuration/./schema-registry.html">schema-registry</a></li>
</ul>
<p>Configuration is loaded in order:</p>
<pre><code>/etc/cdl/default.{ext}
/etc/cdl/{app-name}.{ext}
/etc/cdl/{env}/default.{ext}
/etc/cdl/{env}/{app-name}.{ext}

$HOME/.cdl/default.{ext}
$HOME/.cdl/{app-name}.{ext}
$HOME/.cdl/{env}/default.{ext}
$HOME/.cdl/{env}/{app-name}.{ext}

./.cdl/default.{ext}
./.cdl/{app-name}.{ext}
./.cdl/{env}/default.{ext}
./.cdl/{env}/{app-name}.{ext}

{custom}/default.{ext}
{custom}/{app-name}.{ext}
{custom}/{env}/default.{ext}
{custom}/{env}/{app-name}.{ext}

ENVs
</code></pre>
<p><code>{env}</code> is environment variable <code>ENVIRONMENT</code>; default value <code>development</code><br />
<code>{app-name}</code> is application name with dashes<br />
<code>{ext}</code> is file extension, currently only <code>.toml</code><br />
<code>{custom}</code> is <code>CDL_CONFIG</code> environment variable<br />
<code>ENVs</code> are app own environment variables</p>
<p>Configs are merged top to bottom, so value declared in <code>./</code> overwrites <code>/etc/</code> and so on.
Environment variables supersede every config file.</p>
<p><code>ENVs</code> are in format:</p>
<pre><code>{APP-NAME}_{SETTING-PATH}
</code></pre>
<p><code>SETTING-PATH</code> is structure path of each config option, separated by <code>__</code> - double underscore.</p>
<p>examples:</p>
<pre><code>DATA_ROUTER_KAFKA__BROKERS
COMMAND_SERVICE_AMQP__CONSUME_OPTIONS__NO_LOCAL
QUERY_SERVICE_REPOSITORY_KIND
</code></pre>
<pre><code class="language-toml">communication_method = &quot;kafka&quot;
input_port = 0
insert_destination = &quot;&quot;

[kafka]
brokers = &quot;&quot;
group_id = &quot;&quot;

[amqp]
exchange_url = &quot;&quot;
tag = &quot;&quot;

[services]
schema_registry_url = &quot;&quot;
edge_registry_url = &quot;&quot;
on_demand_materializer_url = &quot;&quot;
query_router_url = &quot;&quot;

[notification_consumer]
source = &quot;&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;api&quot;

[log]
rust_log = &quot;info,api=debug&quot;
</code></pre>
<pre><code class="language-toml">communication_method = &quot;kafka&quot;
repository_kind = &quot;postgres&quot;
async_task_limit = 32

[notifications]
enabled = true
destination = &quot;&quot;

[postgres]
username = &quot;&quot;
password = &quot;&quot;
host = &quot;&quot;
port = &quot;&quot;
dbname = &quot;&quot;
schema = &quot;&quot;

[victoria_metrics]
url = &quot;&quot;

[druid]
topic = &quot;&quot;

[kafka]
brokers = &quot;&quot;
group_id = &quot;&quot;

[amqp]
exchange_url = &quot;&quot;
tag = &quot;&quot;

[amqp.consume_options]
no_local = false
no_act = false
exclusive = false
nowait = false

[grpc]
address = &quot;&quot;

[listener]
ordered_sources = [&quot;&quot;]
unordered_sources = [&quot;&quot;]

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[log]
rust_log = &quot;info,command_service=debug&quot;

</code></pre>
<pre><code class="language-toml">communication_method = &quot;kafka&quot;
cache_capacity = 1000
async_task_limit = 32

[kafka]
brokers = &quot;&quot;
group_id = &quot;&quot;
ingest_topic = &quot;&quot;

[amqp]
exchange_url = &quot;&quot;
tag = &quot;&quot;
ingest_queue = &quot;&quot;

[amqp.consume_options]
no_local = false
no_act = false
exclusive = false
nowait = false

[grpc]
address = &quot;&quot;

[repositories]
key1 = { insert_destination = &quot;&quot;, query_address = &quot;&quot;, repository_type = &quot;DocumentStorage&quot; }
key2 = { insert_destination = &quot;&quot;, query_address = &quot;&quot;, repository_type = &quot;Timeseries&quot; }

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[services]
schema_registry_url = &quot;&quot;

[log]
rust_log = &quot;&quot;
</code></pre>
<pre><code class="language-toml">communication_method = &quot;kafka&quot;
input_port = 50110

[postgres]
username = &quot;&quot;
password = &quot;&quot;
host = &quot;&quot;
port = &quot;&quot;
dbname = &quot;&quot;
schema = &quot;&quot;

[kafka]
brokers = &quot;&quot;
ingest_topic = &quot;&quot;
group_id = &quot;&quot;

[amqp]
exchange_url = &quot;&quot;
tag = &quot;&quot;
ingest_queue = &quot;&quot;

[amqp.consume_options]
no_local = false
no_act = false
exclusive = false
nowait = false

[notifications]
destination = &quot;&quot;
enabled = true

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[log]
rust_log = &quot;info,edge_registry=debug&quot;
</code></pre>
<pre><code class="language-toml">communication_method = &quot;kafka&quot;
input_port = 50203
cache_capacity = 1024

[postgres]
username = &quot;&quot;
password = &quot;&quot;
host = &quot;&quot;
port = &quot;&quot;
dbname = &quot;&quot;
schema = &quot;&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[log]
rust_log = &quot;info,materializer_general=debug&quot;
</code></pre>
<pre><code class="language-toml">input_port = 50203

[services]
object_builder_url = &quot;&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[log]
rust_log = &quot;info,materializer_general=debug&quot;
</code></pre>
<pre><code class="language-toml">communication_method = &quot;kafka&quot;
input_port = 50107

[kafka]
brokers = &quot;&quot;
group_id = &quot;&quot;
ingest_topic = &quot;&quot;

[amqp]
exchange_url = &quot;&quot;
tag = &quot;&quot;
ingest_queue = &quot;&quot;

[amqp.consume_options]
no_local = false
no_act = false
exclusive = false
nowait = false

[services]
schema_registry_url = &quot;&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[log]
rust_log = &quot;info,object_builder=debug&quot;
</code></pre>
<pre><code class="language-toml">communication_method = &quot;kafka&quot;
sleep_phase_length = 1000

[notification_consumer]
brokers = &quot;&quot;
group_id = &quot;&quot;
source = &quot;&quot;

[kafka]
brokers = &quot;&quot;
egest_topic = &quot;&quot;

[services]
schema_registry_url = &quot;'&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[log]
rust_log = &quot;info,partial_update_engine=debug&quot;
</code></pre>
<pre><code class="language-toml">cache_capacity = 1000
input_port = 50103

[services]
schema_registry_url = &quot;&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[repositories]
key1 = { insert_destination = &quot;&quot;, query_address = &quot;&quot;, repository_type = &quot;DocumentStorage&quot; }
key2 = { insert_destination = &quot;&quot;, query_address = &quot;&quot;, repository_type = &quot;Timeseries&quot; }

[log]
rust_log = &quot;info,query_router=debug&quot;
</code></pre>
<pre><code class="language-toml">input_port = 50201

[postgres]
username = &quot;&quot;
password = &quot;&quot;
host = &quot;&quot;
port = &quot;&quot;
dbname = &quot;&quot;
schema = &quot;&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[log]
rust_log = &quot;info,query_service=debug&quot;
</code></pre>
<pre><code class="language-toml">repository_kind = &quot;&quot;
input_port = 50201

[druid]
url = &quot;&quot;
table_name = &quot;&quot;

[victoria_metrics]
url = &quot;&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;&quot;

[log]
rust_log = &quot;info,query_service_ts=debug&quot;
</code></pre>
<pre><code class="language-toml">communication_method = &quot;kafka&quot;
input_port = 50101
import_file = &quot;&quot;
export_dir = &quot;&quot;

[postgres]
username = &quot;&quot;
password = &quot;&quot;
host = &quot;&quot;
port = &quot;&quot;
dbname = &quot;&quot;
schema = &quot;&quot;

[kafka]
brokers = &quot;&quot;

[amqp]
exchange_url = &quot;&quot;

[monitoring]
metrics_port = 0
status_port = 0
otel_service_name = &quot;schema-registry&quot;

[log]
rust_log = &quot;info,schema_registry=debug&quot;
</code></pre>
<h1><a class="header" href="#front-matter" id="front-matter">Front Matter</a></h1>
<pre><code>Title           : Commit message formalization and enforcement
Author(s)       : Łukasz Biel
Team            : CommonDataLayer
Reviewer        : CommonDataLayer
Created         : 2021-03-17
Last updated    : 2021-03-17
Version         : 1.0.0
</code></pre>
<h1><a class="header" href="#commit-message-formalization" id="commit-message-formalization">Commit message formalization:</a></h1>
<h2><a class="header" href="#goal" id="goal">Goal:</a></h2>
<p>To have some standard. It can't be too strict because we are a small team, and this adds extra overhead.</p>
<h2><a class="header" href="#what-we-do-now" id="what-we-do-now">What we do now:</a></h2>
<p>We are using vaguely defined <code>angular commit message spec.</code> This means our commits have titles with some markings, and that's it.</p>
<p>Tags that are used so far:
<code>test</code>
<code>ci</code>
<code>chore</code>
<code>refactor</code>
<code>fix</code>
<code>feature</code>
<code>docs</code>
<code>rfc</code></p>
<p>Some of these overlap, we foregone using the scope of the commit.</p>
<h2><a class="header" href="#what-we-should-be-doing" id="what-we-should-be-doing">What we should be doing:</a></h2>
<p>Document the process. Create a set of <code>tags</code> that are in use.
Some tags are duplicates. Thus I propose to use only:</p>
<ul>
<li><code>chore</code> - dependency updates, ci updates, refactorings, other changes that don't affect <code>CDL</code> functionality</li>
<li><code>test</code> - adding or removing tests; it does not include changes to deployment or CI. These go under <code>chore.</code></li>
<li><code>fix</code> - fixing a bug in <strong>code</strong></li>
<li><code>feat</code> - adding new feature to <strong>CDL</strong></li>
<li><code>docs</code> - adding documentation (readmes, mdbook, plantuml etc.)</li>
</ul>
<p>On merge, we will squash commits and use one of 5 tags. We will not be using the scope as it's basically useless - most of our changes affect everything.
The summary should describe which components were changed.</p>
<p>In the meantime, we started enforcing names on PR's.
I propose that we drop tags in PR titles altogether and use labels.</p>
<p>As for the long commit message:</p>
<p>for dependabot - we can leave them as is.
For our work - we should make sure that commits titled wip aren't included; thus, it may be a good idea to have some standard there.</p>
<p>I propose we use the format as follows:</p>
<pre><code>parent-tag: summary

* tag: summary
* tag: summary
</code></pre>
<p>Parent tag is the main goal of the work, and the list contains all things we did during implementation. E.g.:</p>
<pre><code>feature: add lazers to CDL

* chore: add deployment target on the moon
* docs: document usage of lazers (and how to keep them away from cats)
</code></pre>
<h1><a class="header" href="#commit-message-enforcement" id="commit-message-enforcement">Commit message enforcement</a></h1>
<p>This is a tough topic. We cannot enforce anything via default GitHub means. There are two options:</p>
<ul>
<li>We enforce a standard on ourselves and take care of it.</li>
<li>We build a bot and perform merges only via its API.</li>
</ul>
<p>The first proposal is what we do now. We sometimes miss a commit, but with a concrete set of guidelines (see above), we should not have conflicts.
The second proposal requires extra work. We could use a bot at some point for other things than merging as well.
We can create a task with a clear description of what it would do and backlog it.</p>
<h1><a class="header" href="#front-matter-1" id="front-matter-1">Front Matter</a></h1>
<pre><code>Title           : CDL documentation generation
Author(s)       : Łukasz Biel
Team            : CommonDataLayer
Last updated    : 2021-07-01
Version         : 1.0.0
</code></pre>
<h1><a class="header" href="#glossary" id="glossary">Glossary</a></h1>
<ul>
<li><code>CDL</code> - Common Data Layer</li>
<li><code>QR</code> - Query Router</li>
<li><code>xtask</code> - cargo job; similar to how you may define run targets in npm, rake, etc.</li>
<li><code>PoC</code> - proof of concept</li>
<li><code>Swagger</code> - Swagger is a tool for generating client and server code from OpenAPI specification. It supports most major languages.</li>
</ul>
<h1><a class="header" href="#preface" id="preface">Preface</a></h1>
<p>The necessity of creating this RFC was sparked by amount of manual work needed when documenting API in <code>QR</code>.
Some topics I'm exploring here can be stretched to other parts of CDL documentation, and I'm taking my liberty to include
these parts as well. Thus, reader should mind, that result of this document may span across larger group of components than
<code>QR</code> alone.</p>
<h1><a class="header" href="#current-state" id="current-state">Current state</a></h1>
<p><code>QR</code> API is documented manually via <a href="https://swagger.io/specification/">OpenApi</a> spec file. At the moment of writing this rfc it's outdated.<br />
<code>Configuration</code> is documented manually via <a href="rfc/../configuration/index.html">toml</a> files compliant with <a href="https://github.com/mehcode/config-rs">rust config crate</a>.
Later, it's possible that we'll replace these configs with configuration service.<br />
<code>gRPC</code> configuration is hand-written and <a href="https://github.com/hyperium/tonic">tonic</a> generates rust files with code needed for client and server implementation; 
proto files are located in <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/crates/rpc/proto">rpc crate</a>.<br />
<code>CDL Input Message</code> format is documented by hand in <a href="rfc/../architecture/data_router.html">docs</a>. At the moment of writing this rfc it's outdated.<br />
<code>Notification Message</code> format is not documented to my knowing.<br />
<code>graphQL</code> provides documentation via its endpoint and it's built into library we use. It may be wort considering to provide static graphQL spec in repository.</p>
<h1><a class="header" href="#problem-to-solve" id="problem-to-solve">Problem to solve</a></h1>
<p>Maintaining documentation for formats used in communication within or with CDL is cumbersome.
We need to research options of automating the process and validating its results.</p>
<h1><a class="header" href="#possible-solutions" id="possible-solutions">Possible solutions</a></h1>
<h2><a class="header" href="#generate-rust-code-from-specification-files" id="generate-rust-code-from-specification-files">Generate rust code from specification files:</a></h2>
<p>There are options for generating rust code from proto (already in place) and OpenAPI.</p>
<p>For generating structures needed for our internal and external communication, via Kafka and RabbitMQ, we could use <a href="https://docs.rs/schemafy/0.5.2/schemafy/">schemafy crate</a>.
It most likely needs to be done via <code>xtask</code>, similar to how we are generating things using <a href="https://github.com/hyperium/tonic">tonic</a>.</p>
<p>As for OpenAPI, it would require us to switch away from using <a href="https://github.com/seanmonstar/warp">warp</a> in favour of <a href="https://github.com/hyperium/hyper">hyper</a>. Swagger can generate new crate in our repo, 
that <code>QR</code> would use as a base.</p>
<p>For graphQL api - we would need to provide a <code>xtask</code> which pulls spec from running api instance. There's no easy way to generate rust code from existing api spec. If we were to stick with this approach - we'd have to write something in-house.</p>
<p>TOML configuration is a bit problematic. There are no known crates that can easily generate rust structs out from Toml config, and our structs
are usually reused. What I mean is that generating code would introduce duplication. It's not necessarily bad, but that's confusing.
We probably would need to produce an in-house solution for that problem. 
There's also issue that at some point toml configuration may be phased out in favour of Config Service, so let's not get attached.</p>
<p>In general - it requires some significant work from team to switch fully to configs generating rust code and it can't be done easily in all of the places.</p>
<h2><a class="header" href="#generate-configs-from-rust" id="generate-configs-from-rust">Generate configs from rust</a></h2>
<h3><a class="header" href="#compiler-pass-way" id="compiler-pass-way">Compiler pass way:</a></h3>
<p>We can create a binary that walks through our codebase and based on some criteria generates specs based on rust AST. It's doable, although not an easy task.
More can be read searching for feature <code>rustc_private</code>, it allows users to use rustc as a library.</p>
<h3><a class="header" href="#xtask--lib-way" id="xtask--lib-way">XTask + Lib way:</a></h3>
<p>We can create special traits/methods and use them within custom binaries used only in <code>xtask</code> targets. This way compiler should remove unused instances of code in official binaries,
no extra handling would be visible on production side, but <code>xtask</code> target would be able to perform actions based on these traits.</p>
<p>We would do nothing for <code>graphQL</code>, yet create task that starts an instance and fetches spec.</p>
<p>We would have to work hard on <code>QR</code> as warp doesn't necessarily play with such code generation - 
I don't think it's wise to decorate function expressions with macros, and methods themselves don't provide all info out of the box.
We would have to mark arguments of these methods with where does it come from. Some are provided by header, some are caches, some are bodies or query params. 
If we wouldn't compromise speed and latency, I'd propose to review other options for servers in rust,
as I believe they may be easier to integrate with <code>xtask</code> approach. Alas, there's a lot of uncertainty, and some of the research definitely requires a PoC.</p>
<p>For configuration, it should be really easy to write these traits from scratch. It's ever possible to use <code>serde</code> own traits within our own (for defaults and whatnot).
For JSONs, in form of <code>CDL Input Message</code> and <code>Notification Message</code>, goes the same. Just, we can use existing solution.</p>
<p><code>gRPC</code> may prove a challenge, more research is to be done, probably PoC. Tonic supports generation one way, 
and I'm not sure how much of an effort would it be to replace generated files with something we'd be able to write manually and generate protos from it.
<a href="https://github.com/google/tarpc">tarpc</a> may be worth looking at, as it's purely macro based. We've put some work into it, however I cannot recall any PoC using protobuf format. 
Some context may be found in <a href="https://github.com/google/tarpc/issues/161">this issue</a>, it appears it's possible. For <code>xtask</code> this still would mean custom <code>.protobuf</code> gen.</p>
<h2><a class="header" href="#mixed-solution" id="mixed-solution">Mixed solution</a></h2>
<p>It's not a problem to have mixed solution in some places. Important part is nothing has to be done manually in more than one place.</p>
<h1><a class="header" href="#authors-comment" id="authors-comment">Author's comment</a></h1>
<p>As generating rust code may be tempting and is quite popular in rest of the industry, I think it may be beneficial to generate specs from rust.
Rust macro system is quite easy to use and this differentiates it from most of other languages. We will avoid compiling / and possibly reviewing / specs.
Some initial discussion happened on this topic, and it seems that team, in majority, agrees with that. Next step would be to PoC this.</p>
<h1><a class="header" href="#conclusion" id="conclusion">Conclusion</a></h1>
<p>We've discussed this topic on 07.07.2021.
Conclusion:</p>
<ul>
<li>gRPC isn't our priority, and we are happy with current state; with <code>ar_pe_ce</code> being worked on, it's possible to revive this topic once it gets plugged into cdl.</li>
<li>graphql needs no work; specification in web service is sufficient for now.</li>
<li>configs, public json APIs and QR got their own tasks with preparing PoCs with homegrown solution:
<ul>
<li>https://github.com/epiphany-platform/CommonDataLayer/issues/628</li>
<li>https://github.com/epiphany-platform/CommonDataLayer/issues/629</li>
<li>https://github.com/epiphany-platform/CommonDataLayer/issues/630</li>
</ul>
</li>
</ul>
<pre><code> Title: Branching Strategy
 Author: Samuel Mohr
 Team: CDL
 Reviewer: CDLTeam
 Created on: 2021-06-30
 Last updated: 2021-06-30
</code></pre>
<pre><code>=========================================================
shortened version is available at the end of the document
=========================================================

</code></pre>
<h1><a class="header" href="#glossary-1" id="glossary-1">Glossary</a></h1>
<ul>
<li>Release Branch - A fork originating from the development branch, marking the point in time, in which a feature set is frozen, and an official release is made.</li>
<li>Development Branch - The main, active branch to which all code that is currently worked on is merged. Only developer releases (a.k.a. release candidates) can be done from this main branch.</li>
<li>MAJOR,MINOR,PATCH - The standard notation for semantic versioning. The triplet represents three main categories of versions: major release, minor release, and patches. The importance of the version section decreases from left to right. Additionally, a bump in a specific category resets everything in categories to the right to 0.</li>
<li>Release - A fixed build of either the release branch or development branch that is then published for public usage.</li>
</ul>
<h1><a class="header" href="#current-status" id="current-status">Current Status</a></h1>
<p>When originally developing a strategy for releases, we originally decided to maintain two branches: develop, for all short-term development of features, and main, for stable release of versions, where new versions containing collections of features would be single commits to the release branch, tagged with version numbers.</p>
<p>However, the past few months have led to the team cutting out the middle man and tagging commits in the develop branch, releasing development releases without the need for the main branch. The benefits of having the extra copy were not justified by the extra work of maintaining a separate branch and syncing it with development. Additionally, this didn't solve the issue of how to add bug fixes to already released versions. Though unwise to change existing code, patches made that increment the patch segment of a release's semantic version are a way to fix incorrect behavior of a release without breaking API's or otherwise adding new features. This is not structured into the old approach.</p>
<h1><a class="header" href="#new-approach" id="new-approach">New Approach</a></h1>
<p>If we consider a release to be a collection of features added since a prior release, our goal in producing releases, is to provide releases using the GitHub releases feature that are accessible to end users, as well as maintaining a means by which we can conveniently and correctly provided patches for said releases.</p>
<p>The proposed solution is a combination of the two most popular means for making releases: making and maintaining release branches, and tagging commits in those branches.</p>
<p>When a release is planned, the next branch should be created (sprouted from the common development branch) with name based on the major and minor versions only for the release family (e.g. v1.3).
First, all the features should be completed and then merged into develop. Develop should then be frozen and prepared for next release. A release branch should be then created from the tip of the freeze (if any last-minute fixes are applied).
If a feature is planned but not delivered in time before the freeze, then afterward the freeze not delivered changes should not be committed (only last-minute fixes to existing ones), until release branch is created, main branch is treated as one and it can not get new features. Additionally, a release branch's origin cannot be moved after creation and features cannot be cherry-picked or merged afterward.</p>
<p>After the release branch is created, all further development will proceed on development branch simultaneously. That means, if the release deadline will be missed by a developers, freeze does not have to wait for the feature development to release. That also means that no new features can be committed to the released branch, only patches and/or bug fixes.</p>
<h2><a class="header" href="#bugfixes" id="bugfixes">Bugfixes</a></h2>
<p>When bug fixes are required to fix incorrect behavior in a release. However, the incorrect behavior can also be present on the develop branch. If the code exists in develop, then a bug fix change PR should be made, tested, and squashed and merged to develop. The squashed commit should then be cherry-picked to all relevant and supported branches, resulting in a bump of the version's PATCH number.</p>
<p>If the issue only affects the release code and no longer affects develop code, or the patch is not feasible for cherry-picking (refactored code, too many changes in history etc.) the bug fix PR can be squashed and merged directly to the version branch without needing to cherry-pick from develop.</p>
<p>If the issue affects both development branch and supported release branch(es), but the fix is not feasible to be cherry-picked due to large amount of non-forwardable changes, then fix should be done for each release branch separately from develop, and, if appliable, also introduced to develop.</p>
<h2><a class="header" href="#version-branch-lifetime" id="version-branch-lifetime">Version Branch Lifetime</a></h2>
<p>Release branches have to be supported long term (term will be defined per-case). For example, if we support any release for 6 months, that branch must be kept open for any potential bug fixes that need to be added. When the version is no longer supported, the branch can be marked as obsolete and no further updates to this branch are unlikely to happen. Release branches should not be deleted, overwritten nor squashed.</p>
<p>A set time should be decided for support for all versions generally, though 6 months is a good placeholder for the time being, although, prolonged support may be requested, and therefore it may be necessary to facilitate it.</p>
<p>Currently, we are responsible for keeping 3 last releases in support pipeline. That means 3 quarters or 9 months of releases. Additionally, please keep in mind that RC releases are preview branches and do not have to be supported nor fixes for those are a priority.</p>
<h1><a class="header" href="#tldr" id="tldr">TL;DR</a></h1>
<p>This document is describing what is called, <code>scaled trunk based development</code> and many materials are freely available online in different sources.
This document however introduces and specifies how tagging and release preparations should look like. Besides that, everything should be 1:1 with official documentation about the process.</p>
<h3><a class="header" href="#patch-versioning" id="patch-versioning">Patch Versioning</a></h3>
<ul>
<li>Patches will result in a bump of the current version's PATCH number on the affected release branch.</li>
<li>Patches will not result in bump in development (release candidates).</li>
</ul>
<h3><a class="header" href="#patch-application" id="patch-application">Patch Application</a></h3>
<ul>
<li>If the development branch is affected, the patch should be fixed and changes should be pushed to the development branch.</li>
<li>If the supported release branch is also affected by the proposed patch... (to the developer's discretion):
<ul>
<li>... and the fix is easily applicable, it should be cherry-picked from develop to release branch.</li>
<li>... but the resulting fix is not easy to forward to the release branch (i.e. missing refactor, changes in history, conflicting features, etc), the fix has to be crafted and applied manually for each branch.</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#branch-tagging" id="branch-tagging">Branch Tagging</a></h3>
<ul>
<li>The tip of the release branch will be tagged by its (MAJOR,MINOR) tag.</li>
<li>Each patch will additionally introduce a build tagged with (MAJOR.MINOR.PATCH).</li>
<li>The development branch will carry release candidate builds (RC), tagged as (MAJOR.MINOR.0-rcXX) where XX is a sequential, increment-only value, and (MAJOR.MINOR) are taken from the upcoming release.</li>
</ul>
<h1><a class="header" href="#out-of-scope" id="out-of-scope">Out of Scope</a></h1>
<p>This document does not cover methods by which features can be selected to make up new versions, only how to release those versions and how to patch them post-release.</p>
<h1><a class="header" href="#front-matter-2" id="front-matter-2">Front Matter</a></h1>
<pre><code>Title           : Documentation restructure and RFC process
Category        : Process
Author(s)       : Wojciech Polak
Created         : 2021-07-02
</code></pre>
<h1><a class="header" href="#glossary-2" id="glossary-2">Glossary</a></h1>
<p>RFC - Request for Comments</p>
<h1><a class="header" href="#preface-1" id="preface-1">Preface</a></h1>
<p>This RFC introduces a new process to keep our documentation clean and up-to-date. Both subjects - documentation restructure, and RFC process are married and cannot be discussed separately.</p>
<h1><a class="header" href="#documentation-restructure" id="documentation-restructure">Documentation restructure</a></h1>
<h2><a class="header" href="#goal-1" id="goal-1">Goal</a></h2>
<p>To simplify, clarify documentation structure. What documents should be grouped in what categories. Also, to define what the categories are and what these purposes are.</p>
<h2><a class="header" href="#what-we-have-now" id="what-we-have-now">What we have now</a></h2>
<pre><code>architecture - information about each component - very brief. Almost 70% of the architecture documentation takes information about configuration and environment variables. This information is outdated.

configuration - newest created directory, description of all ENV variables, and configuration files. It is up to date.

deployment - documentation how to deploy CDL

examples - empty

features - brief information about main features like client routing, materialization, and ordering. materialization.md is only an outdated tutorial without further explanation of the goal and overall design.

rfc - directory where we keep all RFC, mixed processes, abandoned RFCs, implemented features, and WIP features.

SUMMARY.md - the main index must be updated whenever we add a new document—often forgotten and outdated. It is used to generate a sidebar with navigation. Very important.

benchmarks.md - empty, TODO

getting_started.md - simple, outdated tutorial on how to play with CDL

how_it_works.md - Simple, probably also outdated, three paragraphs on how CDL works internally. Very brief.

index.md - First file seen when someone visits docs.commondatalayer.org

protocol.md - empty, TODO

schemas_and_views.md - few paragraphs explaining what schema and view is. It is very much outdated before we started materialization.

versioning.md - brief information about CDL and SemVer. It also describes our branching or tagging strategy and releases schedule. I will be outdated very quickly.
</code></pre>
<h2><a class="header" href="#what-should-we-have" id="what-should-we-have">What should we have</a></h2>
<p>Please, keep in mind that an explanation for these changes is written in the next section describing the RFC process.</p>
<pre><code>architecture
Purpose: information about each component, schema registry, object builder, and other applications. It should only contain our internal architecture description, the current state of each element. Information what it does, why and how, what algorithms (from a broad perspective without many implementation details) were used, the protocol between the systems, etc. It should be enough information to adapt a new developer to CDL quickly. 
Change: remove ENV variables as these are duplicated in the following directory.
Used by: CDL team

configuration
Purpose: Description of all ENV variables and configuration files
Change: No changes needed
Used by: CDL user

deployment
Purpose: Documentation of how to deploy CDL
Change: No changes needed
Used by: CDL user

examples
Purpose: a place for tutorials (for example, the one living right now in features/materialization.md, although it needs an update) and guides.
Change: Move features/materialization.md, also maybe getting_started.md
Used by: CDL user

features
Purpose: a place for feature (behavior) description. Each feature should have at least one document describing it. Example: ordering.md, materialization.md, routing.md, CDLIM_versioning.md
Change: Rewrite materialization.md by precisely describing what this feature is. Rest should be kept as it is, especially index.md with table linking feature with RFC and documentation.
Used by: CDL user

processes
Purpose: a place for all our processes. For example: Commit message formalization or this RFC when it's going to be accepted.
Change: New directory. Keep index.md similar to the one in features
Used by: CDL team

rfc
Purpose: a place for all RFCs.
Change: Tag all RFCs with their status. Create features/processes based on RFCs that are closed. There is no need for index.md because of SUMMARY.md.
Used by: CDL team

...
The rest is not that important for now. CDL team should create an issue to clean and update these, but there is no point in discussing these now.
</code></pre>
<h1><a class="header" href="#rfc-process" id="rfc-process">RFC process</a></h1>
<h2><a class="header" href="#goal-2" id="goal-2">Goal</a></h2>
<p>To simplify, clarify our methodology. To explain what should be included in RFC and what steps should be included in the RFC process.</p>
<h2><a class="header" href="#what-we-have-now-1" id="what-we-have-now-1">What we have now</a></h2>
<h3><a class="header" href="#rfc-definition" id="rfc-definition">RFC definition</a></h3>
<p>RFC is describing either the change or current state of the CDL. It might refer to other RFC and the current state of CDL. 
There is no definition of what RFC is and what it should include.</p>
<h3><a class="header" href="#structure" id="structure">Structure</a></h3>
<p>We keep all RFCs in the <code>rfc</code> directory; however, there is little to no resolution after we close it.
Description of the components is mixed with RFCs describing features and processes, resulting in a strange mess where NAF-Commit_message...md lives next to <code>CDLF-000XX-rfc-XX.md</code>. </p>
<h2><a class="header" href="#what-should-we-have-1" id="what-should-we-have-1">What should we have</a></h2>
<p>RFC should only describe <strong>change</strong> to CDL. The change might be:</p>
<ul>
<li>New feature</li>
<li>Modification of existing feature</li>
<li>New process</li>
<li>Modification of existing process</li>
</ul>
<p>It should not contain information about a single component (its current state, as this is the purpose of <code>architecture</code>).</p>
<h3><a class="header" href="#rfc-category" id="rfc-category">RFC Category</a></h3>
<p>Each RFC must contain a category informing whether it describes the feature or the process.
Accepted values: <code>Feature</code>/<code>Process</code>.</p>
<h3><a class="header" href="#rfc-deadline" id="rfc-deadline">RFC Deadline</a></h3>
<p>Each RFC must contain a deadline to speed up the process.
Before the deadline is met, the author should remind all interested parties about it.</p>
<p>If the deadline is met without conclusion, the author of the RFC should create a meeting and invite all interested in these RFC parties. Then during this meeting resolution has to be made.</p>
<p>The deadline must be set at least 3 days from the announcement. RFC author should schedule the meeting on the nearest possible date that is acceptable for all parties.</p>
<p>Until the deadline, everyone is allowed to comment on the RFC. Participants should use the scheduled meeting only to decide when there is more than one strong opinion about the subject. All issues should be defined and discussed via GitHub comments. The deadline meeting should be relatively short if everyone agrees on the subject.</p>
<p>There may be exceptions to this rule - for example, when during a deadline meeting, everyone agrees to wait for new information from superiors, etc.</p>
<h3><a class="header" href="#rfc-ordering-number" id="rfc-ordering-number">RFC ordering number</a></h3>
<p>Each RFC has an assigned ordering number that is an always-increasing integer. In rare situations, there might be gaps between two integers.
RFC ordering number has to be unique.</p>
<p>RFC ordering number and feature ID are not the same concepts.</p>
<h3><a class="header" href="#the-process" id="the-process">The process</a></h3>
<h4><a class="header" href="#draft" id="draft">Draft</a></h4>
<p>The author of the RFC should create a new document in the <code>rfc</code> directory. This directory has a flat structure, and each RFC should have a filename in human-readable format with RFC ordering prefix number and version postfix number. For example: <code>1234_schema_registry_less_CDL_deployment_01.md</code>.</p>
<p>Version postfix number is increased when RFC describes changes for existing RFC; for example, when there is <code>xxx_feature_01.md</code>, the author wants to describe the modification or new approach to the feature (which isn't a new feature itself), they should write <code>yyy_feature_02.md</code>. In that case, however, the ordering number is always different - <code>xxx</code> &lt; <code>yyy</code>.</p>
<p>Format:
<code>&lt;RFC ordering number&gt;_&lt;Human readable name&gt;_&lt;Version postfix number&gt;.md</code></p>
<p>Each RFC has to contain Front Matter in the format:</p>
<pre><code># Front Matter

```
Title           : Schema-Registry-less CDL deployment
Category        : Feature
Author(s)       : Wojciech Polak
Team            : CommonDataLayer
Created         : 2021-06-24
Deadline        : 2021-06-26
CDL feature ID  : CDLF-00016-00
```
</code></pre>
<p>CDL feature ID is required only for the feature description. If RFC describes the process, this field should be omitted.</p>
<p>Team field is optional. If missing, its implied that CDL Team has created it.</p>
<p>RFC format allows other custom fields to be included if necessary, such as why RFC has been abandoned.</p>
<p>For example, the author should commit the draft to a separate branch, for example, <code>rfc/&lt;filename&gt;</code>.</p>
<h4><a class="header" href="#pull-request" id="pull-request">Pull Request</a></h4>
<p>When the draft is ready for review, an author should create PR and inform reviewers.</p>
<h4><a class="header" href="#resolution" id="resolution">Resolution</a></h4>
<p>Each RFC must result in:</p>
<ul>
<li>When accepted:
<ul>
<li>If the team decide to implement feature described in the RFC:
<ul>
<li>Implementator should create tracking issue - he should include its id in <code>features/index.md</code> table.</li>
<li>Update in <code>architecture</code> - all components that were changed should be updated, so this RFC won't be needed to understand how for example, Object Builder works. It should be done as a part of PR that has been merged.</li>
<li>Update in <code>features</code> - if RFCs category is <code>Feature</code> - Update existing or create new feature.md and write a new description of what this feature does. In some cases, the author can selectively copy-paste it from RFC; however, the author must be careful. It should be done as a part of PR that has been merged.</li>
</ul>
</li>
<li>If RFC describes the process: 
<ul>
<li><code>processes/index.md</code> needs to be updated</li>
<li>Update in <code>processes</code> - if RFCs category is <code>Process</code> - Update existing or create a new process.md and write a further description of how the process looks. In some cases, the author can selectively copy-paste it from RFC; however, the author must be careful. Implementator should do it in PR that has been merged.</li>
</ul>
</li>
</ul>
</li>
<li>If the deadline has been met, but no conclusion has been reached - RFC should be updated with the new deadline
(sub-issues).</li>
<li>When abandoned - RFC should be updated with <code>Abandon reason</code>.</li>
</ul>
<p>It also means PRs with implementations should not be accepted and merged before merging documentation.
Exception: It is allowed to split PR into smaller, by merging the first and then separate PR documentation. However, until the last PR implementation is merged, <strong>tracking issue has to be kept open</strong>.</p>
<h4><a class="header" href="#further-changes" id="further-changes">Further changes</a></h4>
<p>Apart from minor typo fixes, and formatting RFC should be immutable.
Whenever there is a need to change the feature or the process, one should create another RFC describing new goals and changes.</p>
<h3><a class="header" href="#summary" id="summary">Summary</a></h3>
<p>RFC should describe <strong>changes</strong> while <code>features</code>/<code>processes</code>/<code>architecture</code> should keep up-to-date information about the current state of CDL.</p>
<h1><a class="header" href="#decisions" id="decisions">Decisions</a></h1>
<blockquote>
<p>Actions are from meeting about RFC that happened on 13.07.2021</p>
</blockquote>
<ul>
<li>Included default minimal deadline period.</li>
<li>Decided to change filenames to something more human-readable</li>
<li>Added ordering number for RFC</li>
<li>Removed boilerplate fields like &quot;Reviewer&quot;, &quot;Last updated&quot;, or &quot;Version&quot;</li>
<li>Decided to remove RFC status from RFC document and keep the information in <code>features/index.md</code> instead</li>
</ul>
<h1><a class="header" href="#front-matter-3" id="front-matter-3">Front Matter</a></h1>
<pre><code>Title: Alternative communication method to Kafka and RabbitMQ
Author: Wojciech Polak
Team: CDL
Reviewer: CDLTeam
Created on: 11/02/2021
Last updated: 11/02/2021
Tracking issue: https://github.com/epiphany-platform/CommonDataLayer/issues/68
</code></pre>
<h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<h2><a class="header" href="#summary-1" id="summary-1">Summary</a></h2>
<p>We should add GRPC communication in all components that are using right now Kafka or <code>RMQ</code>. We should introduce a standard interface and separate whatever we are doing with input messages from the transportation layer.</p>
<h2><a class="header" href="#glossary-3" id="glossary-3">Glossary</a></h2>
<p><code>RMQ</code> names any <code>AMQP</code> server - most common is<code>RabbitMq</code> <br />
<code>MQ</code> - Message Queue - Kafka or <code>RMQ</code> <br />
<code>CS</code> - command service</p>
<h2><a class="header" href="#background" id="background">Background</a></h2>
<p>Right now, most of our communication in CDL ingestion is handled by either Kafka or <code>RMQ</code>. While this is acceptable for some clients, there is a case where CDL should not communicate via message queue at all. Therefore we need a replacement protocol, and we could use GRPC for that purpose.</p>
<p>What is more - right now, we have a partially-baked solution in <code>CS</code> - this service accepts either <code>MQ</code> or <code>GRPC</code> as a communication method; however, <code>DR</code> can only produce messages to Kafka or <code>RMQ</code>. Furthermore, this solution has mixed business logic with the transportation layer, which causes some unnecessary repetitions in the codebase. It makes it harder to maintain than it should be.</p>
<h2><a class="header" href="#goals-and-requirements" id="goals-and-requirements">Goals and Requirements</a></h2>
<ul>
<li>Accept <code>GRPC port</code> env variable (or command-line argument).</li>
<li>If GRPC has been chosen over <code>MQ</code> - start endpoint.</li>
<li>Each service should share code between the <code>MQ</code> handler and <code>GRPC</code> handler.</li>
<li>Code handling message should not depend directly on any communication method. It should be under an abstraction.</li>
</ul>
<h1><a class="header" href="#solutions" id="solutions">Solutions</a></h1>
<h2><a class="header" href="#existing-solution" id="existing-solution">Existing solution</a></h2>
<p>We are currently using <code>GRPC</code> only for querying data and communication between CLI/GUI/API and schema registry.</p>
<p><code>GRPC</code> support in the ingestion part of CDL is not finished.</p>
<p>A client cannot use CDL without <code>MQ</code>.</p>
<p>Additionally, right now, CDL uses the same topic for both error notifications and report notifications.</p>
<ul>
<li>Error notifications inform the client about errors, for example, because of a corrupted message - it has the same purpose as good logging and monitoring. In the GRPC world, we can also return information about the error in the response.</li>
<li>Reports inform clients about the resolution on <code>object</code> level - if data sent to <code>MQ</code> has been processed by CDL and stored in DB or rejected.</li>
</ul>
<p>CDL needs notifications in the async world because there is no way to inform the client about corruption/resolution directly; however, this is not necessary in the GRPC world (at least not the error notification part).</p>
<p>What is worth mentioning - this RFC requires finding some middle ground between sync (<code>GRPC</code>) and async (<code>MQ</code>) world. </p>
<p>By saying <code>GRPC</code> in sync, we mean - it requires that each request returns a response, while <code>MQ</code> has more <em>fire and forget</em> behavior. In both scenarios, rust implementation is using tokio and async-await features.</p>
<p>These features, unfortunately, are not common.
<code>MQ</code> is heavily based on the <code>Stream</code> trait while <code>GRPC</code> uses <code>async-trait</code>.</p>
<p>Unfortunately, because Kafka uses borrowed messages it requires box leaking, which might be dangerous when left alone. The message is also wrapped into the <code>Box</code> to allow dynamic dispatch (to acknowledge either Kafka message or <code>RMQ</code> message).</p>
<h2><a class="header" href="#proposed-solution" id="proposed-solution">Proposed solution</a></h2>
<h3><a class="header" href="#async-trait" id="async-trait">Async trait</a></h3>
<p>As previously mentioned, the transportation layer should be invisible to the user. To do so, I'd like to introduce a new async trait:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait ConsumerHandler {
    async fn handle(&amp;self, msg: &amp;dyn Message) -&gt; Result&lt;()&gt;;
}
<span class="boring">}
</span></code></pre></pre>
<p>Each service would implement that handler trait to receive messages from <code>MQ</code>/<code>GRPC</code>.</p>
<p>First of all, while we switch from <code>Stream</code> trait to <code>async trait</code>, we cannot simply remove <code>Box::leak</code>. We could do it when we limit the code to the ordered single-threaded solution, however that would create a hughe performance hit.</p>
<p>Instead, we still need to rely on leaking because <code>tokio::spawn</code> (called inside of the transportation layer) requires <code>'static</code> lifetime. In the future we might be able to either ditch borrowed message and replace it with owned, or use proposed structured concurrency which would enable to spawn task with some non static lifetime (because we could guarantee that all tasks should finish before we drop the consumer).</p>
<p>Second of all message is no longer wrapped in <code>Box</code> - instead, the user receives only <strong>reference</strong> to the dynamic object.</p>
<p>Lastly - handler returns <code>anyhow::Result</code> - so transportation layer based on that can:</p>
<ul>
<li><code>GRPC</code> - return response either OK/Internal Server Error/Bad Request (TBD how to distinguish between last two)</li>
<li><code>MQ</code> - use acknowledge, negative acknowledge (in <code>RMQ</code>) or only doing nothing and not responding at all to the message broker.</li>
</ul>
<h3><a class="header" href="#internal-implementation" id="internal-implementation">Internal implementation</a></h3>
<p>Internal implementation is quite simple. We keep <code>enum Consumer</code>, which accepts in its constructor our instance of <code>ConsumerHandler</code> along with configuration parameters (URL address to Kafka broker etc.).</p>
<p>Inside of method <code>async fn run(self)</code> we match consumer variant and either run simplest possible <code>while let Some()</code> loop for <code>MQ</code>, or initiate <code>GRPC</code> server.</p>
<p>Per each received message (either from <code>MQ</code> or in <code>GRPC</code> server implementation), we can call <code>consumer_handler.handle(&amp;msg)</code> and wait for the response. Simple as that.
To enable better performance we would call this handler inside of <code>tokio::spawn</code>. What is worth mentioning, <code>MQ</code> acknowledges should be sent inside of the spawned task.</p>
<p><code>GRPC</code> is unordered by design, if client needs an ordering, it needs to send one request at the time - it is it's responsibility, not CDL.</p>
<h3><a class="header" href="#grpc-protocol-schema" id="grpc-protocol-schema">GRPC protocol schema</a></h3>
<p>To unify all internal communication in CDL ingestion, we need to use a common, shared GRPC protocol.</p>
<pre><code class="language-proto">syntax = &quot;proto2&quot;;

package generic_rpc;

service GenericRPC {
    rpc Push(Message) returns (Empty);
}

message Message {
  required string key = 1;
  required bytes payload = 2;
}

message Empty {}
</code></pre>
<p>Thanks to that, we can imitate a message just like the one received from <code>MQ</code>.</p>
<h3><a class="header" href="#notifications--error-reporting" id="notifications--error-reporting">Notifications &amp; Error reporting</a></h3>
<p>No client requires error reporting, and we are already sending logs. Therefore it is not needed and can be removed.</p>
<p>Notifications are a bit more complicated. These are <code>CS</code> specific, and therefore, cannot be part of the transportation layer (transparent to the user code).</p>
<p>We need to introduce <code>ReportSender</code> for GRPC.
An abstracted producer which sends reports to given sink. </p>
<p>One cannot send the report to any <code>MQ</code>. One suggested way is to send the callback to some specified endpoint by sending a <code>POST</code> request. Another is to use elastic search or Postgres.</p>
<p>This issue is open for further discussion.</p>
<h2><a class="header" href="#alternative-solutions" id="alternative-solutions">Alternative solutions</a></h2>
<h3><a class="header" href="#rest" id="rest">REST</a></h3>
<p>Instead of using GRPC we could use simple HTTP REST requests.</p>
<h4><a class="header" href="#advantages" id="advantages">Advantages</a></h4>
<ul>
<li>Simpler to implement</li>
<li>Do not need protocol schema</li>
<li>Based on HTTP - usable with service mesh</li>
<li>No mixed protobuf with pure JSON in payload - simpler to deserialize</li>
</ul>
<h4><a class="header" href="#disadvantages" id="disadvantages">Disadvantages</a></h4>
<ul>
<li>Requires extra effort to switch from GRPC</li>
<li>Does not solve client code generation like GRPC</li>
</ul>
<h3><a class="header" href="#custom-tcp" id="custom-tcp">Custom TCP</a></h3>
<p>Instead of relying on existing protocol, we could also create custom one based on TCP.</p>
<h4><a class="header" href="#advantages-1" id="advantages-1">Advantages</a></h4>
<ul>
<li>Full controll on design</li>
<li>Probably fastest method (if done right)</li>
</ul>
<h4><a class="header" href="#disadvantages-1" id="disadvantages-1">Disadvantages</a></h4>
<ul>
<li>Requires a lot of effort with designing, testing and benchmarking,</li>
<li>Needs extra careful touch in areas regarding timeout connections, closed sockets etc.</li>
<li>Requires writing custom client libraries in popular languages: Java, Python, C#, JavaScript</li>
</ul>
<h3><a class="header" href="#zeromq" id="zeromq">ZeroMQ</a></h3>
<p>While we shouldn't replace Kafka nor RabbitMQ with zeroMQ, we can consider it for ReqResp.</p>
<h4><a class="header" href="#advantages-2" id="advantages-2">Advantages</a></h4>
<ul>
<li>Fast, marketed as &quot;zero-abstraction&quot;</li>
<li>Similar interface to other <code>MQ</code> - it could ease creating an abstraction</li>
</ul>
<h4><a class="header" href="#disadvantages-2" id="disadvantages-2">Disadvantages</a></h4>
<ul>
<li>Rust client library lacks good example how to use it in <code>async-await</code> environment,</li>
<li>There are pinpointed problems with ZMQ listed by one of Rust client maintainers: https://github.com/jean-airoldie/libzmq-rs/issues/125#issuecomment-570551319</li>
</ul>
<h2><a class="header" href="#conclusion-1" id="conclusion-1">Conclusion</a></h2>
<p>GRPC seems to be easiest way to implement <code>MQ</code>-independence, however later we should re-evaluate REST and probably switch to it.
What is worth noting, after we switch CDL to one, abstracted and unified transportation layer, further change from GRPC to REST should be relatively easier.</p>
<h1><a class="header" href="#test-plan" id="test-plan">Test Plan</a></h1>
<p>There should be at least one end-to-end test checking if the whole pipeline works in an <code>MQ</code>-less environment.
We should run exactly same tests as we have now but with different env variables. All tests designed for <code>MQ</code> environment should pass in <code>GRPC</code> environment.</p>
<h1><a class="header" href="#futher-considerations" id="futher-considerations">Futher considerations</a></h1>
<h2><a class="header" href="#schema-registry-replication" id="schema-registry-replication">Schema Registry replication</a></h2>
<p>Replication featutre for Schema Registry needs major refactor (and probably replacement), therefore it is out of the scope of this change. Replication for <code>GRPC</code> should be deactivated.</p>
<h2><a class="header" href="#impact-on-other-teams" id="impact-on-other-teams">Impact on other teams</a></h2>
<p>Teams that are using <code>MQ</code> won't feel any difference. This refactor would allow other clients to use CDL.</p>
<h2><a class="header" href="#security" id="security">Security</a></h2>
<p>No security risk.</p>
<h1><a class="header" href="#tasks-and-timeline" id="tasks-and-timeline">Tasks and timeline</a></h1>
<p>TBD</p>
<h2><a class="header" href="#front-matter-4" id="front-matter-4">Front Matter</a></h2>
<pre><code>Title           : Usage of MessagePack format as a CDL input
Author(s)       : Mateusz 'esavier' Matejuk
Team            : CommonDataLayer
Reviewer        : CommonDataLayer
Created         : 2021-02-17
Last updated    : 2021-02-17
Version         : 1.0.12
CDL feature ID  : CDLF-0000E-00
</code></pre>
<h5><a class="header" href="#abstract" id="abstract">Abstract</a></h5>
<pre><code> The key words &quot;MUST&quot;, &quot;MUST NOT&quot;, &quot;REQUIRED&quot;, &quot;SHALL&quot;, &quot;SHALL
 NOT&quot;, &quot;SHOULD&quot;, &quot;SHOULD NOT&quot;, &quot;RECOMMENDED&quot;,  &quot;MAY&quot;, and
 &quot;OPTIONAL&quot; in this document are to be interpreted as described in
 RFC 2119.
</code></pre>
<p><a href="https://www.ietf.org/rfc/rfc2119.txt">RFC 2119 source</a></p>
<h2><a class="header" href="#glossary-4" id="glossary-4">Glossary</a></h2>
<h5><a class="header" href="#terminology" id="terminology">Terminology</a></h5>
<ul>
<li>CDL - <a href="https://github.com/epiphany-platform/CommonDataLayer">Common Data Layer Project</a></li>
<li>DR - Data Router, an CDL component responsible for ingesting and routing initial messages.</li>
<li>SR - Schema Registry, an CDL component responsible for keeping information about the type of the object conveyed inside the message.</li>
<li>User - user of the CDL. In this case, the user knows how the CDL works, and is assumed to have access to the API and code unless stated otherwise.</li>
<li>Message - (abstract) message sent to CDL for processing</li>
<li>Breaking Change - change in behavior, or API of the CDL that may result with system breakage on release update.</li>
<li>MD - man-day - amount of work completed by one developer in one work day.</li>
</ul>
<h5><a class="header" href="#features-1" id="features-1">Features:</a></h5>
<ul>
<li><a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc/CDLF-00004-00-rfc.md">CDLF-00004-00</a> - CDL Feature - Data Router introduction and message routing.</li>
<li><a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc/CDLF-00009-00-rfc.md">CDLF-00009-00</a> - CDL Feature - Message Batching - ability to digest array of proper CDL (v1 Input format) messages.</li>
<li><a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc/CDLF-0000A-00-rfc.md">CDLF-0000A-00</a> - CDL Feature - Message Ordering - ability to guarantee linear message ingestion.</li>
<li><a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc/CDLF-0000C-00-rfc.md">CDLF-0000C-00</a> - CDL Feature - GRPC as a main communication method, including DR input (direct ingestion)</li>
<li><a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc/CDLF-0000D-00-rfc.md">CDLF-0000D-00</a> - CDL Feature - Introduction of Istio-based service mesh interoperability</li>
</ul>
<h5><a class="header" href="#formats" id="formats">Formats:</a></h5>
<p>v1 Input Format - messages of format v1 are not related to Message Batching, i.e. batch can consist of a list of messages, of version v1, but array itself is not a message format. Input Message v1 is formatted as follows:</p>
<pre><code>	{
	    object_id: UUID,
	    schema_id: UUID,
	    data: { any valid json },
	}
</code></pre>
<p>v1 Batch - it is a batch format introduced to alleviate some problems with transports. It conforms only to the v1 input format, and its treatment is described in RFC for <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc/CDLF-00009-00-rfc.md">CDLF-00009-00</a></p>
<pre><code>[
	{  v1 message }
	{  v1 message }
	{  v1 message }
	...
	=&gt; n
]
</code></pre>
<h2><a class="header" href="#introduction-1" id="introduction-1">Introduction</a></h2>
<h4><a class="header" href="#background-1" id="background-1">Background</a></h4>
<p>In present design, DR can only ingest JSON input format, which is both inefficient, and proves to be troublesome with some features. One example of which is being binary format ingestion. It was proposed to introduce MessagePack as an alternative. MessagePack is as flexible as JSON, meaning there is no need to recompile with each alteration in the protocol, and there is no need to utilize the message schema before initiating the communication. In addition, MessagePack allows the program to maintain binary data as is, without extra escaping or encoding. The Scope of this document include only information about the CDL input transport, which is, by extension, communication between user and DR.</p>
<h4><a class="header" href="#assumptions" id="assumptions">Assumptions</a></h4>
<p>We have to assume that some users may want to retain the ability to communicate over plain JSON, and/or choose a specific format for a specific job, meaning that two formats may have to be employed at all times.</p>
<h4><a class="header" href="#preliminary-testing" id="preliminary-testing">Preliminary testing</a></h4>
<h5><a class="header" href="#performance" id="performance">Performance</a></h5>
<p>Simple preliminary testing using rust, C, and zig (while zig implementation being quite humble) displays some improvements to using message pack format over pure JSON string.</p>
<ul>
<li>testing was performed over 1 000 000 loops, and averaged per one encoding.</li>
<li>results shown are generated by code in C, but those were similar in scale for the two other languages</li>
</ul>
<table><thead><tr><th>#</th><th>JSON</th><th>message pack</th></tr></thead><tbody>
<tr><td>avg encoding of one message</td><td>3.11 × 10-4</td><td>7.850 × 10-5</td></tr>
<tr><td>avg decoding of one message</td><td>7.91 × 10-4</td><td>1.81 × 10-4</td></tr>
</tbody></table>
<p>Similar results can be depicted in readily available publications:
<a href="https://thephp.website/en/issue/messagepack-vs-json-benchmark/">MessagePack vs Json benchmark</a></p>
<h5><a class="header" href="#length-of-the-payload" id="length-of-the-payload">Length of the payload</a></h5>
<p>Example serialization shows that the same message that was serialized in MessagePack was smaller than in JSON.
| # | JSON    | message pack | difference | compression ratio
|---|---------|--------------|------------|------------------|
| # | 121 615 | 101 411      | 20 204      | 0.8338</p>
<h5><a class="header" href="#limitations" id="limitations">Limitations</a></h5>
<ul>
<li>integer values  are limited to 64 bytes</li>
<li>maximum length of binary object is (2^32) -1 bytes (4294967295 bytes or around 3.9 GiB)</li>
<li>maximum length of string object is (2^32) -1</li>
<li>String may be malformed, or be a non-valid UTF-8 sequence</li>
<li>maximum array length is (2^32) -1</li>
<li>maximum key-value map length is (2^32) -1
<a href="https://github.com/msgpack/msgpack/blob/3f0a9aae716596a86878c0d68dc0bd4256673202/spec.md">Source: MessagePack Specification</a></li>
<li>During research, it was apparent that only a few rust libraries support full MessagePack features, for example extensions.</li>
</ul>
<h2><a class="header" href="#solutions-1" id="solutions-1">Solutions</a></h2>
<h5><a class="header" href="#current-or-existing-solution" id="current-or-existing-solution">Current or Existing Solution</a></h5>
<p>DR can only ingest properly formatted JSON messages, consisting of either a single v1 object or an array of separate v1 objects, each potentially unrelated to each other. This means that all special characters and payloads have to be escaped or encoded in some way. Until now, it was proposed to use base64 encoding, however, for each 3 bytes encoded, base64 encoded equivalent requires 4 bytes to be transmitted, meaning 33% increase in payload  (not necessary stored), potentially also inside the CDL network.</p>
<h4><a class="header" href="#proposed-solutions---preamble" id="proposed-solutions---preamble">Proposed Solutions - preamble</a></h4>
<p>There are few different ways of controlling the message recognition, and each one have its drawbacks. The program can either be configured prior to the runtime to expect different messages on different communication mediums, be it kafka topic, or GRPC endpoint, or the code can be placed to expect different markers describing the message type and format and act on those.</p>
<p>Proposed below are detailed designs on how this issue can be handled:</p>
<h5><a class="header" href="#solution-i---separate-endpoints" id="solution-i---separate-endpoints">Solution I - separate endpoints</a></h5>
<p>Each format will have a corresponding abstract transport endpoint. In this case, for each available ingestion method we have to specify different handling methods, due to the nature of each communication medium:</p>
<ul>
<li>Kafka would receive a separate topic to use with MessagePack, or use one topic, however in that case, each partition would have different type of messages, and format mixing would have to be avoided</li>
<li>RabbitMQ would receive separate queue to use with MessagePack</li>
<li>GRPC would have separate endpoint</li>
</ul>
<h6><a class="header" href="#notes" id="notes">Notes:</a></h6>
<ul>
<li>No testing nor PoC was created for this solution, please treat it as being theoretical.</li>
<li>Having separate endpoints will result in CDL not being able to keep the ordering as described in <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc/CDLF-0000A-00-rfc.md">CDLF-0000A-00</a>. It has to be noted that this should not be a problem, as it would be highly unordinarily to interact with a system that, while being focused on message ordering, is using multiple message formats at once. Nevertheless, this limitation have to be considered and, if this solution will be chosen, additional documentation have to be prepared, and this limitation MUST be highlighted.</li>
</ul>
<h6><a class="header" href="#major-concerns" id="major-concerns">Major concerns:</a></h6>
<ul>
<li>This design will result in multiple configurations that have to be tested separately of each other and in tandem.</li>
<li>Some amount of additional configuration have to be added.</li>
<li>Separate endpoints can be misused, resulting in a cascade of errors. Additional error handling has to be introduced to discern the type of issue, and whatever it was related to format errors or not.</li>
<li>Parallel usage of two or more endpoints may end up in thread starvation on constrained machines. In such example, by having one thread available for the workload, the focus would be shifted to one endpoint over another. This is an infrastructure related issue, but it has to be considered in this context.</li>
</ul>
<h6><a class="header" href="#performance-1" id="performance-1">Performance:</a></h6>
<ul>
<li>From initial evaluation it seems that this solution should not have any considerable performance drawbacks.</li>
<li>Parallel usage can result in performance degradation, however this is only a theoretical issue, and was not proven by testing yet due to pre-PoC state of the feature and no observed evidences of this behavior in other parts of  the CDL.</li>
</ul>
<h5><a class="header" href="#solution-ii---single-endpoint-with-metadata-carrier" id="solution-ii---single-endpoint-with-metadata-carrier">Solution II - single endpoint with metadata carrier</a></h5>
<p>Both formats can use this same, abstract, transport endpoint. All the information required to control DR behavior and informing about the state of the message, will be provided in the transport's metadata. This means that we can retain the simplicity of having one, non-specific endpoint, where both message formats will be received.</p>
<p>After preliminary research, it seems that all the protocols can, in theory, pass metadata context alongside the payload.</p>
<h5><a class="header" href="#notes-1" id="notes-1">Notes:</a></h5>
<ul>
<li>For GRPC transport, information about the format would have to be contained in the header, or separate GRPC call (it is an open question, related to RFC on  <a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc/CDLF-0000C-00-rfc.md">CDLF-0000C-00</a></li>
<li>For Kafka and RabbitMQ, metadata can be stored in headers.</li>
<li>Detached tests were executed, in which CDL context was not used, mostly focused on the availability and usability of the libraries, and it’s support. Additionally, PoC was not created for this solution, please treat it as being theoretical.</li>
</ul>
<h5><a class="header" href="#major-concerns-1" id="major-concerns-1">Major concerns:</a></h5>
<ul>
<li>All the future transports will have to support sending some kind of metadata alongside the payload itself. This is quite possible but not guaranteed for all the users in all the use cases. In case new transport will be proposed, and it will not support it, it will have to be discarded or result in reopening this feature while choosing another solution.</li>
<li>Each transport have to get additional code, handling different ways of getting, and possibly parsing, the metadata provided with the message.</li>
<li>It was also stated on the internal meetings that clients and/or libraries in different languages may have problems with this functionality, as it is not widely used. As per project directives, CDL have to be compatible with different languages, and this may be a potential issue.</li>
</ul>
<h5><a class="header" href="#performance-2" id="performance-2">Performance:</a></h5>
<ul>
<li>Assuming the difference in metadata formats and our ability to both parse and serialize it, it can be safely assumed that performance will not suffer, however it depends on the way each library will get and use the metadata provided with the message. Comparing to the usual <code>O(m*m℘)</code> where <code>m</code> is the message size and <code>m℘</code> is the parsing cost, this solution will cost roughly <code>O((m*m℘) + (n*n℘))</code> where <code>n</code> and <code>n℘</code> is metadata length and its parsing cost respectively.</li>
</ul>
<h5><a class="header" href="#solution-iii---single-endpoint-with-format-recognition" id="solution-iii---single-endpoint-with-format-recognition">Solution III - single endpoint with format recognition</a></h5>
<p>In this solution, both formats can be sent to one transport endpoint, similarly to the design proposed in Solution II, the message will not contain metadata in itself, but parsing will be performed for each available serialization method. In case all the methods fail, the message will be considered malformed and handled the usual way, which means reporting the error and continuing to work on the next queued message.</p>
<h5><a class="header" href="#notes-2" id="notes-2">Notes</a></h5>
<ul>
<li>The Nature of this solution and its performance review, introduces the term &quot;Cost of Failure&quot; which is a measure <code>𝜅</code> that can be described as <code>0&lt;𝜅&lt;=m</code>, where m is the length of the message.
<ul>
<li>It describes the ability of the underlying library to recognize the format or fail in the case error.  The earlier the library can recognize parsing error, the lower is the <code>𝜅</code>, and the earlier code can react to the error.</li>
<li>Moreover, <code>𝜅</code> is also related to the attached function, in that case<code>n*𝜅</code> means:
<code>for x in n, return 𝜅(x)</code></li>
<li>It has to be, by definition fluid, and if not argued otherwise, taken at worst-case scenario rate.</li>
<li>In the layman's terms, lower <code>𝜅</code> is better, and until stated otherwise, defaults to <code>𝜅==m</code></li>
</ul>
</li>
<li>Detached tests were executed, in which CDL context was not used, additionally PoC was not created for this solution, please treat it as being theoretical.</li>
</ul>
<h5><a class="header" href="#major-concerns-2" id="major-concerns-2">Major concerns:</a></h5>
<ul>
<li>Cost of Failure will scale alongside the number of formats. Currently, this is limited to two which this document describes, although this is only true for the current state, and it may or may not change in the future.</li>
<li>Different libraries providing support for either JSON and MessagePack can behave differently. Cost of Failure, in those, is not documented or at least not easily available. Due to that, it would be wise that to assume the worst scenario, which is <code>𝜅==m</code> for each deserialization method.</li>
</ul>
<h5><a class="header" href="#performance-3" id="performance-3">Performance:</a></h5>
<ul>
<li>Comparing the performance to the other designs mentioned before, clearly shows that it is potentially more costly, ranging anywhere from <code>0</code> to <code>n*𝜅</code> where <code>n</code> is the number of formats CDL supports (currently this document describes the second format) assuming the worst scenario where <code>𝜅=m</code></li>
</ul>
<h5><a class="header" href="#potential-improvements" id="potential-improvements">Potential improvements:</a></h5>
<ul>
<li>
<p>assuming that one of the deserialization methods will fail, it is possible to parallelize those and to try to get at least one result out of several methods. This may improve performance to up to a single <code>𝜅</code>, while degrading memory usage (due to the possibility that deserialization methods will be destructive in the context of the used language) and requiring n threads to start working on the same message at the same time. This may prove helpful in cases when the user will want to use different formats and mix messages. Withal, it will be more troublesome to use for users that are using either one or the other.</p>
</li>
<li>
<p>Format recognition can be adjusted per queue. Assuming that the user will either send one format or the other, for each queue, it is possible for DR to keep information in its cache or configuration, informing code what to expect. The proposed solution would be to add a counter for each transport, that would track the ratio of formats that were successfully recognized and in which format they appear to be sent. This would help &quot;guess&quot; which of the deserialization methods have the best chances of success on a given queue. This will improve performance of this design in both edge cases (one type of message arriving via transport) and in case of mixed messages, without introducing severe performance issues.</p>
</li>
</ul>
<h5><a class="header" href="#solution-iv---single-endpoint-with-deserialization-marker" id="solution-iv---single-endpoint-with-deserialization-marker">Solution IV - single endpoint with deserialization marker</a></h5>
<p>This design uses some kind of marker that allows to easily discover what type of message arrived. There is potentially a few different ways to solve it each with its issue, all the same it is more of an expansion. Solutions like this are very low-level and usually are, but not always, tailored for specific usage and not generic, which is the opposite of what CDL tries to be.</p>
<ol>
<li>
<p>First byte:
If we can assume that the JSON formatted message will start  with either byte 0x5B &quot;<code>[</code>&quot; or 0x7B &quot;<code>{</code>&quot;.
In this case, we can check the first character that arrives, and decide the message format on that information. This also may be implemented as an improvement for Solution III. This method assumes that the message will not start with a non-whitespace character, and MessagePack will not use those bytes for its own serialization methods (which according to its specification will be the case). Please note that this option may result in unnecessary and heavy CPU branching if done incorrectly, while not providing many tools and methods to prevent it.</p>
</li>
<li>
<p>Extension (header) check:
This is the same method as the one adverted above, nonetheless in this case, correct behavior can be ensured using MessagePack's extension types. Unfortunately, it is unsure by reading the specification whenever the header or appendix is created to accommodate extension data. In this case, Empiric testing was proven successful in determining that for a specific implementation, <code>messagepack-rs</code> it is indeed a header consisting of bytes <code>0xD6</code> which, according to specification, represent the <code>ext</code> family of formats. However, one issue is apparent, which is a severely lacking support for user extensions in rust libraries, of which at least one out of 5 supported it. Rust support were found in the &quot;First that works&quot; approach, and it is unclear what support is available for the other languages, nevertheless support is apparent and well-defined in MessagePack specification.
<a href="https://github.com/msgpack/msgpack/blob/3f0a9aae716596a86878c0d68dc0bd4256673202/spec.md">MessagePack Specification</a></p>
</li>
<li>
<p>Marker injection
Last and &quot;dirtiest&quot; option is to inject the required specific byte before or after the payload. This option is guaranteed to break support on the client side, and simply judging from the sheer volume of changes on both sides of the CDL system, is heavily discouraged.</p>
</li>
</ol>
<h5><a class="header" href="#notes-3" id="notes-3">Notes</a></h5>
<p>Suggested changes require alteration in v1 specification to force users to trim the messages from white spaces before committing them to the transport.
The Cost of Failure for this solution is 0(1), presumably also confined to one branch operation.</p>
<h5><a class="header" href="#major-concerns-3" id="major-concerns-3">Major concerns:</a></h5>
<ul>
<li>Changes to existing v1 spec that are not clear or apparent. It means that v1 format will not change, but the way of delivery will. It is unclear right now how to announce those changes and if those should result in v2 format specification or not, also it is not known if the trimming should be used on the side of the receiver or the sender.</li>
<li>Depending on specification and partial loss of flexibility - DR will now employ another set of restrictions, however minor, to the message format to be able to correctly and concisely discern the incoming content format.</li>
</ul>
<h5><a class="header" href="#solution-v---single-endpoint-with-specific-deserialization-focus" id="solution-v---single-endpoint-with-specific-deserialization-focus">Solution V - single endpoint with specific deserialization focus</a></h5>
<p>The Method presented here will be the simplest, albeit with some drawbacks. Taking into consideration the scalability of the DR, we can provide multiple DR instances with configurations that are in counterbalance to each other. Providing this is the second format that is intended to be supported, two instances should be perfectly able to cover all the cases, in which the first instance will look for JSON formatted messages, and the second will respond on MessagePack payload.
This method will work with different configurations, especially in the case in which the user will use explicitly one format over another and use CDL system with that knowledge in hand. Configuration MUST be provided during the application startup.</p>
<h6><a class="header" href="#major-concerns-4" id="major-concerns-4">Major concerns:</a></h6>
<ul>
<li>This design will have to be extended with better error handling that the one that is present currently. In case there will be mixed-message scenario, where multiple formats are used in tandem, there is a guarantee that either instance will throw a deserialization error, as it will be configured to handle a different format.</li>
</ul>
<h6><a class="header" href="#notes-4" id="notes-4">Notes:</a></h6>
<p>No testing nor PoC was created for this solution, please treat it as being theoretical.</p>
<h2><a class="header" href="#further-considerations" id="further-considerations">Further Considerations</a></h2>
<h4><a class="header" href="#impact-on-other-teams-1" id="impact-on-other-teams-1">Impact on other teams</a></h4>
<p>Depending on the chosen solution, we may have to notify all users about the changes. It may also be necessary to introduce those as a &quot;breaking change&quot;.</p>
<h4><a class="header" href="#scalability" id="scalability">Scalability</a></h4>
<p>Using Solution I may result in additional endpoints that have to be either provided or created. Apart from that, from initial research, there appear to be no impact on scalability whatsoever.</p>
<h4><a class="header" href="#availability-problems" id="availability-problems">Availability problems</a></h4>
<p>Some of the solutions depends on the specific usage and/or implementation of the specific libraries and languge support. It is encouraged to put additional effort in weighting the specific solution against the other.</p>
<h4><a class="header" href="#testing-1" id="testing-1">Testing</a></h4>
<p>This feature MUST undergo thorough testing before being accepted. Test cases MUST include:</p>
<ul>
<li>Failure scenarios:
<ul>
<li>Edge scenarios where messages are unreadable or follow worst-case scenario path.</li>
</ul>
</li>
<li>Malformed messages:
<ul>
<li>reaction of the software to malformed or misaligned messages.</li>
</ul>
</li>
<li>Happy test:
<ul>
<li>Proper behavior with proper values.</li>
</ul>
</li>
<li>Format testing for each, readily available repository type:
<ul>
<li>Document storage.</li>
<li>Time series storage.</li>
<li>Binary storage.</li>
</ul>
</li>
<li>Performance testing with all the before mentioned cases:</li>
</ul>
<h4><a class="header" href="#workload-estimation" id="workload-estimation">Workload estimation</a></h4>
<p>Success rate should be acceptable, considering there is not a lot of solution-sepcific logic to introduce. Depending on which solution is chosen. It is initially roughly estimated to 20MD</p>
<h2><a class="header" href="#deliberation" id="deliberation">Deliberation</a></h2>
<h4><a class="header" href="#out-of-scope-1" id="out-of-scope-1">Out of scope</a></h4>
<ul>
<li>Any other, not mentioned features, were not taken into consideration in this scope. Especially versioning (CDLF-00010-00)</li>
<li>Communication within CDL system itself, and an output format are out of the scope of this document.</li>
<li></li>
</ul>
<h4><a class="header" href="#open-questions" id="open-questions">Open Questions:</a></h4>
<ul>
<li>Usage and behavior in Service mesh can not be checked at this point. This RFC should be revisited when the feature is complete and testing can be performed.</li>
<li>CDLF-0000D-00 - Service Meshing - format handling changes should not affect Service Meshing itself, however at this point in time it is hard to guarantee that.</li>
</ul>
<h4><a class="header" href="#notes-5" id="notes-5">Notes</a></h4>
<ul>
<li>CDLF-00009-00 - Message Batching - have to be taken into consideration while writing this feature</li>
<li>CDLF-0000A-00 - Message Ordering - guaranteeing order in case of mixed formats will not be possible, albeit while using a specific format or different endpoint, order can be enforced.</li>
</ul>
<h3><a class="header" href="#end-matter" id="end-matter">End Matter</a></h3>
<p>This is the version of the RFC that was deliberately cut down to ingestible size and format. The number of cases and solutions were condensed to those that were considered by the researcher the most valuable, leaving out minor variation that can be deduced from already provided solutions.</p>
<h5><a class="header" href="#references-1" id="references-1">References:</a></h5>
<p>MessagePack related materials:
<a href="https://github.com/msgpack/msgpack/blob/3f0a9aae716596a86878c0d68dc0bd4256673202/spec.md">MessagePack Specification</a>
<a href="https://thephp.website/en/issue/messagepack-vs-json-benchmark/">MessagePack Benchmark</a>
<a href="https://msgpack.org/">MessagePack Project</a></p>
<p>CDL :
<a href="https://github.com/epiphany-platform/CommonDataLayer">CDL project</a>
<a href="https://github.com/epiphany-platform/CommonDataLayer/discussions/categories/rfc">CDL - RFC discussions</a>
<a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/docs/rfc">CDL - RFC candidates</a>
<a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc">CDL - RFC releases</a></p>
<p>References to notable rust libraries used in research:
<a href="https://lib.rs/crates/messagepack-rs">messagepack-rs (library with user extensions for Rust)</a></p>
<h1><a class="header" href="#front-matter-5" id="front-matter-5">Front Matter</a></h1>
<pre><code> Title: CDL Ingestion API versioning
 Author: Łukasz Biel
 Team: CDL
 Reviewer: CDLTeam
 Created on: 5/2/2021
 Last updated: 19/3/2021
 Tracking issue: https://github.com/epiphany-platform/CommonDataLayer/issues/225
 =====================================================
 this rfc is outdated, and kept for archivisation reasons
 following RFCs superseeds it:
 - CDLF-00010-00-rfc-02.md
 =====================================================
</code></pre>
<h1><a class="header" href="#introduction-2" id="introduction-2">Introduction</a></h1>
<p>We need to introduce a way to version the <code>CIM</code> schema.</p>
<h2><a class="header" href="#summary-2" id="summary-2">Summary</a></h2>
<p>We should provide JSON schema with <code>CIM</code> format.
We need to return the most recent message version in case of deserialization failure in <code>DR</code>.</p>
<h2><a class="header" href="#glossary-5" id="glossary-5">Glossary</a></h2>
<p><code>CIM</code> - CDL Ingestion Message <br />
<code>DR</code> - data router</p>
<h2><a class="header" href="#goals-and-requirements-1" id="goals-and-requirements-1">Goals and Requirements</a></h2>
<p>It should be transparent to CDL users which schema they have to use when inserting data into CDL.
In case of version mismatch, <code>DR</code> must send a notification with information about the mishap, or,
in the case of synchronous protocols, it must return a descriptive error.</p>
<h1><a class="header" href="#solutions-2" id="solutions-2">Solutions</a></h1>
<h2><a class="header" href="#existing-solution-1" id="existing-solution-1">Existing solution</a></h2>
<p><code>CIM</code> schema versioning does not exist.
Users can find data format specifications in <a href="rfc/../architecture/data_router.html">data-router</a>.
It contains only mandatory fields, skipping optional, e.g., <code>orderGroupId</code>.
The most recent version of the data format is available when looking into the CDL code.</p>
<h2><a class="header" href="#proposed-solution-1" id="proposed-solution-1">Proposed solution</a></h2>
<h3><a class="header" href="#1-publish-schema-do-not-validate-it" id="1-publish-schema-do-not-validate-it">1. Publish schema, do not validate it</a></h3>
<p>We will publish the <code>CIM</code> schema in <code>/docs/cdl_schema/vX.json</code> in JSON schema format.
The schema version will only consist of the <code>MAJOR</code> version number.
Furthermore, new versions will be published once we propose a new mandatory field.
Optional fields will not result in a version change.
<code>data-router</code> will not check the optional <code>version</code> field in the <code>CIM</code> message.
If the message fails to deserialize, <code>DR</code> will produce an error with a schema version that is compatible with it.</p>
<blockquote>
<p>If we cannot deserialize the message to the expected format, DataRouter cannot easily extract the <code>version</code> field.
We could parse payload as <code>JSONMap</code> and find a field named <code>version</code>, but all we'd gain is a more verbose error
message &quot;DR supports version X, you sent us Y&quot;. While it, at the surface, seems better, in reality,
the user is aware of what version they are sending, and this extra step is unnecessary.</p>
</blockquote>
<p>In the proposed scenario, <code>DR</code> can handle only one version of the schema.
Furthermore, multiple versions of the <code>data-router</code> may share one version of the schema.
DataRouter assumes that if the <code>version</code> field is missing, the payload is in supported data format (&quot;most recent&quot;).</p>
<h4><a class="header" href="#comments--questions" id="comments--questions">Comments &amp; Questions</a></h4>
<ul>
<li>Should we use JSON schema with MsgPack as well? In theory, we are only describing the format of <code>CIM</code>.
However, what if <code>MsgPack</code> requires more info that <code>JSONschema</code> would be able to convey?
<ul>
<li>As for our knowing this is not a problem.</li>
</ul>
</li>
<li>Let's assume we document a feature, e.g., new field <code>xyz</code>.
The user would send this field in payload expecting <code>feature</code> to work, but that depends on the <code>DR</code> he's running.
If the user is running an older version of <code>DR</code>, it would quietly discard the given field.
The whole premise of versioning is to ease debugging of errors encountered in CDL.
Using only <code>MAJOR</code> seems to go against it. We may need to add a <code>MINOR</code> to the mix, or report extra fields.
<ul>
<li>We will throw errors on extra/unknown fields we receive.</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#2-validate-version-support-multiple-parsers" id="2-validate-version-support-multiple-parsers">2. Validate version, support multiple parsers</a></h3>
<p>We will publish specifications in the same way as mentioned in the prior solution.
We will read the <code>version</code> field first (extracting it before parsing the whole JSON), and <code>DR</code> will choose the correct parser based on that version.
Internally all messages would be parsed to one unified format.
Such a feature will guarantee that the client can use new versions of CDL without upgrading his applications.
In this case, we should assume that if a message has no <code>version</code> field, it uses <strong>oldest</strong>, probably <code>1.0</code> schema.</p>
<h4><a class="header" href="#comments--questions-1" id="comments--questions-1">Comments &amp; Questions</a></h4>
<ul>
<li>How are we going to approach deprecations in this case?</li>
<li>Client still may send a too recent version of format to an old instance of CDL.</li>
<li>Supporting many deserializers is problematic at best, so we need to be extra careful when introducing this.</li>
</ul>
<h3><a class="header" href="#other-considerations" id="other-considerations">Other considerations</a></h3>
<h4><a class="header" href="#headers" id="headers">Headers</a></h4>
<p>We could use headers to pass info about the version.
<code>GRPC</code>, <code>Kafka</code>, and <code>RabbitMQ</code> support headers (however, <code>RabbitMQ</code> rust clients have slight issues).
However, the header should not dictate the workings of a program. In the first proposal,
a header with a different version that current would cause an error,
in the second, only header with a too recent version.</p>
<h2><a class="header" href="#decided-solution" id="decided-solution">Decided solution</a></h2>
<p>We will follow with option 1, with an exception that <code>data-router</code> will check version of ingested message and fail if it's incompatible.
Implementation will start when we will introduce major change to <code>CIM</code> format.</p>
<h1><a class="header" href="#further-considerations-1" id="further-considerations-1">Further considerations</a></h1>
<h2><a class="header" href="#impact-on-other-teams-2" id="impact-on-other-teams-2">Impact on other teams</a></h2>
<p>Depending on the solution, external teams have to familiarize themselves with schemas either
during each upgrade or when <code>DR</code> changelog explicitly states bump of <code>MAJOR</code> number in <code>CIM</code>.</p>
<h1><a class="header" href="#rfc-changelog" id="rfc-changelog">RFC Changelog</a></h1>
<ul>
<li>19.03.2021 - Updated RFC with decision</li>
</ul>
<h2><a class="header" href="#front-matter-6" id="front-matter-6">Front Matter</a></h2>
<pre><code>  Title           : CDL Ingestion API versioning - changes to the format
  Author(s)       : Mateusz 'esavier' Matejuk
  Team            : CommonDataLayer
  Reviewer        : CommonDataLayer
  Created         : 2021-06-14
  Last updated    : 2021-06-14
  Version         : 1.1.0
  CDL feature ID  : CDLF-00010-00
  Related issue:  https://github.com/epiphany-platform/CommonDataLayer/issues/553
  Related issue:  https://github.com/epiphany-platform/CommonDataLayer/issues/552

  ==================================================
  This document makes following direct RFCs obsolete:
  CDLF-00010-00-rfc-01.md
  ==================================================

</code></pre>
<h5><a class="header" href="#abstract-1" id="abstract-1">Abstract</a></h5>
<pre><code>  The key words &quot;MUST&quot;, &quot;MUST NOT&quot;, &quot;REQUIRED&quot;, &quot;SHALL&quot;, &quot;SHALL
  NOT&quot;, &quot;SHOULD&quot;, &quot;SHOULD NOT&quot;, &quot;RECOMMENDED&quot;,  &quot;MAY&quot;, and
  &quot;OPTIONAL&quot; in this document are to be interpreted as described in
  RFC 2119.
</code></pre>
<p><a href="https://www.ietf.org/rfc/rfc2119.txt">RFC 2119 source</a></p>
<h2><a class="header" href="#introduction-3" id="introduction-3">Introduction</a></h2>
<p>Due to outside requirements, we would like to introduce input protocol versioning.
This document states changes that needs to be made in CDL Ingestion Message format, and explains the reasoning behind those.</p>
<h5><a class="header" href="#formats-1" id="formats-1">Formats:</a></h5>
<p>v1 CDL Input Message - as for today, this is the message format used as CDL's input.</p>
<pre><code>{
  object_id: UUID,
  schema_id: UUID,
  data: { any valid json },
}
</code></pre>
<p>V1 CDL Input Message Change - proposed change related to versioning:</p>
<pre><code>{
  version: String
  object_id: UUID,
  schema_id: UUID,
  data: { any valid json },
}
</code></pre>
<p>Format changes have to encompass changes related to other features if those exist.</p>
<h2><a class="header" href="#changes-to-data-router-behavior" id="changes-to-data-router-behavior">Changes to Data Router Behavior:</a></h2>
<p>Due to forementioned changes, DataRouter will have to discard messages that don't version field, as it is mandatory. Additionally, protocols that are not supported have to be discarded as well and error is to be emitted to appropriate channels.</p>
<h4><a class="header" href="#reasoning" id="reasoning">Reasoning:</a></h4>
<p>It was requested for this feature to be as simple as possible, so we decided that stringified version will be used, i.e. &quot;1.0&quot;. This will require additional complexity on implementation side but, will be easier to use by external systems.</p>
<h3><a class="header" href="#notes-6" id="notes-6">Notes:</a></h3>
<p>Currently, there is no process of determining if DataRouter can handle all the features provided by protocol version, however, protocol validation will be introduced as a new feature, and will undergo separate research and design, as validation is outside the scope of this document.</p>
<p>This feature must be revisited when encryption and compression of incoming streams are going to be implemented.</p>
<h5><a class="header" href="#references-2" id="references-2">References:</a></h5>
<p>Parent RFC:
<a href="rfc/CDLF-00010-00-rfc-01.html">CDLF-00010-00-rfc-01</a></p>
<p>CDL :
<a href="https://github.com/epiphany-platform/CommonDataLayer">CDL project</a>
<a href="https://github.com/epiphany-platform/CommonDataLayer/discussions/categories/rfc">CDL - RFC discussions</a>
<a href="https://github.com/epiphany-platform/CommonDataLayer/tree/develop/docs/rfc">CDL - RFC candidates</a>
<a href="https://github.com/epiphany-platform/CommonDataLayer/tree/main/docs/rfc">CDL - RFC releases</a></p>
<h1><a class="header" href="#front-matter-7" id="front-matter-7">Front Matter</a></h1>
<pre><code>Title: Materialization
Author: Sam Mohr
Team: CDL
Reviewers: CDL Team
Created on: February 7th, 2021
Last updated: February 11th, 2021
Tracking Issue: https://github.com/epiphany-platform/CommonDataLayer/issues/227
</code></pre>
<h1><a class="header" href="#introduction-4" id="introduction-4">Introduction</a></h1>
<p>Demand has been expressed for Materialization by multiple clients.</p>
<h2><a class="header" href="#summary-3" id="summary-3">Summary</a></h2>
<p>Materialization is the projection of value(s) into a different format.</p>
<p>Materialization in the Common Data Layer will need to provide some means
to project data (either a single value or a combination of values) in a
user-defined manner.</p>
<h2><a class="header" href="#context" id="context">Context</a></h2>
<h3><a class="header" href="#origin" id="origin">Origin</a></h3>
<p>Materialization was suggested as a potential feature in the CDL a while ago
as a means for teams using the CDL for storage to make complicated queries
quicker to read by simply precalculating the results and then caching them.
It was recently brought up again as potential client teams saw the usefulness
in materialization and requested it be implemented for their use.</p>
<h3><a class="header" href="#previous-efforts" id="previous-efforts">Previous Efforts</a></h3>
<p>The CDL Lite, a small, monolithic version of the CDL created during the
prototyping phase of the CDL for testing performance, has a version of
materialization implemented. The solution involves adding two extra types
to each Schema: Views and Relations. </p>
<p>Schemas can have zero to many views, and views are each defined as a format
to map a value belonging to a schema into using <a href="https://jmespath.org">JMESPath</a>. Any schema
can point to any other schema (like a directed edge in a graph) with a relation,
which defines a parent-child relationship between schemas. For safety, there is
a recursion limit when loading child data to allow users to configure cyclical
relationships but preventing memory overloads.</p>
<p>The combination of views and relations allows for deterministic projection of any
value under a schema into a user-defined format, even allowing for the inclusion
of pluralities of data belonging to other schemas. However, this solution was only
implemented in the CDL Lite because the CDL Lite runs on a single binary and a
single database, and has no need to use the network to retrieve data from other
repositories. If a schema's relation points to data in other repositories, the full
version of the CDL would need to make a network request for data in any different
repository at least once and probably more, which incurs a very high networking cost.
This would need to be mitigated if we consider this solution for the full CDL.</p>
<h3><a class="header" href="#how-it-affects-other-components" id="how-it-affects-other-components">How It Affects Other Components</a></h3>
<p>The CDL was designed to be agnostic of the format of data stored in it, as well
as where it is stored. To that end, all possible repositories simply implement a
common interface for accepting arbitrary data for storage, and those repositories
don't communicate with each other, only ingesting from the common Data Router and
reporting insertions of data to a common Kafka topic.</p>
<p>However, data can be directly inserted into any repository through its respective
Command Service, and if a user uses their own Command Service that they wrote
themselves, then there is no constraint currently requiring them to properly report
insertion events over Kafka.</p>
<p>If some materialization is recalculated based on when data updates, we don't have
a consistent means (currently) of determining when data was successfully inserted
into an arbitrary repository, and will need to find one, or only support periodic
recalculations if any.</p>
<h2><a class="header" href="#technical-requirements" id="technical-requirements">Technical Requirements</a></h2>
<p>We will need to provide projection either for single values or multiple values, even
distributed across multiple repositories. The projection will need to be available
both on-demand and recalculated automatically when values are updated in storage.
For at least projections recalculated on change, they will need to be cached in
ElasticSearch, and potentially other user-defined caches in the future.</p>
<h2><a class="header" href="#out-of-scope-2" id="out-of-scope-2">Out of Scope</a></h2>
<p>At least for now, partial updates of cached data are out of scope for this feature.
They are extremely complex in their interaction with the proposed approaches in this RFC,
and would take far too long to implement in the same time span as the rest of the work
entailed by this feature.</p>
<h2><a class="header" href="#future-goals" id="future-goals">Future Goals</a></h2>
<p>Though generically implementing storage in caches like ElasticSearch in this story
will simplify later efforts to support other caches (e.g. MySQL, Redis, etc.), it
is not a requirement. However, at some point, it would be useful to provide these 
other caches for users and allow them to configure which caches to either store all
data in, or the data for specific projections.</p>
<h1><a class="header" href="#solutions-3" id="solutions-3">Solutions</a></h1>
<p>As materialization will likely comprise multiple components all forming a solution,
this RFC will discuss the options for each component, which are:</p>
<ul>
<li>Method of Materialization</li>
<li>On-Demand Materialization (calculation of projections per request)</li>
<li>Cached Materialization (calculation of projections without prompting)</li>
<li>Caching of Materialization (storage of projections in a cache, namely ElasticSearch for now)</li>
<li>Configuration of Materialization</li>
</ul>
<h2><a class="header" href="#method-of-materialization" id="method-of-materialization">Method of Materialization</a></h2>
<h3><a class="header" href="#method-1-adaptation-of-cdl-lite-approach" id="method-1-adaptation-of-cdl-lite-approach">Method 1: Adaptation of CDL Lite Approach</a></h3>
<p>The CDL Lite approach (<a href="rfc/CDLF-00011-00-rfc-01.html#previous-efforts">seen above</a>) describes adding Views and Relations
to schemas such that any value under a schema can deterministically be projected (including
child values belonging to child schemas based on the parent schema's relations). For example,
given a schema <code>Work Order</code> that has a child schema <code>Asset</code>, the definition of a view <code>Cost</code>
written as the <a href="https://jmespath.org">JMESPath</a> <code>{ cost: cost + sum(assets[].cost) }</code> for <code>Work Order</code>s,
the <code>Cost</code> view could be deterministically calculated for each <code>Work Order</code>.</p>
<p>Views would have the fields <code>name</code> and <code>jmespath</code>, and schemas would have zero to many views.
Relations would have the fields <code>optional</code>, <code>terminal</code>, and <code>path</code>. <code>optional</code> determines whether
an error should be thrown if no children are present. <code>terminal</code> determines what type of ID's are
found at the end of the <code>path</code> for this relation (a single ID, a list of ID's, or a map of ID's).
The <code>path</code> is the path to the ID's of the children from the root of a child value, e.g. a path
of <code>[ &quot;birthday&quot;, &quot;id&quot; ]</code> in an object</p>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;John Doe&quot;,
  &quot;birthday&quot;: {
    &quot;month&quot;: &quot;February&quot;,
    &quot;day&quot;: 12,
    &quot;year&quot;: 1975,
    &quot;id&quot;: &quot;valid-uuid&quot;
  }
}
</code></pre>
<p>will render &quot;valid-uuid&quot;.</p>
<p>To calculate the projection for a view, first get all of the children of the given value based
on the child relations of its schema (and their respective children recursively) replace the ID's
at the end of each <code>path</code> with the value under the respective ID, and then calculate the view of
the data with all child data incorporated.</p>
<h4><a class="header" href="#pros" id="pros">Pros</a></h4>
<p>This solution offers highly configurable output, even beyond the combination of arbitrary
values, this allows for complex operations on the data using anything that JMESPath provides.
Also, rather than having to specify a view for every specific value, any value under a schema
can be automatically materialized.</p>
<h4><a class="header" href="#cons" id="cons">Cons</a></h4>
<p>Unfortunately, as mentioned with this solution's place in the CDL Lite, the number of network
requests in this solution is O(m + n), where m is the number of schemas and n is the number of
data values requested. So long as users can use these features freely, there is potential for
serious performance loss. The best option is to keep these complicated features, but to inform
users that using complex views and relations will cause slow performance. In addition, partial
updates will be extremely difficult to implement later.</p>
<h3><a class="header" href="#method-2-simple-joins-only" id="method-2-simple-joins-only">Method 2: Simple Joins Only</a></h3>
<p>With the versatility of JMESPath and it's options for manipulating data, users are able to perform
an extremely wide number of complex calculations and aggregations of the data stored in the CDL.
This makes it more difficult for us to determine what users have done, and by extension, to know
accurately what data under a schema requires updating, forcing an update of every value under a
schema when any dependency changes.</p>
<p>Most of the behaviors we are providing to users for the combination and aggregation of data are
available in the cache services, but would be slower and defeat the point of precalculating and
caching their desired format. However, from when materialization was first proposed, the wants
of users were very simple: just the ability to &quot;join&quot; data.</p>
<p>To simplify things, we can constrain the possible operations in JMESPath by either removing the
included functions like addition and division, or by forking JMESPath to define our own
highly-simplified version of the format (both to constrain users and our development time).</p>
<h4><a class="header" href="#pros-1" id="pros-1">Pros</a></h4>
<p>Either way, simply providing a way to reshape data while still providing all of the other features
in the same manner will satisfy most of our users without giving them a potential footgun that
will need to be strongly documented.</p>
<h4><a class="header" href="#cons-1" id="cons-1">Cons</a></h4>
<p>This approach would take more research, especially if we decide to fork JMESPath instead of
constraining the existing implementation. Also, for those users that would prefer to do all of the
work in one spot, they will be left needing to do work in two places: in the CDL and then in their
chosen cache (e.g. ElasticSearch).</p>
<h2><a class="header" href="#the-materialization-services" id="the-materialization-services">The Materialization Service(s)</a></h2>
<p>The materialization service(s) for eager and automatic projection will each be responsible for
collecting data from various repositories, manipulating it, and either sending it somewhere. In
this shared behavior leaves two solutions: move the common behavior to a library and render as 
two applications, or write a single common application that performs both tasks internally and
has one input port for materialization requests as well as a &quot;cron&quot; job for materialization to
be sent to a cache such as ElasticSearch.</p>
<h3><a class="header" href="#method-1-a-library-used-by-2-services" id="method-1-a-library-used-by-2-services">Method 1: A Library Used by 2 Services</a></h3>
<p>A library is the de facto way to move common behavior from multiple applications to one place.
Thus, in spite of the cost of adding yet another crate to our compilation process, the code cleanup
and resultant simplification of the 2 services would be great for long-term maintainability. Also,
in deployment of multiple instances of each application, resource usage would be more correctly
distributed where needed when the services run in different processes, especially if there is a
significant difference in the number of requests to one service or another.</p>
<h3><a class="header" href="#method-2-a-single-common-service" id="method-2-a-single-common-service">Method 2: A Single, Common Service</a></h3>
<p>On the other hand, the library holding the common materialization logic would not be useful
elsewhere in our application, and may not be worth the effort to separate from our services. Though
on the surface, there is benefit in code structure and proper resource distribution with the 2
service-approach, code structure can be achieved with good use of modules, and <code>tokio</code> will properly
handle the distribution of resource if the application properly leverages async behavior, which the
CDL already does everywhere else.</p>
<h2><a class="header" href="#caching-of-materialized-data" id="caching-of-materialized-data">Caching of Materialized Data</a></h2>
<p>The current expected behavior is to store all data in ElasticSearch, but in keeping with the CDL's
philosophy of dynamic modules, we will ideally, at some point in the future, support multiple caches,
or even better, user-defined caches. The decision to be made is whether to provide infrastructure
now to plan for the future, or to implement a functional solution for now and extend it later.</p>
<p>To implement it now, the most straight-forward approach is to follow the way of the Command Service
and add a user service for each cache called a Cache Service that takes data from the materialization
service(s) over gRPC and stores it in a user-defined cache. This would mean creating a gRPC definition
for the cache service, and interfacing only with ElasticSearch for now, likely through the <a href="https://docs.rs/elasticsearch/7.10.1-alpha.1/elasticsearch/">official
client library</a> for Rust, which would basically require copying of the source for
a Command Service.</p>
<p>To implement only communication with ElasticSearch would entail extending the materialization service(s)
to use the <code>elasticsearch</code> in place of a user-defined Cache Service, and to have slightly more error
handling in the materialization service(s) instead of the Cache Service. It would potentially be able
to determine earlier if failure occurred in storing projected data in the cache, but users would be able
to observe when materialization issues occur by looking at when the last update was made to the cached
data and comparing it to the normal rate of update.</p>
<p>The simpler approach is to leave more work for later, but it is more beneficial to the structure of
this project to separate it for a good separation of concerns and a more common structure across our
sub-applications.</p>
<h2><a class="header" href="#configuration-of-materialization" id="configuration-of-materialization">Configuration of Materialization</a></h2>
<h3><a class="header" href="#what-to-store" id="what-to-store">What to Store</a></h3>
<p>The main data to be stored is the definitions of the materialization to be done and how often it
should be recalculated. Views and Relations have already been defined above, so the following fields
are for configuration of materialization frequency:</p>
<table><thead><tr><th align="right">Field</th><th align="center">Type</th><th align="left">Description</th></tr></thead><tbody>
<tr><td align="right">eager</td><td align="center">bool</td><td align="left">whether to recalculate on every update</td></tr>
<tr><td align="right">minWaitTime</td><td align="center">int</td><td align="left">minimum number of seconds to wait between updates</td></tr>
<tr><td align="right">updateEvery</td><td align="center">int</td><td align="left">number of seconds to update after when updates arrive</td></tr>
</tbody></table>
<h3><a class="header" href="#where" id="where">Where</a></h3>
<p>Though it would be possible to store the configuration information where it is needed (in the
materialization service(s)), we already have the schema registry and a configuration service (in
development), so there are more appropriate locations for configuration data to be stored. </p>
<p>If the materialization is per-schema, then the obvious approach is to extend what a schema holds and
store materialization definitions there. However, for the metadata about when to update projections,
it makes more sense to move it to the configuration service like the metadata for schemas. For schema
data, we are making the distinction between business data and application behavior, which is JSON Schema
definitions and Kafka topics/AMQP exchanges, respectively.</p>
<h2><a class="header" href="#test-plan-1" id="test-plan-1">Test Plan</a></h2>
<p>The simple use cases are:</p>
<ul>
<li>Eager materialization</li>
<li>Caching of projected data</li>
<li>Addition of materializer definitions</li>
<li>Configuration of automatic projection timing</li>
</ul>
<p>These cases can all be tested primarily with manual testing, with some unit tests advised.</p>
<p>The primary complex use cases are:</p>
<ul>
<li>Deeply-nested data</li>
<li>Inter-related schema projection</li>
<li>Proper cache updates when complex data updates</li>
</ul>
<p>Though at least manual testing is required for these, at least some Python tests should be added
to ensure that this feature isn't accidentally broken by future feature additions.</p>
<h1><a class="header" href="#edge-registry-1" id="edge-registry-1">Edge registry</a></h1>
<h2><a class="header" href="#motivation" id="motivation">Motivation</a></h2>
<p>CDL lacks the ability to declare relations between two objects. We need some way of storing that information if we want to be able to create complex views. Edge registry would allow us to store relations between objects no matter if they are stored in same repository or in different ones. </p>
<h2><a class="header" href="#proposed-changes" id="proposed-changes">Proposed changes</a></h2>
<hr />
<p>:memo: Proposed names are not final, and they are likely to be changed. They are here to provide the idea of what should be done, not exact way how it should be done nor how it should be named. </p>
<hr />
<p>Add a special kind of repository named edge registry. There will be at most one edge registry deployed at any time. It will keep information about relations on the object level and on the schema level. Schema registry should publish notifications for any object related change. </p>
<h3><a class="header" href="#api" id="api">API:</a></h3>
<ul>
<li>Add relation type – adds schema level entry </li>
<li>Get relation type – returns schema level entry</li>
<li>Delete relation type – removes schema level entry </li>
<li>List relation types – lists relations, can be filtered by schema id parameter </li>
<li>Add related object – adds object level entry </li>
<li>Get related objects – returns object level entries, filtered by relation_id and/or object_uuid list, requires at least one filter(should not be able to  query whole db at once) </li>
<li>Delete related – objects – removes object level entry </li>
</ul>
<h3><a class="header" href="#information-to-be-stored" id="information-to-be-stored">Information to be stored</a></h3>
<h4><a class="header" href="#schema-level" id="schema-level">Schema level</a></h4>
<ul>
<li>relation id: UUID, autogenerated </li>
<li>schema 1 id: UUID </li>
<li>schema 2 id: UUID </li>
</ul>
<h4><a class="header" href="#object-level" id="object-level">Object level</a></h4>
<ul>
<li>relation id: UUID </li>
<li>object 1: UUID </li>
<li>object 2: UUID </li>
</ul>
<h3><a class="header" href="#enhancements" id="enhancements">Enhancements</a></h3>
<p>Functionalities which might be worth implementing, but aren't required for the feature to be ready.</p>
<h4><a class="header" href="#storing-relations-for-sub-objects" id="storing-relations-for-sub-objects">Storing relations for sub objects</a></h4>
<p>Storing relationship between two flat structures is easy. Doing the same for nested objects is much more complicated. In case of nested object any child, grandchild etc. of the entity can be related to any other entity or one of its children. This makes storing relations outside of related object almost impossible. For example, we cannot use array indices – removal of one child would create a cascade effect. </p>
<p>This can be migrated by introducing relation markers. Relation marker is an additional UUID field which is put directly to sub object which is related to another object. By adding those relation markers to object level entities, we can store information on sub object relationships without having to worry about object changes (from relation perspective). </p>
<h5><a class="header" href="#required-changes" id="required-changes">Required changes:</a></h5>
<ul>
<li>Add Schema 1 Marker Path, Schema 2 Marker Path to Schema level – optional string fields containing information on where in object structure marker field is located (if this is a sub object relation) </li>
<li>Add Object 1 Marker, Object 2 Marker to Object level – optional UUID fields, marker values of related sub objects </li>
</ul>
<h4><a class="header" href="#relation-graph" id="relation-graph">Relation graph</a></h4>
<p>Main advantage of keeping relation information separately from data is the ability to generate whole relation graph without touching the data itself. This makes generating relation graph a cheap operation and allows multiple optimizations to take place when data is being fetched. </p>
<h5><a class="header" href="#optimizations" id="optimizations">Optimizations</a></h5>
<p>Since optimizations are just causally related to graph repository itself, they will only be briefly mentioned here. For more information on them see descriptions on how query plans are generated in SQL databases since those two topics (and their optimizations) are very much alike. </p>
<p>Most SQL databases uses three implementations of join operation (hash join, merge join, nested loop). Proper algorithm is chosen based on amount of estimated number of rows affected by the operation (both inputs and output) and few other factors. Additionally, query engine decides which input set should be left and which should be right operand of the operation. Choosing wrong operand side can kill performance of some join algorithms. Another optimization that SQL Engines do is choosing proper order of join operations. When we expect multiple tables to be joined together its SQL Engine job to decide order result sets will be joined. The most performant way in most cases is just to sort trees of join operations by size of expected outcome. </p>
<p>All mentioned optimizations are based on number of expected records used in join operation. Some of them requires knowledge of whole join graph before they can be used. </p>
<h5><a class="header" href="#required-changes-1" id="required-changes-1">Required changes</a></h5>
<ul>
<li>Add another api endpoint which for given input - object representing multiple join operations (relation tree) and array(s) of object ids – returns output – a tree containing object ids. </li>
</ul>
<h5><a class="header" href="#example" id="example">Example</a></h5>
<hr />
<p>:memo: Input/output formats will be changed, for sake of convenience relations are represented by name (not UUID), object_ids as integer numbers. </p>
<hr />
<hr />
<p>:memo: Filtering on edge registry layer can be done only for object_id fields. </p>
<hr />
<p>Get me all the employees, their addresses, information about their boss and boss's boss where employee id is one of 203, 403 and boss id is one of 2,4. </p>
<p>SQL pseudocode: <code>Select * from employee join address join employee as boss join employee as boss2 where employee.id in [203,403] AND boss.id in [2,4]</code></p>
<p>Input format: </p>
<pre><code>{ 
    root:  employee 
    relations: { 
        lives_in:  true, 
        boss: { 
            boss: true 
        } 
    }, 
    ids: [203,403],
    filters: { 
        boss: [2,4] 
    } 
} 
</code></pre>
<p>Output format: </p>
<pre><code>[
    {
        id: 203, 
        lives_in: [24], 
        boss: [
            { 
                id: 2, 
                boss: [1] 
            }
        ]
    }
] 
</code></pre>
<h2><a class="header" href="#other-solutions" id="other-solutions">Other solutions</a></h2>
<h3><a class="header" href="#keep-relationship-information-directly-on-oneboth-of-related-options" id="keep-relationship-information-directly-on-oneboth-of-related-options">Keep relationship information directly on one/both of related options</a></h3>
<p>In this case to do any join operation we need to fetch all the values of specific schema. This can be costly for situations where only few items are related and join result is smaller than both inputs of join operation. Additionally, with this solution we cannot do optimizations mentioned in Relation graph section, or we can do them in a limited way. </p>
<h2><a class="header" href="#more-things-to-consider" id="more-things-to-consider">More things to consider</a></h2>
<ul>
<li>Should we check if object exists in registry before accepting relation data to be saved – technically it is simple to do (if we do not have object delete operation), however in async environment (message queues) it might be harder to use by clients – there is no message processing ordering guarantee between different repositories. </li>
<li>What should we do when user tries to remove relation on schema level if there are object level entries for this relation? </li>
<li>Should we have some mechanism of cleaning edge registry from entries to objects which do not exist </li>
</ul>
<h1><a class="header" href="#rfc-changelog-1" id="rfc-changelog-1">RFC Changelog</a></h1>
<ul>
<li><code>19.03.2021</code> - Changed name from <code>edge-repository</code> to <code>edge-registry</code></li>
</ul>
<h1><a class="header" href="#materialized-views" id="materialized-views">Materialized views</a></h1>
<h2><a class="header" href="#motivation-1" id="motivation-1">Motivation</a></h2>
<p>CDL lacks the ability to filter and transform data before returning it to the user. The purpose of this proposal is allowing users to define views which would allow them to receive data in different format then it was inserted to CDL. Because creating some complex views on the fly can be computationally expensive, we are introducing concept of materialized views – views which will be precalculated, but may not always contain the latest data. Materialized views will be refreshed automatically on user defined conditions (for now those setting will be defined per deployment). </p>
<hr />
<p>:warning: Implementing this proposal requires implementation of  <a href="https://github.com/epiphany-platform/CommonDataLayer/blob/develop/docs/rfc/CDLF-00012-00-rfc-01.md">CDLF-00012-00</a></p>
<hr />
<h2><a class="header" href="#proposed-changes-1" id="proposed-changes-1">Proposed changes</a></h2>
<ul>
<li>Add 3 new components: 
<ul>
<li>Partial Update Engine – responsible for deciding if materialized view data needs to be updated, if we need to recalculate whole view or if we can keep it up to date by changing only few entries </li>
<li>Object builder – responsible for fetching data from various repositories and joining it together </li>
<li>Materializers – responsible for storing materialized view data in user specified service (e.g. Elasticsearch, Postgres) </li>
</ul>
</li>
<li>Change view definition structure stored in schema registry </li>
</ul>
<h2><a class="header" href="#feature-list-1" id="feature-list-1">Feature list</a></h2>
<ul>
<li>Automated materialized view updates - Partial Update Engine </li>
<li>Limit how often views can be recalculated – Partial Update Engine </li>
<li>Limit how much resources are used for view updates - Partial Update Engine &amp; Object builder </li>
<li>Filtering(SQL counterparts): 
<ul>
<li>By relation – object builder (JOIN) </li>
<li>By object id – object builder (WHERE) </li>
<li>By schema id – schema registry (view definition) (FROM) </li>
<li>By custom field – object builder (WHERE) </li>
<li>By grouped field – materializers (HAVING) </li>
</ul>
</li>
<li>Grouping, window functions – materializers </li>
<li>Joins – Object builder </li>
<li>Data transformations - materializers </li>
</ul>
<h2><a class="header" href="#message-flow" id="message-flow">Message Flow</a></h2>
<p>Following diagram presents message flow in cdl related to materialized view functionality. Some components which do not participate in the process are omitted.</p>
<p><img src="https://user-images.githubusercontent.com/9082099/109971040-128d9600-7cf6-11eb-8a11-10e41334a757.png" alt="image (6)" /></p>
<p>Data router receives new message: </p>
<ul>
<li>When message is standard input data: 
<ul>
<li>Data router asks schema registry for the repository data needs to be inserted (or uses cache) (1)</li>
<li>Data router passes message to proper repository (2)</li>
<li>Repository inserts data into database and send notification to notification storage (e.g.  Kafka) (3)</li>
</ul>
</li>
<li>When message is information about relation between two objects: 
<ul>
<li>Data router passes message to Edge registry (4)</li>
<li>Edge registry sends notification to notification storage (5)</li>
</ul>
</li>
</ul>
<p>When there are new notifications in notification service (this will not happen right away; Partial Update Engine might be in sleep phase): </p>
<ul>
<li>Partial Update Engine fetches information about changed objects and relations. It decides when materialized view needs to be partially recalculated (6)</li>
<li>Partial Update Engine sends to object builder (8): 
<ul>
<li>view id </li>
<li>array of ids on which we need to calculate the view(schema_id, object_id pairs)</li>
</ul>
</li>
<li>Object builder fetches view definition (or uses cache) (9)</li>
<li>Object builder asks edge registry for relation graph for specified objects (10)</li>
<li>Object builder fetches object information from repositories, performs filtering, join operations etc. (11) </li>
<li>Object builder sends transformed object data and array of ids to materializers (12)</li>
</ul>
<h2><a class="header" href="#schema-repository" id="schema-repository">Schema repository</a></h2>
<p>Schema repository is responsible for storing information about schemas and views. Exact structure of view definition will be designed once we agree on general solution. </p>
<h2><a class="header" href="#partial-update-engine-1" id="partial-update-engine-1">Partial Update Engine</a></h2>
<p>Partial Update Engine is responsible for decision on when partial view should be recalculated. It requests object builder to build partial view based on information of outdated records. </p>
<p>Partial Update Engine Loop: </p>
<ul>
<li>
<p>Fetch definition of all views (7; can use cache) </p>
</li>
<li>
<p>Process all pending notifications: 
For each notification take every view definition and check if view data is affected by the change. Return list of object_id, schema_id pairs - called change list later in this document. </p>
<p>Notifications: </p>
<ul>
<li>Add relation between objects</li>
<li>Delete relation between objects </li>
<li>Insert/update object </li>
</ul>
<p>If notification mentions object which can be used by view new record is added to the change list. For relation related notifications it is fine to add only one side of relation. </p>
</li>
<li>
<p>Run reduce on generated change lists – so there are no duplicates</p>
</li>
<li>
<p>Split change lists if they are too large – max list size defined in config </p>
</li>
<li>
<p>Queue all views for which we generated change records to be refreshed </p>
</li>
<li>
<p>Go to sleep for X seconds – time defined in config </p>
</li>
</ul>
<h2><a class="header" href="#object-builder-1" id="object-builder-1">Object Builder</a></h2>
<p>Object builder responsibility is creating complex objects according to recipes - view definitions. </p>
<p>Object Builder Loop:</p>
<ul>
<li>Wait for request to build a view </li>
<li>Fetch view definition from schema registry (9) </li>
<li>Fetch relation graph from Edge registry (10) </li>
<li>Fetch objects from repositories (11) </li>
<li>Perform filtering by custom fields, join operations </li>
<li>Send data to materializers component </li>
</ul>
<p>It is important to note that object builder output contains view id, change list received from partial update engine, and requested objects with information how they were created (each returned object contains ids of every object which was used for its creation). </p>
<h2><a class="header" href="#materializers" id="materializers">Materializers</a></h2>
<p><img src="https://user-images.githubusercontent.com/9082099/109971136-2f29ce00-7cf6-11eb-8e61-bbc018f83f15.png" alt="image (5)" /></p>
<p>There are two approaches on how materializers can look like. The main difference between them is if we allow grouping, window functions, filtering on grouped entities to happen in external services (e.g., Elasticsearch, Postgres). Problem with grouping is that merging multiple rows into one makes it almost impossible to apply partial updates.</p>
<h3><a class="header" href="#approach-i" id="approach-i">Approach I</a></h3>
<p>Materializer is just a single connector service and a database. Its job is to update the database on changes received from object builder in a transactional way: delete all records based on objects from change list, add records returned from object builder. </p>
<p>Grouping, advanced filtering and similar operations are done on the fly as user requests the data – they are mostly cheap operations. If that's not enough users can always create pipelines to do more data transformations. </p>
<h3><a class="header" href="#approach-ii" id="approach-ii">Approach II</a></h3>
<p>This approach requires us to store another copy of data. First phase materializer is a component which extends the job of materializers from Approach I, but it uses storage which is internal to CDL. Then it performs data transformation (grouping, window functions, filtering on grouped records) on whole view (not just part of it) and pushes result down the pipeline to another  service which recreates the views in external storage. </p>
<h2><a class="header" href="#final-notes" id="final-notes">Final notes</a></h2>
<h3><a class="header" href="#what-about-fetching-view-data-which-are-not-materialized-views-calculated-on-the-fly" id="what-about-fetching-view-data-which-are-not-materialized-views-calculated-on-the-fly">What about fetching view data which are not materialized (views calculated on the fly)?</a></h3>
<p>Adding another specialized materializer solves this case easily. However, since message flow is bit different in this case (processing request starts from this specialized materializer) this case is not described in this document to not obscure general idea. </p>
<h2><a class="header" href="#alternative-solutions-1" id="alternative-solutions-1">Alternative solutions</a></h2>
<h3><a class="header" href="#partial-update-engine-periodically-querying-repositories-for-updated-records" id="partial-update-engine-periodically-querying-repositories-for-updated-records">Partial Update Engine periodically querying repositories for updated records</a></h3>
<p>This approach should work correctly, but it has some minor drawback: </p>
<ul>
<li>Even if cdl is idle (not data changes) partial update engine needs to do its work </li>
<li>Requires repositories to store last changed time which currently is not a requirement </li>
</ul>
<p>Anyway, those are minor drawback, we could as well use this solution. </p>
<h1><a class="header" href="#rfc-changelog-2" id="rfc-changelog-2">RFC Changelog</a></h1>
<ul>
<li><code>19.03.2021</code> - Followed rename from <code>edge-repository</code> to <code>edge-registry</code></li>
</ul>
<h1><a class="header" href="#front-matter-8" id="front-matter-8">Front Matter</a></h1>
<pre><code>Title           : CDL publishing deployment configurations
Author(s)       : Łukasz Biel
Team            : CommonDataLayer
Reviewer        : CommonDataLayer
Created         : 2021-05-17
Last updated    : 2021-05-17
Version         : 1.0.0
CDL feature ID  : CDLF-00015-00
</code></pre>
<h1><a class="header" href="#introduction-5" id="introduction-5">Introduction</a></h1>
<p>We need to revisit what we publish in terms of deployment - currently there are multiple configurations present in repositories,
and often we have difficulty keeping them up to date. 
Having multiple deployments increases complexity of every change, as PR must visit much more files, more scenarios have to be tested manually.</p>
<p>On top of deployments comes provisioning database. Currently, we support doing that to Postgres, with Victoria Metrics being automatically in &quot;correct&quot; state,
and Druid is set up by the Client.
Our Postgres has 2 methods of provision present in repository, both of them assuming schema name, so they are not portable in this regard.
Provisioning happens via sqlx and postgres docker volume.</p>
<h1><a class="header" href="#glossary-6" id="glossary-6">Glossary</a></h1>
<p><em>Migration</em> - SQL migration, including only CDL internal structure (no schema migrations). Right now it means 0 to x state only.</p>
<h1><a class="header" href="#topics" id="topics">Topics</a></h1>
<h2><a class="header" href="#deployment-4" id="deployment-4">Deployment</a></h2>
<h3><a class="header" href="#currently-supported" id="currently-supported">Currently supported</a></h3>
<h4><a class="header" href="#helm-1" id="helm-1">helm</a></h4>
<ul>
<li>mandatory (in theory k8s is mandatory, but helm is better replacement)</li>
<li>we will end up using it for some tests</li>
<li>we try to avoid using it for development - it's slow to compile crates on k8s, environments can be tricky</li>
</ul>
<h4><a class="header" href="#docker-compose-1" id="docker-compose-1">docker-compose</a></h4>
<ul>
<li>optional - initially used for local deployment setup, currently only supplies infra for testing</li>
</ul>
<h4><a class="header" href="#horust" id="horust">horust</a></h4>
<ul>
<li>optional</li>
<li>linux-only (uses linux api's incompatible with OSX and Windows)</li>
<li>bare-metal</li>
</ul>
<h4><a class="header" href="#custom-bare-metal" id="custom-bare-metal">custom bare metal</a></h4>
<ul>
<li>optional</li>
<li>configuration can be propagated to other forms of deployment</li>
</ul>
<h3><a class="header" href="#proposed" id="proposed">Proposed</a></h3>
<p>We revisit helm and publish a template where you plug in individual toml configurations.
This should reduce helm complexity and ease maintenance.
We will keep cdl-config up to date. A good idea may be using <code>schemars</code> crate, or writing our own version with TOML support.
We will provide basic config for <code>kafka + all repositories</code> and <code>rabbit/grpc with postgres</code>.
With this combination we can manage local development and k8s deployments, and have just one place where we keep configs up to date.
Docker-compose and horust can stay in separate repo, for personal use. We shouldn't require a PR to update those as well 
(not to mention config files can mitigate the need to update).
We shouldn't provide <code>infra</code> helm files - just CDL deployment.
Infrastructure <code>docker-compose</code> could stay in separate repo for development use.</p>
<h2><a class="header" href="#database-setup" id="database-setup">Database setup</a></h2>
<h3><a class="header" href="#currently" id="currently">Currently</a></h3>
<p>We have docker-compose postgres setup and schema-registry setup in two separate directories.
A client has to download these files and modify them before being able to apply.</p>
<h3><a class="header" href="#options" id="options">Options</a></h3>
<h4><a class="header" href="#embedding-db-setup-into-cdl" id="embedding-db-setup-into-cdl">Embedding db setup into CDL</a></h4>
<p>Tricky and hard to manage. We have to remember that CDL is not Postgres only. While this may be an option for <code>edge</code> and <code>schema</code> registries, this is out of question for repositories now.</p>
<h4><a class="header" href="#setting-up-directory-with-migrations" id="setting-up-directory-with-migrations">Setting up directory with migrations</a></h4>
<p>We will need to have migrations per <code>app</code>, assuming each application can use separate database.
We need to pick a tool to run these migrations.</p>
<h5><a class="header" href="#druid" id="druid">Druid</a></h5>
<p>Probably need to write this tool ourselves. Druid requires conversion from CDL schema to its internal format, and maybe,
assuming <code>SR-less deployments</code>, this tool has to be usable without schema whatsoever.</p>
<blockquote>
<p>druid support is currently on hold</p>
</blockquote>
<h5><a class="header" href="#influx--victoriametrics" id="influx--victoriametrics">Influx / VictoriaMetrics</a></h5>
<p>This db seems to not require migrations</p>
<h5><a class="header" href="#postgres" id="postgres">Postgres</a></h5>
<p>Postgres is very tricky. There's a lot of tools that do what we want in better or worse manner.</p>
<ul>
<li>SQLx is rust based option, but it forces us to write scripts with <code>schema</code> knowledge - we should avoid it.</li>
<li>FlyWay is generic tool (could be used with other SQL databases if we ever support them), used by our clients, that can handle multiple schemas etc.</li>
<li>We can always bash-script these migrations into db.</li>
</ul>
<h4><a class="header" href="#no-config-files" id="no-config-files">No config files</a></h4>
<p>User would be left alone. We'd still have to provide documentation on how to write these migrations.</p>
<hr />
<p>I'd vote for having migrations in folder structure <code>deployment/migrations/{app}</code> for <code>edge-registry</code> and <code>schema-registry</code> and
<code>deployment/migrations/{db}-repository</code> for repositories. We don't mention any tool or how to use them. We will manage something internally.</p>
<p>This also means that no <code>druid-conversion-tool</code> now, only <code>../migrations/postgres-repository</code> will exist.</p>
<p>As for <code>migrations</code> name. Now they will be only single script per tool, but we have to future-proof ourselves to a moment when a repo will require
change of db-schema.</p>
<h1><a class="header" href="#decisions-1" id="decisions-1">Decisions</a></h1>
<blockquote>
<p>Actions are from meeting about deployment that happened on 24.06.2021.</p>
</blockquote>
<h2><a class="header" href="#helm-2" id="helm-2">Helm</a></h2>
<p>Some additional work has to be done with helm. It will stay within main repository, however we need to apply changes to charts.
Issue https://github.com/epiphany-platform/CommonDataLayer/issues/583 was created to accommodate work that needs to be done.
We are going to switch helm to support configuration tomls.
Caveats are:</p>
<ul>
<li>configuration must be dynamic and partially based on values.</li>
<li>we have to keep in mind that host addresses depend on <code>deployment</code> name.</li>
<li>user should be able to configure communication method or repository kind they're deploying.</li>
</ul>
<h2><a class="header" href="#docker-compose-2" id="docker-compose-2">Docker-Compose</a></h2>
<p>docker-compose will be removed from main repository. We'll place development composes of databases/kafka/amqp/etc. in https://github.com/epiphany-platform/CommonDataLayer-deployment.
We'll depend one repository on another, so that postgres database is setup via scripts from main CDL repo.</p>
<p>We may verify whether using submodules or depending on repository being at specific path is better.</p>
<h2><a class="header" href="#horustbare-metal" id="horustbare-metal">Horust/bare-metal.</a></h2>
<p>Horust and development config files may stay where they are. Both methods are there for fast and easy, non-production setup. They don't introduce
lots of extra clutter.</p>
<h2><a class="header" href="#postgres-1" id="postgres-1">Postgres</a></h2>
<p>We'll split current db init scripts into separate directories. At moment of writing this, that will be <code>document-repository</code>, <code>edge-registry</code> and <code>schema-registry</code>.
We won't provide any way of running these on production databases. Helm will adjust to linking from multiple folders.</p>
<p>Proposed path: <code>{root}/deployment/db-setup/postgres/{app-name}</code>.</p>
<h1><a class="header" href="#front-matter-9" id="front-matter-9">Front Matter</a></h1>
<pre><code>Title           : Schema-Registry-less CDL deployment
Author(s)       : Wojciech Polak
Team            : CommonDataLayer
Reviewer        : CommonDataLayer
Created         : 2021-06-24
Last updated    : 2021-06-24
Version         : 1.0.0
CDL feature ID  : CDLF-00016-00
</code></pre>
<h2><a class="header" href="#glossary-7" id="glossary-7">Glossary</a></h2>
<h3><a class="header" href="#terminology-1" id="terminology-1">Terminology</a></h3>
<ul>
<li>CDL - Common Data Layer</li>
<li>DR - Data Router, a CDL component responsible for routing ingested messages.</li>
<li>SR - Schema Registry, a CDL component responsible for keeping information about the type of object conveyed inside the message.</li>
<li>QR - Query Router, a CDL component responsible for routing queries to the proper repository.</li>
<li>CS - Command Service, a CDL component, ingestion part of the repository.</li>
<li>QS - Query Service, a CDL component, query part of the repository.</li>
<li>CIM - CDL Ingestion Message</li>
<li>User - user of the CDL.</li>
<li>Message - message sent to CDL for processing.</li>
<li>Breaking Change - behavior change, or API of the CDL, may result in system breakage on release update.</li>
<li>Routing information - information necessary for DR and QR to successfully deliver the provided message and querying the repository's data.</li>
</ul>
<h2><a class="header" href="#formats-2" id="formats-2">Formats</a></h2>
<p>v1.0 input message format up to this point:</p>
<pre><code>{
    object_id: UUID,
    schema_id: UUID,
    data: { any valid JSON }
}
</code></pre>
<p>v1.0 input message format to accommodate schema-registry-less routing:</p>
<pre><code>{
    options: {
        repository: String?, // Optional field
    }, // If empty, can be ommited for backwards compability
    schema_id: UUID,
    object_id: UUID,
    data: { any valid JSON }
}
</code></pre>
<p>Static configuration format:</p>
<pre><code>[repositories]
repository_name = { insert_destination: String, query_address: String, schema_type: SchemaType }
</code></pre>
<p>SchemaType</p>
<pre><code>&quot;documentstorage&quot; | &quot;timeseries&quot;
</code></pre>
<h2><a class="header" href="#introduction-6" id="introduction-6">Introduction</a></h2>
<h3><a class="header" href="#background-2" id="background-2">Background</a></h3>
<p>It was requested from the CDL stack to route messages in an environment where SR is not deployed.</p>
<p>Currently, CDL is using SR to decide which insert destination and query address is used during data and query routing.
This information is tightly tied to the Schema (and schema id).
Each Schema stored in the PostgreSQL table contains these fields:</p>
<table><thead><tr><th>id</th><th>name</th><th>schema type</th><th>insert destination</th><th>query address</th></tr></thead><tbody>
<tr><td>UUID</td><td>String</td><td>SchemaType</td><td>String</td><td>String</td></tr>
</tbody></table>
<p>Also worth noting is that the user can use one CS to store more than one Schema. Therefore one Schema is not equivalent to one service stack.</p>
<p>The second purpose (which in many cases can be omitted) is that each Schema is linked to its definition, which might be used for validation purposes.</p>
<p>After implementing this feature, CDL will insert and query data from proper repositories without using SR. It is intended for limited environments and therefore has a limited number of supported features.</p>
<h3><a class="header" href="#assumptions-1" id="assumptions-1">Assumptions</a></h3>
<ul>
<li>CDL should allow to omit Schema Registry deployment and provide an alternative way of routing data.</li>
<li>Considering CIM format, any new fields or configuration must not be a breaking change.</li>
</ul>
<h3><a class="header" href="#limitations-1" id="limitations-1">Limitations</a></h3>
<ul>
<li>In this alternative, state routing would become a static table, which requires restarting the services after each change.</li>
<li>Data validation would be unavailable as we would not keep track of schema definitions.</li>
<li>Materialization would be unavailable, as Views also are handled by SR and are coupled with Schemas.</li>
</ul>
<h2><a class="header" href="#proposed-solution-2" id="proposed-solution-2">Proposed solution</a></h2>
<h3><a class="header" href="#static-routing" id="static-routing">Static routing</a></h3>
<p>Like reverse proxies allow the user to set up static routing, CDL should allow it via configuration files.
Schema definitions would be separated from routing information.
This routing would take precedence over querying SR (in case both of them are available).</p>
<p>Each routing service (DR and QR) would load the same static configuration file, and then whenever the field
<code>repository</code> would be present in the new CIM input message. It would use static routing information to send the request further.</p>
<ul>
<li>Data Router would use the <code>insert_destination</code> field.</li>
<li>Query Router would use <code>query_address</code> and <code>schema_type</code> fields.</li>
</ul>
<p><code>schema_id</code>, while is not associated with any Schema definition, is still used as an element of the object identification pair:
(<code>schema_id</code>, <code>object_id</code>) and required by both QR and DR. As mentioned in the Background section, one repository stack might be used to store more than one Schema. Also, CDL does not guarantee the uniqueness of <code>object_id</code> outside of its Schema.</p>
<p>Because there is no SR responsible for registering Schema and assigning it <code>schema_id</code>, it means that the user must prepare their predefined UUIDs.</p>
<h3><a class="header" href="#possible-extension" id="possible-extension">Possible extension</a></h3>
<p>In the future, whenever Configuration Service would be introduced, one might think about moving this Static routing into a dynamic environment.</p>
<h3><a class="header" href="#major-concerns-5" id="major-concerns-5">Major concerns</a></h3>
<p>The static routing allows new kinds of errors and bugs in the CDL system. The CDL user must consider the possibility of service configuration sync issues because there is no single source of truth or consensus algorithm.</p>
<p>It is crucial to use the same configuration for both ingesting and querying data.</p>
<h2><a class="header" href="#further-considerations-2" id="further-considerations-2">Further Considerations</a></h2>
<h3><a class="header" href="#impact-on-other-teams-3" id="impact-on-other-teams-3">Impact on other teams</a></h3>
<p>This feature does not provide any breaking change. Therefore all other teams can use CDL with Schema Registry without problems.</p>
<h3><a class="header" href="#scalability-1" id="scalability-1">Scalability</a></h3>
<p>No impact. Because static routing is Read-Only, one can scale all services without any extra cost or configuration.</p>
<h3><a class="header" href="#testing-2" id="testing-2">Testing</a></h3>
<p>This feature MUST undergo thorough testing (in the best scenario, automatic E2E tests). Test cases MUST include:</p>
<ul>
<li>Failure scenarios:
<ul>
<li>Message is corrupted</li>
<li>Message contains repository which is not defined in static routing</li>
<li>Corrupted configuration</li>
</ul>
</li>
<li>Green path test</li>
<li>Performance tests</li>
</ul>
<h1><a class="header" href="#front-matter-10" id="front-matter-10">Front Matter</a></h1>
<pre><code>Title           : Query raw routes
Author(s)       : Łukasz Biel
Team            : CommonDataLayer
Last updated    : 2021-07-06
Version         : 1.0.0
Notes           : Raw routes are already implemented
</code></pre>
<h1><a class="header" href="#glossary-8" id="glossary-8">Glossary</a></h1>
<ul>
<li><code>QR</code> - Query Router</li>
<li><code>QS</code> - Query Service</li>
<li><code>QS-ts</code> - Query Service - timeseries</li>
<li><code>gRPC</code> - Remote procedure call protocol - <a href="https://grpc.io/">website</a></li>
<li><code>VM</code> - Victoria Metrics</li>
</ul>
<h1><a class="header" href="#description-1" id="description-1">Description</a></h1>
<p>Raw routes in <code>QR</code> and <code>QS</code> allow users to bypass CDL api in places where it's lacking.</p>
<p>Accessing <code>raw</code> route is done via <code>/raw</code> endpoint in <code>QR</code>:</p>
<pre><code class="language-yaml">  /raw:
    get:
      summary: Execute queries given in body
      operationId: executeRawQuery
      parameters:
        - name: SCHEMA_ID
          in: header
          description: 'Schema ID of document to execute a queries'
          required: true
          schema:
            type: string
            example: &quot;15251181-f749-42e0-b4a4-e4b3d90e990d&quot;
        - name: REPOSITORY_ID
          in: header
          description: 'Static routing repository name'
          required: false
          schema:
            type: string
            example: document_backup_repository
      requestBody:
        required: true
        content:
          application/json:
            schema:
              oneOf:
                - $ref: '#/components/schemas/Raw'
      responses:
        '200':
          description: &gt;-
            JSON containing the results of executed commands
          content:
            application/json:
              schema:
                type: object
</code></pre>
<p>It's body is a json specific to targeted schema/repository, with schema:</p>
<pre><code class="language-json">{ &quot;raw_statement&quot;: &quot;$ACTUAL_STATEMENT_WITHIN_JSON_STRING&quot; }
</code></pre>
<p><code>QS</code> and <code>QS-ts</code> have respective <code>gRPC</code> endpoints.</p>
<h2><a class="header" href="#document-repositories" id="document-repositories">Document repositories:</a></h2>
<pre><code class="language-protobuf">  rpc QueryRaw (RawStatement) returns (ValueBytes);
</code></pre>
<h3><a class="header" href="#postgresql-1" id="postgresql-1">PostgreSQL</a></h3>
<p><code>RawStatement</code> is an SQL query <strong>string</strong> that will get executed directly on the database, and result will be serialized into 2 dimensional string array.</p>
<blockquote>
<p>There is no safeguard for insert/delete statements, nor for config changes, however, using these isn't a good practice.
User created for QS to access postgres should have those operations disabled.</p>
</blockquote>
<h2><a class="header" href="#timeseries-repositories" id="timeseries-repositories">Timeseries repositories:</a></h2>
<pre><code class="language-protobuf">  rpc QueryRaw (RawStatement) returns (ValueBytes);
</code></pre>
<h3><a class="header" href="#victoria-metrics" id="victoria-metrics">Victoria Metrics</a></h3>
<p>&quot;RawStatement&quot; should be an escaped json with schema:</p>
<pre><code class="language-json">{
  &quot;method&quot;: &quot;$METHOD&quot;,
  &quot;endpoint&quot;: &quot;$ENDPOINT&quot;,
  &quot;queries&quot;: [[&quot;$KEY1&quot;, &quot;$VALUE1&quot;], [&quot;$KEY2&quot;, &quot;$VALUE2&quot;]]
}
</code></pre>
<ul>
<li><code>$ENDPOINT = /query_range | /query | /export | ...</code> - any available VictoriaMetrics endpoint</li>
<li><code>$METHOD = GET | POST</code> - http method, only 2 are allowed, please refer to <code>VM</code> documentation for information when to use which.</li>
<li><code>$KEY, $VALUE pairs</code> - request parameters, when <code>$METHOD == GET</code> they are translated to <code>query?</code> arguments, otherwise they are sent as a <strong>FORM</strong> body.</li>
</ul>
<h3><a class="header" href="#druid-1" id="druid-1">Druid</a></h3>
<p>&quot;RawStatement&quot; is a string sent directly to Druid instance as a body, via <code>POST</code> method.</p>
<h1><a class="header" href="#additional-work" id="additional-work">Additional work</a></h1>
<p>We have to hide raw endpoints behind a feature flag. Such feature flag will be accessible from configuration tomls in the form of:</p>
<pre><code class="language-toml">[features]
raw_endpoint = true|false
</code></pre>
<p>with default value set to true, as to not create breaking changes in 1.0 release. When 2.0 lands, we may consider turning this off by default.</p>
<p>Depending on this feature, <code>QR</code> may or may not expose <code>/raw</code> endpoint. There's no option for conditional exposition in query-services,
thus their routes must be available, just when <code>raw_endpoint = false</code> they will return Status code <code>Unavailable</code>.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
